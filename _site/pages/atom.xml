<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>雷丹的博客</title>
    <link>http://localhost:4000</link>
    <description>My personal blog</description>
    
      <item>
        <title>大规模社交用户的标签自动生成技术研究</title>
        <link>http://localhost:4000/2017/04/10/weibo_user_profile.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/10/weibo_user_profile.html</guid>
        <pubDate>Mon, 10 Apr 2017 14:12:25 +0800</pubDate>
        <description>&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;社会标签(Social Tagging)作为一种新型网络信息组织方式,由网络信息的提供者或者用户自发为某类信息赋予一定数量的标签,
选用自由词对感兴趣的网络信息资源进行描述来实现网络信息的分类,正在改变着传统的网络信息组织模式,
被广泛应用在社交网络、商业以及科研教育领域。社会标签较传统的主题词具有更大的灵活性、易用性,同时资源描述性关键词的增加也便于
对资源进行准确查找,尤其是对多媒体资源来说,用户所标注的标签信息就更为重要。&lt;/p&gt;

&lt;p&gt;Word2Vec从提出至今，已经广泛应用在自然语言处理、机器学习、数据挖掘等领域，从它延伸出的Doc2Vec、Graph2Vec也具有广泛的应用前景，
它已经成为了深度学习在自然语言处理中的基础部件，本文尝试利用大规模微博社交网络数据，提出将异构网络（微博用户关系网络、用户标签网络、用户文本网络）
进行联合训练的方法并得到用户与标签的嵌入式表示。这项技术可用于建模用户兴趣，推荐用户感兴趣的商品、信息和好友，进行用户画像，以及寻找目标用户进行商品推广。
本文提出的联合训练方法产生的用户与标签的嵌入式表示优于分别训练用户关系网络、用户标签网络产生的用户和标签的嵌入式表示，并在预测用户标签的准确率上优于同类方法。&lt;/p&gt;

&lt;p&gt;关键字：推荐系统，图嵌入表示，大数据&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Social Tagging is a new type of organizing the information of Internet. The users of Internet will provide some tags to
some objects spontaneously.We can use these tags to sort resources that we are interested in. Social Tagging is changing
the the method of the organization of network information resources and widely used in the social network service,business
and education. Social tagging is more flexible and convenient than traditional key words. With the increase of the number of 
social tags，it is more convenient to locate resource. Especially for multi-media resource，social tagging plays an increasingly important role.&lt;/p&gt;

&lt;p&gt;Word2vec has been widely applied in the fields of NLP，machine learning and data mining since it was first proposed by Mikolov. Doc2vec
and Graph2Vec，which are all originated from Word2vec，also have wide application foreground. Word2vec has become fundamental component in 
the field of NLP. This paper tries to use huge amounts of social networks data. In this paper, a new algorithm is presented, which can be
use to train heterogeneous network(user relationship network and user tags network and user text network). Through this algorithm,we can 
get embedding of both user and tag and then we can use these embedding in the fields，such as user interests modeling，Commodity recommendation and User portrait. The embedding produced by our algorithm is better than that produced by separately training each network.
Besides,in the case of predicting user tags，our algorithm is better than other similar methods.&lt;/p&gt;

&lt;p&gt;keywords:Recommendation System,Graph Embedding,Big Data&lt;/p&gt;

&lt;h2 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h2&gt;
&lt;h3 id=&quot;11-研究背景&quot;&gt;1.1 研究背景&lt;/h3&gt;

&lt;p&gt;微博作为一种新型媒体，是一种基于草根用户的关系构建个性化用户信息的即时传递、共享和获取平台。它具有信息实时性，内容简洁性，用户交互性等特点。
微博之所以可以成为当今国内外主流的社交媒体，主要是因为其具有强大的用户实时交互性，用户在使用微博的过程中，会在微博的网络空间中结成种种关系，
比如用户之间的关注关系，社区中的好友和亲情关系，实时交互过程中因共同购买或评论产品而结成的共同评论关系等，有效分析和挖掘微博中复杂的用户关系不仅可以激发、
助推和引导社会事件的发展趋势，还可以准确高效的为关注某一兴趣和爱好的微博群体进行个性化推荐，甚至可以大大降低企业和消费者的交易成本，推动企业营销模式的不断创新。
此外，微博在凝聚民心，降低事件危害以及政务互动等方面也发挥着不可替代的积极作用。由此可见，微博的兴起赋予了社会经济活动前所未有的大众化和网络化的内涵，
极大提升了社交媒体的社会服务效能。但是急剧增长的微博用户数量和海量用户下的交互行为增加了社会、经济与生产的复杂性，使一些社会实践变得更加不可预测，难以控制，
从而为分析社会化效应带来了新的挑战，因此如何正确理解微博用户之间的关系以及用户在关系交互中所产生的行为，成为学者迫切需要研究的新方向。&lt;/p&gt;

&lt;p&gt;社会化标签（Social Tagging） 也称为collaborative tagging，指的是用户在网络中自发得分配电子标签或关键词来描述网络上的资源，并且在网络用户群体中共享这些标签。这种方法允许用户使用自己的语言、以“标签”的形式对信息资源的内外部特征进行标注,以实现资源的查找和共享。社会化标签系统与预先定义网络资源类别
 来对网络资源分类的机制相比，它可以使用户自发产生和分配标签，这对网络资源分类有积极意义。随着社会化标签的广泛应用，公众分类法（folksonomy）应运生，这种分类方法改变了传统利用专家来对网络资源分类的方式，它是从公众的角度来分类资源。公众分类法是互联网所推崇的共享与协作精神的体现,是新的互联网信息环境中一种独具特色的信息组织工具。它的产生为互联网信息组织与检索的改进提供了新方向。随着社交网络，图片分享，视频分享业务的蓬勃发展，社会化标签系统被广泛应用在互联网的各个领域，它可以对社交网络和数据挖掘提供技术支持，而且有助于改进搜索结果，提高广告投放的准确率，同时利用所有用户产生的标签数据可以挖掘出其他有价值的信息，比如用户社群，潜在目标客户等。社会化标签系统将用户产生的大量通过网络传播而聚合起来，可以实现对网络资源的合作标记和公众分类，体现网络用户的群体智慧。&lt;/p&gt;

&lt;p&gt;在web2.0中，用户不仅可以通过豆瓣来分享图书,通过优酷来分享视频，通过微博来发表博文，通过Flicker来发布照片，通过youtube上传视频等方式来创造内容，
用户这些行为有一个共同的特征，即用户会自由的选择一些词(Term)或者短语(Phrase)来标注相关网络信息资源，
我们称用户的这种行为为标注(Tagging),用户所选择的词或者词语为标签(Tag),提供标注行为的系统为社会标签系统，
本文研究的就是利用微博用户关系与用户已有标签来为每个用户推荐相关的标签，便于建模用户兴趣，为用户之间的衔接赋予更丰富的信息，
推荐用户感兴趣的商品、信息和好友，进行用户画像，以及寻找目标用户进行商品推广。&lt;/p&gt;

&lt;h3 id=&quot;12-问题的提出&quot;&gt;1.2 问题的提出&lt;/h3&gt;
&lt;p&gt;在Web2.0环境下，互联网已经成为全球最大的知识库，它在给人类的生活和工作带来革命性变化的同时，也引发了“信息泛滥”，“信息迷航”等问题，社会化标签推荐能够根据用户的需求主动的将合适的信息、商品、知识提供给用户，可以有效缓解这些问题。同时，作为由用户产生的元数据，社会化标签能够独特反应用户的需求及其变化&lt;a href=&quot;#mathes2004folksonomies&quot;&gt;[1]&lt;/a&gt;，而且“用户-资源-社会化标签”之间的关系网络能够为个性化信息推荐系统提供十分有价值的基础数据，由此部分学者对基于社会化标签的个性化知识推荐进行了密切的关注，并从以下三个方面进行了探索。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于矩阵的方法，即通过构建“用户-资源”矩阵、“用户-社会化标签”矩阵，“社会化标签-资源”矩阵实现知识推荐。Ji AT等人依据“用户-资源”矩阵，“用户-标签”矩阵，“标签-资源”矩阵构建Naive Bayesian分类器，以此为基础实现协同过滤推荐&lt;a href=&quot;#JiAT2007Collaborative&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于聚类的方法,主要包括用户、资源、社会化标签三种对象的聚类，其中基于社会化标签的聚类是当前研究的重点。一个重要的研究思路就是一句社会化标签之前的共现频率，利用k-means聚类算法、马尔科夫聚类算法等方法对社会化标签进行聚类，进而依据聚类的结果为用户提供个性化推荐。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于图论的方法。其中社会网络分析是学者们关注的重点，如Shiratsuchi等人依据用户使用的标签之间的相似性
建立用户社会网络&lt;a href=&quot;#shiratsuchi2006finding&quot;&gt;[3]&lt;/a&gt;，并利用Clauset提出的local midularity算法&lt;a href=&quot;#clauset2005finding&quot;&gt;[4]&lt;/a&gt;划分网络社区，进而实现协同过滤推荐。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;显然，学者们提出的三类方法都有其优势，但是都面临各自的问题。首先第一类方法，面临一个重要的问题就是社会化标签使用量的“幂率分布”规律，排序在前几位的社会化标签具有较大的使用量，而大量的社会化标签都处于“长尾”区域，由此相关的矩阵可能非常不规则，从而严重制约个性化推荐算法。其次第二类方法中基于社会化标签的聚类方法属于基于内容的推荐思想，难以发现用户新的兴趣，而且社会化标签使用量的“幂律分布”问题同样会制约推荐效果。最后，第三类方法中基于社会网络分析的方法属于协同过滤思想，虽然能够发现用户新兴趣，但是个性化推荐需要依据用户的特定知识需求，在一般的社会网络中，个性间的“关系互动”并不意味着在特定的知识情境下必然能产生“知识互动”由此，当前学者们提出的基于社会化标签的各种推荐方法都有自身无法克服的劣势，如何利用社会化标签是实现精准的推荐是学者研究的重要问题，本文中提出一种有监督训练社会化标签的方法，我们对海量微博数据构建用户关系网络，用户转发网络，用户文本网络，用户标签网络，其中用户转发网络与用户关系网络刻画的是用户之间的交互关系，我们通过一阶相似性和二阶相似性找到相似用户&lt;a href=&quot;#tang2015line&quot;&gt;[5]&lt;/a&gt;，在此基础上利用用户标签网络和用户文本网络为用户注入标签信息，利用这种方式我们可以得到用户与标签的嵌入式表示，在预测用户标签任务中，效果优于同类方法。&lt;/p&gt;

&lt;h3 id=&quot;13-研究内容与研究方法&quot;&gt;1.3 研究内容与研究方法&lt;/h3&gt;

&lt;p&gt;本文主要围绕以下几项工作展开研究：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;传统无监督的词嵌入方法的研究: 包括CBOW模型和Skip-gram模型的设计思想、目标函数推导、以及基于Hierarchical Softmax和Negative Sampling的优化方法研究。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;大规模消息网络嵌入式表示方法的研究: 包括大规模消息网络中局部相似性(一阶相似)和全局相似性(二阶相似)的原理，以及
对这两种相似性的建模方法和负采样法对目标函数进行优化的研究。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当前社会化标签推荐方法的研究：包括基于矩阵和协同过滤的标签推荐方法、基于社会化标签共现频率聚类的社会化标签推荐方法、基于图论的标签推荐方法，以及他们的优缺点。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;针对异构网络的有监督的社会化标签推荐方法研究：包括有监督训练异构网络的思想、目标函数的提出、利用Negative Sampling方法对目标函数进行优化;提出大规模无向图异构网络联合训练算法、大规模无向图异构网路先训练后优化算法、大规模有向图异构网络联合训练算法、大规模有向图异构网络先训练后优化算法。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于海量微博数据集的研究：包括对海量微博数据集构建无标记网络(用户关系网络、用户转发网络)和有标记网络(用户标签网络、用户微博文本网络)并利用本文提出的异构网络的有监督的社会化标签推荐方法来得到用户及其标签的嵌入式表示来
验证方法的有效性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于Spark的大数据处理平台研究：包括Hadoop、Spark、Hive等大数据处理技术的研究；对5台物理服务器构建Spark大数据处理平台方法的研究。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;14-本文的组织安排&quot;&gt;1.4 本文的组织安排&lt;/h3&gt;
&lt;p&gt;本文分为六章，具体的内容组织如下：&lt;/p&gt;

&lt;p&gt;第一章：引言。给出课题的研究目的和意义，提出论文的主要目标与主要内容&lt;/p&gt;

&lt;p&gt;第二章：国内外研究现状。 给出社交网络中几个热门研究方向及其研究进展，这几个热门研究方向包括：社会化标签系统、个性化推荐系统、社交网络社区分析、意见领袖挖掘等。&lt;/p&gt;

&lt;p&gt;第三章: 介绍传统无监督词嵌入表示方法和大规模消息网络的嵌入式表示方法的常见模型、原理、优化方法以及当今流行的大数据处理技术的架构、原理和特性。&lt;/p&gt;

&lt;p&gt;第四章: 介绍相关工作，阐述本文提出的异构网络的有监督的社会化标签推荐方法的原理、目标函数及其训练方法；详细介绍大规模无向图异构网络联合训练算法、大规模无向图异构网路先训练后优化算法、大规模有向图异构网络联合训练算法、大规模有向图异构网络先训练后优化算法；基于5台服务器搭建Spark大数据处理平台的方法总结。&lt;/p&gt;

&lt;p&gt;第五章: 基于海量微博数据训练第四章提出的4种模型，并介绍实验评价指标和分析实验结果。&lt;/p&gt;

&lt;p&gt;第六章: 论文总结，总结本文工作所取得的成果，并对下一步工作提出展望。&lt;/p&gt;

&lt;p&gt;最后部分是参考文献和致谢。&lt;/p&gt;

&lt;h2 id=&quot;2-国内外研究现状&quot;&gt;2. 国内外研究现状&lt;/h2&gt;
&lt;p&gt;本文对微博平台的使用主体也就是微博用户间的用户关系、用户标签、用户微博博文，以及微博用户转发等数据展开相关研究，针对微博用户方面的内容也有很多人进行了研究，主要从以下几个角度进行研究:&lt;/p&gt;

&lt;h3 id=&quot;21-面向微博用户关系模式信息推荐&quot;&gt;2.1 面向微博用户关系模式信息推荐&lt;/h3&gt;

&lt;p&gt;面向微博用户关系模式信息推荐的基本思想是首先建立用户和信息源之间以及用户和用户之间的对应关系，然后进行用户社群分析，建立相似用户群或兴趣共同体。当相同用户群的某个用户或者某几个用户对某信息或者商品感兴趣时，可以预测共同体的其他成员也感兴趣，从而将该信息推荐给其他成员。常用的分析方法是，通过分析用户社交网络中用户之间的相互关系，然后根据用户的不同关系进行基于内容的协同过滤推荐。在微博使用过程中，用户积极选择并参与构建个性化关系，与一些具有相似特征的用户自发的聚集到一起形成群体，用户社群分析作为用户关系挖掘的主要技术手段，他在常规复杂系统的研究中比较成熟。&lt;/p&gt;

&lt;h3 id=&quot;22-社会化标签系统&quot;&gt;2.2 社会化标签系统&lt;/h3&gt;
&lt;p&gt;标签是由用户在自由，不受约束环境下创造出来的，因此具有自由性和低限度的特点，当然标签系统的优点也往往正是它的缺点，标签具有一定的社会性和含糊性，也同时存在着例如同义词、多义词、一词多义甚至拼写错误的情况，所以导致了标签系统存在大量重复、不规范、无效的标签，我们称之为噪音。当用户对其感兴趣的资源进行标注标签行为的时候，规范、有效、高质量的标签会创造出标签系统的循环性，促进系统良性循环。&lt;/p&gt;

&lt;p&gt;社会化标签系统是Web用户利用社会化标签对Web资源进行标注的环境，它包括三个基本的实体，分别是Web用户，Web资源和社会化标签。另外，还包括一个关系集合。社会化标签系统的模型可以用一个四元组来表示\(F=(U,T,R,A)\),其中，\(U\)是Web用户的有限集合，\(T\)是社会标签的有限集合，\(R\)是Web资源的有限集合，\(A \subseteq U \times T \times R\)是一个三元关系集合，元素\(a=(u,t,r) \in A\)表示用户\(a\)使用标签\(t\)标注了资源\(r\)。标签共现分析是揭示标签语义关系的重要途径，Michlmayr和Cayzer &lt;a href=&quot;#michlmayr2007learning&quot;&gt;[6]&lt;/a&gt;指出，如果两个标签被某一用户结合或共同使用去标注某一个书签，那么这两个标签之间一定存在着某种语义关系。Szomszor等人&lt;a href=&quot;#ecs14007&quot;&gt;[7]&lt;/a&gt;通过实验表明标签共现关系的重要本质是能够用来揭示标签之间的语义关系,并利用Jaccard系数来衡量标签之间的共现关系。Kipp和Campbell &lt;a href=&quot;#kipp2006patterns&quot;&gt;[8]&lt;/a&gt;利用共词分析来抽取社会化书签服务系统delicious中的标签模型，他们发现标签的数量和使用频率之间遵循幂律分布，即只有少量的标签被经常用来标注资源而大部分标签被使用的次数较少。Begelman，Keller和Smadja等人&lt;a href=&quot;#begelman2006automated&quot;&gt;[9]&lt;/a&gt;使用标签聚类技术，提出了基于标签共现分布相似性的算法，并利用谱聚类实现了标签的聚类分析。王萍和张际平&lt;a href=&quot;#王萍2010一种社会性标签聚类算法&quot;&gt;[10]&lt;/a&gt;把标签共现定义为两个标签用来标注同一资源,并设计了一种基于标签相似性的聚类算法对标签共现网络进行分割,来建立标签聚类簇。&lt;/p&gt;

&lt;h3 id=&quot;23-社会化标签在个性化推荐领域的应用&quot;&gt;2.3 社会化标签在个性化推荐领域的应用&lt;/h3&gt;
&lt;p&gt;随着互联网技术的发展，用户的日常生活和互联网建立起了紧密的联系，与此同时，互联网上产生了海量用户数据，海量数据为个性化推荐系统创造了独一无二的优势，近几年，个性化推荐技术逐渐成为众多研究者的研究热点&lt;a href=&quot;#Xu:2006:CAS:2114193.2114262&quot;&gt;[11]&lt;/a&gt;&lt;a href=&quot;#yin2013connecting&quot;&gt;[12]&lt;/a&gt;。文献&lt;a href=&quot;#基于社会化标签的协同过滤推荐策略研究&quot;&gt;[13]&lt;/a&gt;和文献&lt;a href=&quot;#lacic2014recommending&quot;&gt;[14]&lt;/a&gt;均介绍了各种类型的成熟推荐技术，这些推荐技术各有利弊，分别适用于不同类型不同场景下的推荐系统。如Alexandrin Popescul等提出概率框架，合并基于内容和基于协同过滤的方法，加上EM算法学习的二次内容信息用于解决稀疏问题，辅助混合模型推荐。&lt;/p&gt;

&lt;p&gt;微博是Web2.0的重要应用，其中包含了丰富的网络和用户信息，在微博中标签是一种表示用户兴趣和属性的有效方式，一个用户的兴趣也通常隐藏在他/她的文本和网络中，Zhiyuan Liu提出一种概率模型，网络正则化的概率标签模型NTDM&lt;a href=&quot;#涂存超:24&quot;&gt;[15]&lt;/a&gt;,用来进行微博用户标签推荐，NTDM用来对微博个人介绍中的词和语义关系进行建模，同时将其所在的网络结构信息通过正则化的方式考虑进来，产生了很好的效果。
社会化标签技术对个性化推荐的精确、高效起到了推进作用&lt;a href=&quot;#kumar2014exploiting&quot;&gt;[16]&lt;/a&gt;，Chatti等在个人学习环境(PLE)设置不同的标签来研究基于标签的协同过滤算法，Godoy等实现以标签为基础的分类，其结果证明基于标签的分类优于那些使用文本、文档以及其他与内容相关的数据来源的推荐效果&lt;a href=&quot;#godoy2012one&quot;&gt;[17]&lt;/a&gt;；Yoshida等使用通过结合标签的排名和基于内容的过滤得出标签的相关性水平排名，从而提高项目推荐性能&lt;a href=&quot;#yoshida2012improving&quot;&gt;[18]&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;24-社交网络中关键用户挖掘分析&quot;&gt;2.4 社交网络中关键用户挖掘分析&lt;/h3&gt;
&lt;p&gt;关于微博用户所具有的结构差异特性，不同用户因其自身属性及其所在关系位置的不同，所以其在关系中所处的位置和交互方式也各不相同，并逐步形成一定的影响力，用户影响力分析主要研究如何基于用户的交互活动水平来研究用户与用户是如何相互影响以及研究用户在社交网络中影响力的大小。在社区中影响力大的用户是关键用户（或称意见领袖），能在一定程度上引导舆论，影响用户行为和政治观点等。&lt;/p&gt;

&lt;h2 id=&quot;3-模型介绍和大数据处理的相关技术&quot;&gt;3. 模型介绍和大数据处理的相关技术&lt;/h2&gt;

&lt;h3 id=&quot;31-分布式向量表示&quot;&gt;3.1 分布式向量表示&lt;/h3&gt;

&lt;h4 id=&quot;311-统计语言模型&quot;&gt;3.1.1 统计语言模型&lt;/h4&gt;
&lt;p&gt;word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;，它简单、高效，因此引起了很多人的关注，word2vec是用来生成词向量的工具，而词向量和语言模型有着密切的联系。当今的互联网迅猛发展，每天都在产生大量的文本，图片，语言和视频数据，要从这些数据处理并挖掘出有价值的信息，离不开自然语言处理（Nature Language Processing，NLP）技术，其中统计语言模型（Statistical Language Model）就是很重要的一环，它是所有NLP的基础，被广泛应用于语音识别，机器翻译，分词，词性标注和信息检索等任务。统计语言模型是用来计算一个句子的概率的概率模型，他通常基于一个语料库来构建。假设\(W = w_1^T :=(w_1,w_2,\cdots,w_T)\)表示由T个词\(w_1,w_2,\cdots,w_T\)按顺序构成一个句子，则\(w_1,w_2,\cdots,w_T\)的联合概率&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(W) = p(w_1^T) = p(w_1,w_2,\cdots,w_T)
\label{eq:eq9}
\end{equation}
就是这个句子的概率，利用Bayes公式，上式可以被链式分解为&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w_1^T) = p(w_1)\bullet p(w_2|w_1) \bullet p(w_3|w_1^2) \cdots p(w_T|w_1^{T-1})
\label{eq:eq10}
\end{equation}
其中的条件概率\(p(w_1)\bullet p(w_2|w_1) \bullet p(w_3|w_1^2) \cdots p(w_T|w_1^{T-1})\)就是语言模型的参数，那么给定一个句子\(w_1^T\)就可以很快地算出相应的\(p(w_1^T)\)了。
刚才我们考虑了一个给定长度为\(T\)的句子，就需要计算\(T\)个参数，假设对应词典\(D\)的大小(即词汇量)为\(N\),那么如果考虑长度为T的任意句子，总过就需要\(T \bullet N^T\)个参数，这些参数的量级是很大的。&lt;/p&gt;

&lt;h4 id=&quot;312-n-gram模型&quot;&gt;3.1.2 n-gram模型&lt;/h4&gt;
&lt;p&gt;n-gram模型可以用来计算上述的参数，考虑\(p(w_k|w_1^{k-1})(k &amp;gt; 1)\)的近似计算。利用Bayes公式有&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w_k|w_1^{k-1}) = {p(w_1^k) \over p(w_1^{k-1})}
\label{eq:eq11}
\end{equation}
根据大数定理，当语料库足够大时，
\begin{equation}
p(w_k|w_1^{k-1}) \approx {count(w_1^k) \over count(w_1^{k-1})}
\label{eq:eq12}
\end{equation}
其中\(count(w_1^k)\)和\(count(w_1^{k-1})\)在表示词串\(w_1^k\)和\(w_1^{k-1}\)语料中出现的次数,当k很大时，统计会很耗时。从公式\eqref{eq:eq10}可以看出，一个词出现的概率与它前面的所有词都相关。n-gram模型的基本思想是一个词出现的概率和它前面固定数目的词相关，它做了一个\(n-1\)阶的Markov假设，认为一个词出现的概率只与它前面的\(n-1\)个词相关，即
&lt;script type=&quot;math/tex&quot;&gt;p(w\_k|w\_1^{k-1}) \approx p(w\_k|w\_{k-n+1}^{k-1})&lt;/script&gt;
于是公式\eqref{eq:eq12}可以简化为
\begin{equation}
p(w_k|w_1^{k-1}) \approx {count(w_{k-n+1}^k) \over count(w_{k-n+1}^{k-1})}
\label{eq:eq13}
\end{equation}
这样简化，不仅使得参数统计变得更容易，也使得参数总数变得更少了。&lt;/p&gt;

&lt;h4 id=&quot;313-神经概率语言模型&quot;&gt;3.1.3 神经概率语言模型&lt;/h4&gt;
&lt;p&gt;本小节介绍Bengio等人提出的一种神经概率语言模型&lt;a href=&quot;#bengio2003neural&quot;&gt;[20]&lt;/a&gt;，该模型用到了一个重要的工具—词向量。对词典\(D\)中的任意词\(w\)指定一个任意长度的实值向量\(v(w) \in \Bbb R^m\),\(v(w)\)就称为\(w\)的词向量，m为词向量的长度。
图1给出了神经网络的结构示意图，它包括四个层：输入层(Input)，投影层(Projection)，隐藏层(Hidden)和输出层(Output)，其中\(W,U\)分别为投影层与隐藏层以及隐藏层和输出层之间的权值矩阵，\(p,q\)分别为隐藏层和输出层上的偏置向量。
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neuro-network.png&quot; alt=&quot;title for image&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图1 神经网络结构图&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;对于语料\(C\)中的任意一个词\(w\)，将Context(w)取为前\(n-1\)个词(类似于n-gram)，这样二元对\(Context(w),w\)就是一个训练样本了，接下来将讨论
样本\(Context(w),w\)经过如图1所示的神经网络时是如何参与运算的。一旦语料\(C\)和词向量的长度\(m\)给定后，投影层和输出层的规模就确定了，前者为\((n-1)m\),后者为\(N=|D|\),即语料C的词汇量大小，而隐藏层的规模\(n_n\)是可调参数，由用户指定。将输入层的\(n-1\)个词向量按顺序首尾相接地拼起来形成了一个长向量，其长度是\((n-1)m\),有了\(x_w\)了接下来的计算过程就很平凡了，具体为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left\{  
\begin{array}  
{l l}  
z_w &amp;=tanh(Wx_w + p)\\
y_w &amp;=Uz_w+q 
\end{array}  
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;其中tanh为双曲正切函数，用来做隐藏层的激活函数，上式中，tanh作用在向量上表示它作用在向量的每一分量上。经过上述两步计算得到的\(y_w =(y_{w,1},y_{w,2},\cdots,y_{w,N})\)只是一个长度为N的向量，其分量不能表示概率，如果想要\(y_w\)的分量\(y_{w,i}\)表示当上下文为\(Context(w)\)时下一个词恰为词典\(D\)中第i个词的概率，则还需要做一个softmax归一化，归一化后，\(p(w|Context(w))\)就可以表示为&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w|Context(w)) = {e^{y_{w,i_w}} \over \sum_{i=1}^N e^{y_{w,i}}}
\label{eq:eq14}
\end{equation}
其中\(i_w\)表示词\(w\)在词典\(D\)中的索引。
公式\eqref{eq:eq14}给出了概率\(p(w|Context(w))\)的函数表示，即找到了上一节中提到的函数\(F(w,Context(w),\theta)\),其中\(\theta\)是待确定的参数。\(theta\)有两部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词向量：\(v(w) \in \Re^m \) , \(w \in D\) 以及填充向量&lt;/li&gt;
  &lt;li&gt;神经网络参数：\(W \in \Bbb R^{n_h \times (n-1)m},p \in \Bbb R^{n_h};U \in R^{N \times n_h},q \in \Bbb R^N\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些参数均通过训练算法得到.值得一提的是，通常的机器学习算法中，输入都是已知的，而在上述神经概率语言模型中，输入\(v(w)\)也需要通过训练才能得到。&lt;/p&gt;

&lt;h4 id=&quot;314-词向量的理解&quot;&gt;3.1.4 词向量的理解&lt;/h4&gt;
&lt;p&gt;在NLP任务中，我们将自然语言交给机器学习算法来处理，但机器无法直接理解人类的语言，因此首先要做的事情就是将语言数字化，词向量提供了一种很好的方式。一种最简单的词向量是one-hot representation，他就是用一个很长的向量来表示一个词，向量的长度为词典\(D\)的大小N,向量的分量只有一个1，其余全为0,1的位置对应该词在词典中的索引。但是这种词向量表示又有一些缺点，容易受维数灾难的困扰，尤其是将其应用到Deep Learning的场景时。另一种词向量是Distributed Representation，它最早是Hinton于1986年提出的&lt;a href=&quot;#rumelhart1988learning&quot;&gt;[21]&lt;/a&gt;，可以克服one-hot representation的上述缺点，其基本思想是：通过训练将某种语言中的每一个词映射成一个固定长度的短向量，所有这些向量构成一个词向量空间，而每一个则可以视为该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断他们之间的相似性了。Word2vec采用的就是这种Distributed Representation的词向量。&lt;/p&gt;

&lt;h3 id=&quot;32-基于-hierarchical-softmax-的模型&quot;&gt;3.2 基于 Hierarchical Softmax 的模型&lt;/h3&gt;
&lt;p&gt;word2vec中用到了两个重要模型 - CBOW(Continuous Bag-of-Words Model)模型和Skip-gram模型(Continuous Skip-gram Model)，关于这两个模型，作者Tomas Mikolov在文&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;给出了如图2和图3所示的模型
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow.png&quot; alt=&quot;cbow&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图2 CBOW模型&lt;/p&gt;
    
&lt;/div&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram.png&quot; alt=&quot;skip-gram&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图3 Skip-gram模型&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;对于CBOW和Skip-gram两个模型，word2vec给出了两套框架，他们分别是基于Hierarchical Softmax和Negative Sampling来进行设计，本节介绍基于Hierarchical Softmax的CBOW和Skip-gram模型。在3.1节中我们提到基于神经网络的语言模型的目标函数通常取为如下对数似然函数&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} logP(w|Context(w))
\label{eq:eq15}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中的关键是条件概率函数\(P(w|Context(w))\)的构造，文&lt;a href=&quot;#bengio2003neural&quot;&gt;[20]&lt;/a&gt;中的模型就给出了这个函数的一种构造方法，即公式\eqref{eq:eq14}。对于word2vec中基于Hierarchical Softmax的CBOW模型，优化的目标函数也形如公式\eqref{eq:eq15}；而对于基于Hierarchical Softmax的Skip-gram模型，优化的目标函数则形如：&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} logP(Context(w)|w)
\label{eq:eq16}
\end{equation}&lt;/p&gt;

&lt;p&gt;下面将介绍\(p(w|Context(w))\)或者\(p(Context(w)|w)\)的构造。&lt;/p&gt;

&lt;h4 id=&quot;321-cbow模型&quot;&gt;3.2.1 CBOW模型&lt;/h4&gt;
&lt;p&gt;本小节介绍word2vec中的第一个模型—CBOW模型。&lt;/p&gt;
&lt;h5 id=&quot;3211-网络结构&quot;&gt;3.2.1.1 网络结构&lt;/h5&gt;
&lt;p&gt;图四给出了CBOW模型的网路结构，它包括三层:输入层、投影层、输出层。下面以样本\(Context(w),w\)为例(这里假设Context(w)由w前后各c个词构成)，下面对这三个层作简要说明。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;输入层&lt;/strong&gt;: 包含Context(w)中的2c个词的词向量\(v(Context(w)_1),v(Context(w)_2),\cdots,v(Context(w)_{2c}) \in R^m\),这里,m的含义同上表示词向量的长度。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;投影层&lt;/strong&gt;: 将输入层的2c个向量做求和累加，即\(x_w = \sum_{i=1}^2c v(Context(w)_i) \in R^m\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;输出层&lt;/strong&gt;: 输出层对应一棵二叉树，它是以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权值构造出来Huffman树，在这颗Huffman树中，叶子节点
共N(=|D|)个，分别对应词典D中的词，非叶子节点N-1个(图中标成黄色的那些顶点)。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对比神经概率语言模型的网络图(见图2和图3)和CBOW模型的结构图(见图4)，易知它们主要有以下三处不同:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(从输入层到投影层的操作) 前者是通过拼接，后者通过累加求和。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(隐藏层) 前者有隐藏层，后者无隐藏层。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(输出层) 前者是线性结构，后者树形结构。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow-net.png&quot; alt=&quot;cbow-net&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图4 CBOW模型的网络结构&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;在3.1.3节介绍的神经概率语言模型中，我们指出，模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算。而从上面的对比中可见，CBOW模型对这些计算复杂度高的地方有针对性的进行了改变，首先去掉了隐藏层，其次，输出层改用Huffman树，从而为利用Hierarchical softmax技术奠定了基础。&lt;/p&gt;

&lt;h5 id=&quot;3212-梯度计算&quot;&gt;3.2.1.2 梯度计算&lt;/h5&gt;

&lt;p&gt;Hierarchical Softmax是word2vec中用于提高性能的一项关键技术，为了描述方便起见，在具体介绍这个技术之前，先引入若干相关记号。考虑Huffman树中的某个叶子节点，假设它对应词典D中的词w，记&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(p^w\)：从根结点出发到达\(w\)对应&lt;/li&gt;
  &lt;li&gt;\(l^w\)：路径\(p^w\)中包含的结点的个数&lt;/li&gt;
  &lt;li&gt;\(p_q^w,p_2^w,\cdots,p_{l^w}^w\): 路径\(p^w\)中的\(l^w\)个结点,其中\(p_1^w\)表示根节点,\(p_{l^w}^w\)表示词w对应的结点。&lt;/li&gt;
  &lt;li&gt;\(d_2^w,d_3^w,\cdots,d_{l^w}^w \in {0,1}\)：词w的Huffman编码，它由\(l^w-1\)位编码构成，\(d_j^w\)表示路径\(p^w\)中第j个结点对应的编码(根节点不对应编码)。&lt;/li&gt;
  &lt;li&gt;\(\theta_1^w,\theta_2^w,\cdots,\theta \in R^m\)：路径\(p^w\)中非叶子结点对应的向量,\(\theta_j^w\)表示路径\(p^w\)中第j个非叶子结点对应的向量。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;下面介绍如何利用\(x_w \in R^m\) 以及Huffman树来定义函数\(p(w|Context(w))\),我们从二分类的角度考虑问题，那么对于每一个非叶子结点，就需要为其左右孩子结点指定一个类别，即哪一个是正类(标签为1)，哪一个是负类(标签为0)。碰巧，除了根节点以外，树中每个结点都对应了一个取值为0或1的Huffman编码。因此，一种最自然的做法就是将Huffman编码为0的结点定义为正类，编码为1的结点定义为
负类，word2vec选用的这个约定：&lt;/p&gt;

&lt;p&gt;\begin{equation}
Lable(P_i^w) = 1 - d_i^w, i = 2,3,4,\cdots,l^w
\label{eq:eq17}
\end{equation}
所以根据逻辑回归，一个结点分为正类的概率是\(\sigma(x_w^T \theta) = {1 \over 1 + e^{-x_w^T \theta}}\),被分为负类的概率为\(1-\sigma(x_w^T \theta)\),其中\(\theta\)是待定参数，非叶子结点对应的那些向量\(\theta_i^w\)就可以扮演参数\(\theta\)的角色。对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径\(p^w\)（且这条路径是唯一的）。路径\(p^w\)上存在\(l^w-1\)个分支，将每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来，就是所需的\(P(w|Context(w))\)。
条件概率&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w|Context(w)) = \prod_{j=2}^{l^w}p(d_j^w|X_w,\theta_{j-1}^w)
\label{eq:eq18}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(p(d_j^w|X_w,\theta_{j-1}^w) = [\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \bullet [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}\)
将公式\eqref{eq:eq18}带入公式\eqref{eq:eq15}得到&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{j=2}^{l^w}{(1-d_j^w)log[\sigma(X_w^T\theta_{j-1}^w)]+d_j^wlog[1-\sigma(X_w^T\theta_{j-1}^w)]}
\label{eq:eq19}
\end{equation}&lt;/p&gt;

&lt;p&gt;至此，已经推导出了对数似然函数\eqref{eq:eq19}，这个就是CBOW模型的目标函数，下面利用随机梯度下降法来优化这个目标函数，观察目标函数\eqref{eq:eq19}易知，该函数中的参数包括向量\(X_w,\theta_{j-1}^w,w \in C,j = 2,\cdots,l^w\)。通过求导可得&lt;/p&gt;

&lt;p&gt;\begin{equation}
\theta_{j-1}^w := \theta_{j-1}^w + \eta[1 - d_j^w - \sigma(X_w^T\theta_{j-1}^w)]X_w
\label{eq:eq20}
\end{equation}
其中\(\eta\)表示学习率，下同。&lt;/p&gt;

&lt;p&gt;\begin{equation}
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_{j=2}^{l^w} {\partial L(w,j) \over \partial X_w }, \widetilde{w} \in Context(w)
\label{eq:eq21}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中
\[
{\partial L(w,j) \over \partial X_w } = [1-d_j^w -\sigma(X_w^T\theta_{j-1}^w)]\theta_{j-1}^w
\]
下面以样本(Context(w),w)为例，给出了CBOW模型中采用随机梯度下降法更新各参数的伪代码
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow-pseudocode.png&quot; alt=&quot;cbow-pseudocode.png&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图5 CBOW模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;322-skip-gram模型&quot;&gt;3.2.2 Skip-gram模型&lt;/h4&gt;
&lt;p&gt;本小结介绍word2vec中的另一个模型—Skip-gram模型，由于推导过程与CBOW大同小异，因此会沿用上节引入的记号。&lt;/p&gt;
&lt;h5 id=&quot;3221-网络结构&quot;&gt;3.2.2.1 网络结构&lt;/h5&gt;

&lt;p&gt;图6给出了Skip-gram模型的网络结构，同CBOW模型网络结构也一样，它也包括三层：输入层，投影层，输出层，下面以样本\(w,Context(w)\)为例，对这三个层做简要说明。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;输入层&lt;/strong&gt;：只含有当前样本中心词\(w\)的词向量\(v(w) \in R^m\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;投影层&lt;/strong&gt;: 这是个恒等投影，把\(v(w)\)投影到\(v(w)\)。因此，这个投影层其实是多余的，这里之所以保留投影层只要是方便和CBOW模型的网络结构做对比。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;: 和CBOW模型一样，输出层也是一颗Huffman树&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram-net.png&quot; alt=&quot;skip-gram-net&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图6 Skip-gram模型的网络结构示意图&lt;/p&gt;
    
&lt;/div&gt;

&lt;h5 id=&quot;3222-梯度计算&quot;&gt;3.2.2.2 梯度计算&lt;/h5&gt;
&lt;p&gt;对于Skip-gram模型，已知的是当前词w，需要对其上下文Context(w)中的词进行预测，因此目标函数应该形如公式\eqref{eq:eq16},且关键是条件概率函数\(p(Context(w)|w)\)的构造，
Skip-gram模型将其定义为
\[
p(Context(w)|w) = \prod_{u \in Context(w)} p(u|w)
\]
上式中的\(p(u|w)\)可以按照上一小节介绍的Hierarchical Softmax思想，写为
\[
p(u|w) = \prod_{j=2}^{l^u} p(d_j^u|v(w),\theta_{j-1}^u)
\]
其中&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(d_j^u|v(w),\theta_{j-1}^u) = [\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}[1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}
\label{eq:eq22}
\end{equation}&lt;/p&gt;

&lt;p&gt;将公式\eqref{eq:eq22}依次带回目标函数\eqref{eq:eq16}可以得到对数似然函数的具体表达式&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{u \in Context(w)} \sum_{j=2}^{l^u}  (1-d_j^u)log[\sigma(v(w)^T\theta_{j-1}^u)]+d_j^ulog[1-\sigma(v(w)^T\theta_{j-1}^u)]
\label{eq:eq23}
\end{equation}
下面对目标函数求导后可知参数的更新公式为：
\[
\theta_{j-1}^u := \theta_{j-1}^u + \eta[1-d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]v(w)
\]&lt;/p&gt;

&lt;p&gt;\[
v(w) := v(w) + \eta \sum_{u \in Context(w)} \sum_{j=2}^{l^u} {\partial L(w,u,j) \over \partial v(w)}
\]&lt;/p&gt;

&lt;p&gt;其中
\[
 {\partial L(w,u,j) \over \partial v(w)} = [1 -d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]\theta_{j-1}^u
\]
下面以样本\(w,Context(w)\)为例，给出Skip-gram模型中采用随机梯度下降法更新各参数的伪代码
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram-pseudocode.png&quot; alt=&quot;skip-gram-pseudocode&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图7 Skip-gram模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h3 id=&quot;33-基于negative-sampling的模型&quot;&gt;3.3 基于Negative Sampling的模型&lt;/h3&gt;
&lt;p&gt;本节将介绍基于Negative Sampling的CBOW和Skip-gram模型。Negative Sampling(简称NEG)是Tomas Mikolov等人在&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;中提出的，它是NCE(Noise Contrastive Estimation)的一个简化版本，目的是用来提高训练速度并改善所得词向量的质量。与Hierarchical Softmax相比，NEG不再使用复杂的Huffman树，而是利用(相对简单的)随机负采样，能大幅提高性能，因此可作为Hierarchical Softmax的一种替代。&lt;/p&gt;

&lt;h4 id=&quot;331-cbow模型&quot;&gt;3.3.1 CBOW模型&lt;/h4&gt;
&lt;p&gt;在CBOW模型中，已知词w的上下文Context(w),需要预测w,因此对于给定的Context(w)，词w就是一个正样本，其他词就是负样本，现在假定已经选好了一个关于w的负样本子集\(NEG(w) \neq \emptyset \)
且对\(\forall \widetilde{w} \in D\),定义&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(n) =  
\begin{cases}  
1, &amp;\text{$\widetilde{w} = w$} \\[2ex]
0, &amp;\text{$\widetilde{w} \neq w$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;表示词\(\widetilde{w}\)的标签，即正样本标签为1，负样本标签为0.&lt;/p&gt;

&lt;p&gt;给定一个正样本(Context(w),w),我们希望最大化&lt;/p&gt;

&lt;p&gt;\begin{equation}
g(w) = \prod_{u \in {w}\bigcup NEG(w)} p(u|Context(w))
\label{eq:eq24}
\end{equation}
其中&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(u|Context(w)) =  
\begin{cases}  
\sigma(X_w^T\theta^u), &amp;\text{$L^w(u) =1$} \\[2ex]
1-\sigma(X_w^T\theta^u), &amp;\text{$L^w(u)=0$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;这里\(X_w\)仍表示Context(w)中各词的词向量之和，而\(\theta^u \in R^m\)表示词u对应的一个辅助向量，为待训练参数。负采样的思想是增大正样本的概率同时降低负样本的概率，于是，
对于一个给定的语料库\(C\),函数&lt;/p&gt;

&lt;p&gt;\[
G = \prod _{w \in C}g(w)
\]
就可以作为整体优化目标，为了计算方便，对G取对数，最终的目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{u \in {w} \bigcup NEG(w) }L^w(u)log[\sigma(x_w^T \theta^u)]+[1-L^w(u)]log[1- \sigma(x_w^T \theta^u)]
\label{eq:eq25}
\end{equation}&lt;/p&gt;

&lt;p&gt;利用随机梯度下降来计算参数的更新公式：
\[
\theta^u := \theta^u + \eta[L^w(u) -\sigma(x_w^T\theta^u)]x_w
\]&lt;/p&gt;

&lt;p&gt;\[
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_{u \in {w} \bigcup NEG(w)} {\partial L(w,u) \over \partial x_w} ,\widetilde{w} \in Context(w)
\]&lt;/p&gt;

&lt;p&gt;其中
\[
{\partial L(w,u) \over \partial x_w} = [L^w(u) - \sigma(x_w^T\theta^u)]\theta^u
\]
下面以样本(Context(w),w)为例，给出基于Negtive Sampling的CBOW模型中采用随机梯度下降法更新各参数的伪代码&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-cbow-code.png&quot; alt=&quot;neg-cbow-code&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图8 基于负采样的CBOW模型训练方法&lt;/p&gt;
    
&lt;/div&gt;
&lt;h4 id=&quot;332-skip-gram模型&quot;&gt;3.3.2 Skip-gram模型&lt;/h4&gt;
&lt;p&gt;本小节介绍基于Negative Sampling的Skip-gram模型，将CBOW下的目标函数改写为&lt;/p&gt;

&lt;p&gt;\begin{equation}
G = \prod_{w \in C} \prod_{u \in Context(w)} g(u)
\label{eq:eq26}
\end{equation}&lt;/p&gt;

&lt;p&gt;这里\(\prod_{u \in Context(w)} g(u)\)表示对于一个给定的样本(w,Context(w)),我们希望最大化的量，\(g(u)\)类似于上一节的\(g(w)\),定义为&lt;/p&gt;

&lt;p&gt;\begin{equation}
g(u) = \prod_ {z \in u \bigcup NEG(u)} p(z|w)
\label{eq:eq27}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中NEG(u)表示处理词u时生成的负样本子集，条件概率&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(z|w) =  
\begin{cases}  
\sigma(v(w)^T\theta^z), &amp;\text{$L^u(z) =1;$} \\[2ex]
1-\sigma(v(w)^T\theta^z), &amp;\text{$L^u(z)=0$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以最终的目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = logG = \sum_{w \in C} \sum_{\widetilde{w} \in Context(w)} \sum_ {u \in {w} \bigcup NEG^{\widetilde{w}}(w)}{L^w(u)log[\sigma(v(\widetilde{w})^T \theta^u)]+[1-L^w(u)]log[1-\sigma(v(\widetilde{w})^T \theta^u)]} 
\label{eq:eq28}
\end{equation}&lt;/p&gt;

&lt;p&gt;于是\(\theta^u\)的更新公式可写为
\[
\theta^u := \theta^u + \eta[L^w(u) - \sigma(v(\widetilde{w})^T\theta^u)]v(\widetilde{w})
\]&lt;/p&gt;

&lt;p&gt;\(v(\widetilde{w})\)的更新公式可以写为&lt;/p&gt;

&lt;p&gt;\[
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_ {u \in {w} \bigcup NEG^{\widetilde{w}}(w)} {\partial L(w,\widetilde{w},u) \over \partial v(\widetilde{w})}
\]
下面以样本(w,Context(w))为例,给出基于Negative Sampling的Skip-gram模型中采用随机梯度下降法更新各参数的伪代码&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-skip-gram-code.png&quot; alt=&quot;neg-skip-gram-code&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图9 基于负采样的Skip-gram模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;333-负采样算法&quot;&gt;3.3.3 负采样算法&lt;/h4&gt;
&lt;p&gt;在基于Negative Sampling的CBOW和Skip-gram模型中，负采样是个很重要的环节，对于一个给定的词\(w\)，如何生成\(NEG(w)\)呢？&lt;/p&gt;

&lt;p&gt;词典\(D\)中的词在语料\(C\)中出现的次数有高有低，对于那些高频词，被选为负样本的概率就比较大，反之，对于那些低频词，被其选中的概率就应该比较小，这就是采样过程中的一个大致要求，本质上是一个带权采样问题，在word2vec中，记\(l_0=0,l_k = \sum_{j=1}^klen(w_j),k=1,2,\cdots,N\),这里\(w_j\)表示词典D中的第j个词，则以\(\{l_j\}_{j=0}^N\)为剖结点可以得到区间\([0,1]\)上的一个非等距剖分，\(I_i = (l_{i-1},l_ i],i = 1,2,\cdots,N\)为其N个剖分区间。进一步引入区间\([0,1]\)上的一个等距离剖分，剖分结点为\(\{m_j\}_{j=0}^M\),其中M \(\gg\)N,具体见图10给出的示意图：&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-sample.png&quot; alt=&quot;neg-sample&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图10 Table(·)映射的建立示意图&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;将内部剖分节点\(\{m_j\}_{j=1}^{M-1}\)投影到非等距剖分上，如图10中虚线所示，则可建立\(\{m_j\}_{j=1}^{M-1}\)与区间\(\{I_j\}_{j=1}^N\)(或者说\(\{w_j\}_{j-1}^{M-1}\))的映射关系是
\[
Table(i) = w_ k, m_ i \in I_ k, i = 1,2,\cdots,M-1
\]
有了这个映射，采样的方法为：每次生成一个\([1,M-1]\)间的随机整数r，Table(r)就是一个样本，如果对\(w_i\)进行负采样时，如果碰巧选到\(w_i\)时，需要重新负采样，直到选非自身的值为止。&lt;/p&gt;

&lt;h3 id=&quot;34-大规模消息网络嵌入式表示&quot;&gt;3.4 大规模消息网络嵌入式表示&lt;/h3&gt;

&lt;p&gt;大规模消息网路嵌入式表示(Large-scale Information Network Embeding，简记LINE)主要是用来研究大规模消息网络结点间的关系，并用低维向量空间来表示该网络结构，这项技术广泛应用在多个领域，比如：数据可视化，结点分类，链接预测。LINE相比于
当前存在的图嵌入技术&lt;a href=&quot;#tenenbaum2000global&quot;&gt;[22]&lt;/a&gt; &lt;a href=&quot;#belkin2001laplacian&quot;&gt;[23]&lt;/a&gt;的主要优点是它可以适用于现实世界中的真实网络(拥有上百万顶点，上千万边的网络)。LINE适用于任意类型的消息网络，比如说，无向网络，有向网络，
带权重的网络。这种方法通过优化保留了局部和全局网络结构的目标函数来刻画消息网络的特征从而得到每一个结点的低维向量表示。局部网络结构，又称一阶相似性，捕获的是网络中两个顶点的链接关系，大部分图嵌入方法均可以保留一阶相似性，比如IsoMap&lt;a href=&quot;#tenenbaum2000global&quot;&gt;[22]&lt;/a&gt;。由于在现实世界的真实网络中，很多合理的链接并没有被捕获，仅仅通过一阶相似性不足以表示全局网络结构，文&lt;a href=&quot;#tang2015line&quot;&gt;[5]&lt;/a&gt;中提出了结点间的二阶相似性。二阶相似性通过判断两个结点间是否共享邻居来判断这两个结点是否相似，这个思想和我们直观上的想法—“我们可以通过某人的朋友来了解一个人”。&lt;/p&gt;

&lt;p&gt;下面举例说明，在图10中的消息网络中，边可以是有向的，无向的或者带权重的。顶点6和顶点7由于存在边直接连接，所以顶点6和顶点7存在一阶相似性。顶点5和顶点6由于共享相同的邻居(顶点1，顶点2，顶点3，顶点4),所以顶点5和顶点6具有二阶相似性。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/line-2-prox.png&quot; alt=&quot;line-2-prox&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图11 网络中的二阶相似性&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;下面具体来用数学公式来刻画一阶相似性和二阶相似性。&lt;/p&gt;

&lt;h4 id=&quot;341-一阶相似性&quot;&gt;3.4.1 一阶相似性&lt;/h4&gt;

&lt;p&gt;一阶相似性指的是网络中两个顶点的局部成对相似性，为了对一阶相似性建模，对于任意无向边\((i,j)\),我们定义了顶点\(v_i\)和\(v_j\)的联合概率如下：&lt;/p&gt;

&lt;p&gt;\begin{equation}
p_ 1(v_ i,v_ j) = {1 \over 1+exp(-\vec{u_ i}^T \cdot \vec{u _ j})}
\label{eq:eq29}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(\vec{u_ i} \in R^d\)是顶点\(v_ i\)的低维向量表示，\eqref{eq:eq29}定义了在\(V \times V\)空间上的一个概率分布\(p(\cdot,\cdot)\),它的经验分布为
\(\hat{p_ 1} = {w_ {ij} \over W}\),其中\(W = \sum _{(i,j) \in E} w_{ij}\),为了保留一阶相似性，一个最直接的想法就是最小化前面两个分布的距离，即目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_ 1 = - \sum _ {(i,j) \in E} w_ {ij} \log p_ 1(v_ i,v_ j) 
\label{eq:eq30}
\end{equation}&lt;/p&gt;

&lt;p&gt;一阶相似性只适合于无向图，不适合于有向图，通过找到\( \{\vec{u_ i}  \}_{i = 1,\cdots,|V|}\)来最小化目标函数\eqref{eq:eq30}，我们便可以得到每个顶点的\(d\)维向量表示。&lt;/p&gt;

&lt;h4 id=&quot;342-二阶相似性&quot;&gt;3.4.2 二阶相似性&lt;/h4&gt;
&lt;p&gt;二阶相似性适用于无向图和有向图，为了不损失一般性，我们考虑一个有向图网络(无向边可以看做两个具有相同权重但是方向相反的有向边)，二阶相似性认为如果两个顶点共享邻居结点的话，那么这两个结点相似。在这种情况下，每一个顶点被指定一个上下文(Context)，如果顶点在上下文上具有相似的分布，那么认为这两个顶点是相似的。
因此，一个顶点担任两个角色，第一，顶点本身；第二，其他顶点的上下文(Context)。所以我们引入两个向量\(\vec{u_i} 和 {\vec{u_i}}^{\prime}\)，其中\(\vec{u_i}\)是\(v_i\)的顶点表示，\({\vec{u_i}}^{\prime}\)是\(v_i\)的上下文表示，对于每一个有向边(i,j)，我们定义通过顶点\(v_i\)生成上下文\(v_j\)的概率如下&lt;/p&gt;

&lt;p&gt;\begin{equation}
p_2(v_ j|v_ i) = {exp({\vec{u_ j}^{\prime}}^T \cdot \vec{u_ i} ) \over \sum _ {k=1} ^{|V|} exp({\vec{u_ k}^{\prime}}^T \cdot \vec{u_ i} )}
\label{eq:eq31}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(|V|\)是顶点或者上下文的数量，对于每一个顶点\(v_i\)，公式\eqref{eq:eq31}定义了在\(v_i\)在上下文下的条件概率分布\(p_2(\cdot|v_ i)\),正如上文所说，二阶相似性表示的是，如果顶点在上下文上具有相似的概率分布，那么这些顶点相似。为了保留二阶相似性，我们应使\(p_2(\cdot|v_i)\)与经验分布\(\hat{p_2}(\cdot|v_i)\)距离最近,因此我们定义如下目标函数。&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_2 = \sum_{i \in V} \lambda_ i d(p_2(\cdot|v_i),\hat{p_2}(\cdot|v_i))
\label{eq:eq32}
\end{equation}
其中\(d(\cdot,\cdot)\)表示两个概率分布之间的距离，由于网络中顶点之间的重要性不同，\(\lambda_i\)表示每个顶点的重要性，它可以用每个顶点的度或者PageRank&lt;a href=&quot;#page1999pagerank&quot;&gt;[24]&lt;/a&gt;,其中\(\hat{p_2}(v_j|v_ i) = {w_{ij} \over d_ i}\),\(w_{ij}\)是边(i,j)的权重，\(d_i\)是顶点i的出度，我们通过KL距离来计算\(d(\cdot,\cdot)\)并且\(\lambda_i = d_i\),目标函数可以简化为
\begin{equation}
O_2 = - \sum _{(i,j) \in E} w_{ij}\log p_2(v_j|v_i)
\label{eq:eq33}
\end{equation}&lt;/p&gt;

&lt;p&gt;通过学习出\(\{\vec{u_ i}\}_{i = 1 \cdots |V|}\) 和\(\{\vec{u_ i}^{\prime}\}_{i = 1 \cdots |V|}\)使目标函数达到最小，
我们便可以用一个d维向量\(\vec{u_ i}\)表示每一个顶点\(v_i\)。&lt;/p&gt;

&lt;h3 id=&quot;35-spark&quot;&gt;3.5 Spark&lt;/h3&gt;

&lt;p&gt;Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。2009年由加州大学伯克利分校AMPLab实验室开发，并于2010年成为Apache的开源项目之一。Spark与Hadoop，Storm和MapReduce等其他大数据技术相比，Spark有很多优势，首先，Spark为我们提供了一个全面、统一的框架用于管理各种不同性质(文本数据、图表数据等)的数据集和数据源(批量数据或实时流数据)的大数据处理的需求。Spark可以将运行在Hadoop集群中的应用在内存中运行并且其速度提升100倍，甚至可以将应用在磁盘上的运行速度提升10倍。开发者可以快速的使用Java、Scala或者Python编写程序并部署在Spark集群中，它本身自带了一个超过80个高阶操作符集合，可以用它在shell中交互式的查询数据。Spark除了Map和Reduce操作之外，还支持SQL查询、流数据、机器学习、和图表数据处理，开发者可以在一个数据管道用例中单独使用某一操作符或者将这些操作符结合在一起使用。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/spark-structure.png&quot; alt=&quot;spark-structure&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图12 Spark体系架构模型中各个组件&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;351-hadoop和spark&quot;&gt;3.5.1 Hadoop和Spark&lt;/h4&gt;
&lt;p&gt;Hadoop这项大数据处理技术已有十多年历史，而且被看做是首选的大数据集合处理的解决方案。MapReduce是一路计算的优秀解决方案，不过对于需要多路计算的算法来说，并非十分高效。数据处理流程中的每一步都需要一个Map阶段和一个Reduce阶段，而且如果要利用这一解决方案，需要将所有用例都转换成MapReduce模式。在下一步开始之前，上一步的作业输出数据必须要存储到分布式文件系统中。因此，复制和磁盘存储会导致这种方式速度变慢。另外Hadoop解决方案中通常会包含难以安装和管理的集群。而且为了处理不同的大数据用例，还需要集成多种不同的工具，比如用于机器学习的Mahout和流数据处理的Storm。如果想要完成比较复杂的工作，就必须将一系列的MapReduce作业串联起来然后顺序执行这些作业。每一个作业都是高时延的，而且只有在前一个作业完成之后下一个作业才能开始启动。&lt;/p&gt;

&lt;p&gt;Spark则允许程序开发者使用有向无环图（DAG）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。Spark运行在现有的Hadoop分布式文件系统基础之上（HDFS）提供额外的增强功能。它支持将Spark应用部署到现存的Hadoop v1集群（with SIMR – Spark-Inside-MapReduce）或Hadoop v2 YARN集群甚至是Apache Mesos之中。我们应该将Spark看作是Hadoop MapReduce的一个替代品而不是Hadoop的替代品。其意图并非是替代Hadoop，而是为了提供一个管理不同的大数据用例和需求的全面且统一的解决方案。&lt;/p&gt;

&lt;h4 id=&quot;352-spark的特性&quot;&gt;3.5.2 Spark的特性&lt;/h4&gt;
&lt;p&gt;Spark通过在数据处理过程中成本更低的洗牌（Shuffle）方式，将MapReduce提升到一个更高的层次。利用内存数据存储和接近实时的处理能力，Spark比其他的大数据处理技术的性能要快很多倍。
Spark还支持大数据查询的延迟计算，这可以帮助优化大数据处理流程中的处理步骤。Spark还提供高级的API以提升开发者的生产力，除此之外还为大数据解决方案提供一致的体系架构模型。
Spark将中间结果保存在内存中而不是将其写入磁盘，当需要多次处理同一数据集时，这一点特别实用。Spark的设计初衷就是既可以在内存中又可以在磁盘上工作的执行引擎。当内存中的数据不适用时，Spark操作符就会执行外部操作。Spark可以用于处理大于集群内存容量总和的数据集。
Spark会尝试在内存中存储尽可能多的数据然后将其写入磁盘。它可以将某个数据集的一部分存入内存而剩余部分存入磁盘。开发者需要根据数据和用例评估对内存的需求。Spark的性能优势得益于这种内存中的数据存储。&lt;/p&gt;

&lt;p&gt;Spark的其他特性包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;支持比Map和Reduce更多的函数。&lt;/li&gt;
  &lt;li&gt;优化任意操作算子图（operator graphs）。&lt;/li&gt;
  &lt;li&gt;可以帮助优化整体数据处理流程的大数据查询的延迟计算。&lt;/li&gt;
  &lt;li&gt;提供简明、一致的Scala，Java和Python API。&lt;/li&gt;
  &lt;li&gt;提供交互式Scala和Python Shell。目前暂不支持Java。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;353-spark生态系统&quot;&gt;3.5.3 Spark生态系统&lt;/h4&gt;
&lt;p&gt;除了Spark核心API之外，Spark生态系统中还包括其他附加库，可以在大数据分析和机器学习领域提供更多的能力。这些库包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark Streaming: Spark Streaming基于微批量方式的计算和处理，可以用于处理实时的流数据。它使用DStream，简单来说就是一个弹性分布式数据集（RDD）系列，处理实时数据&lt;/li&gt;
  &lt;li&gt;Spark SQL: Spark SQL可以通过JDBC API将Spark数据集暴露出去，而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询。用户还可以用Spark SQL对不同格式的数据（如JSON，Parquet以及数据库等）执行ETL，将其转化，然后暴露给特定的查询。&lt;/li&gt;
  &lt;li&gt;Spark MLlib: MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。&lt;/li&gt;
  &lt;li&gt;Spark GraphX: GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了这些库以外，还有一些其他的库，如BlinkDB和Tachyon。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BlinkDB&lt;/strong&gt;是一个近似查询引擎，用于在海量数据上执行交互式SQL查询。BlinkDB可以通过牺牲数据精度来提升查询响应时间。通过在数据样本上执行查询并展示包含有意义的错误线注解的结果，操作大数据集合。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tachyon&lt;/strong&gt;是一个以内存为中心的分布式文件系统，能够提供内存级别速度的跨集群框架（如Spark和MapReduce）的可信文件共享。它将工作集文件缓存在内存中，从而避免到磁盘中加载需要经常读取的数据集。通过这一机制，不同的作业/查询和框架可以以内存级的速度访问缓存的文件。
此外，还有一些用于与其他产品集成的适配器，如Cassandra（Spark Cassandra 连接器）和R（SparkR）。Cassandra Connector可用于访问存储在Cassandra数据库中的数据并在这些数据上执行数据分析。&lt;/p&gt;

&lt;h3 id=&quot;36-hive&quot;&gt;3.6 HIVE&lt;/h3&gt;
&lt;p&gt;Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。&lt;/p&gt;

&lt;p&gt;下面是Hive的架构图：&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/hive-structure.png&quot; alt=&quot;hive-structure&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图13 HIVE体系架构&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;由上图可知，hadoop和mapreduce是hive架构的根基。Hive架构包括如下组件：CLI（command line interface）、JDBC/ODBC、Thrift Server、WEB GUI、metastore和Driver(Complier、Optimizer和Executor)，这些组件可以分为两大类：服务端组件和客户端组件。&lt;/p&gt;

&lt;p&gt;服务端组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Driver组件：该组件包括Complier、Optimizer和Executor，它的作用是将我们写的HiveQL（类SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的mapreduce计算框架。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metastore组件：元数据服务组件，这个组件存储hive的元数据，hive的元数据存储在关系数据库里，hive支持的关系数据库有derby、mysql。元数据对于hive十分重要，因此hive支持把metastore服务独立出来，安装到远程的服务器集群里，从而解耦hive服务和metastore服务，保证hive运行的健壮性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thrift服务：Thrift是facebook开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive集成了该服务，能让不同的编程语言调用hive的接口。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;客户端组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CLI：command line interface，命令行接口。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thrift客户端：上面的架构图里没有写上Thrift客户端，但是hive架构的许多客户端接口是建立在thrift客户端之上，包括JDBC和ODBC接口。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WEBGUI：hive客户端提供了一种通过网页的方式访问hive所提供的服务。这个接口对应hive的hwi组件（hive web interface），使用前要启动hwi服务。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-主要工作&quot;&gt;4. 主要工作&lt;/h2&gt;

&lt;h3 id=&quot;41-spark大数据处理平台的搭建&quot;&gt;4.1 Spark大数据处理平台的搭建&lt;/h3&gt;

&lt;p&gt;为处理海量微博数据，我申请了五台机器搭建了Spark大数据处理集群，集群中机器配置如下，&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;机器IP&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;结点类别&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;内存&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;硬盘&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;CPU&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.33&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;master&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.34&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.35&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.36&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.37&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在该集群中，我选用的Hadoop的版本为2.6.1,Spark版本为1.5.0，Hive版本为1.2.1，主要利用分布式文件存储(Hdfs)存储海量微博数据，Hive的调度引擎是基于Yarn的，可以通过Hive解析器将查询语句转化成基于MapReduce的分布式程序，
本文中大部分数据处理工作(数据清洗、抽取、变形)利用Hive,大大提高了的效率，由于实验室机房大部分端口未开放，所以采用
Nginx反向代理技术将监控页面开放给用户.&lt;/p&gt;

&lt;p&gt;图14为Spark大数据平台监控界面，用户可以在此平台观察程序运行结果，各阶段运行时间，管理用户程序。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/spark-page.png&quot; alt=&quot;spark-page&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图14 Spark平台监控界面&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;图15为Hadoop大数据平台监控界面，用户可以在此平台查看基于MapReduce的程序的执行状态，执行结果，执行时间等相关信息。
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/yarn-page.png&quot; alt=&quot;yarn-page&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图15 Hadoop平台监控界面&lt;/p&gt;
    
&lt;/div&gt;

&lt;h3 id=&quot;42-异构网络的有监督的社会化标签推荐方法&quot;&gt;4.2 异构网络的有监督的社会化标签推荐方法&lt;/h3&gt;
&lt;p&gt;传统的无监督文本嵌入方法，比如Skip-gram，Paragraph Vectors可以学习到比较通用微博用户向量表示，但是这种方法对指定的任务不会产生很高的准确率，我们提出一种有监督的学习方法，它可以利用打标签的数据(用户-标签网络)和无标签数据(用户-用户网络)来产生对指定分类任务更有针对性的标签，从而提升用户分类的准确率。为了达到这个目标，我们需要对有标记数据和无标记数据进行统一表示。
下面来定义五个网络:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;User-User Network:用户关注关系网络,记为\(G_{uu}=(V,E_{uu})\)这个网络记录了用户之间的关注关系，\(V\)是爬取的微博用户的集合，\(E_{uu}\)是边的集合，表现的是用户之间的关注关系，如果两个用户之间有关注关系，这两个用户之间有一条边。用户-用户网络捕获了用户之间的关注关系，这个是用户嵌入表示，比如Skip-Gram，所需要的重要信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Forward Network：用户转发网络,记为\(G_{uf} = (V_1,E_{uf})\),这个网络记录了微博用户的转发关系，其中的转发特征是根据微博文本中的&lt;code class=&quot;highlighter-rouge&quot;&gt;//@&lt;/code&gt;来确定的，
\(V_1\)是爬取数据微博文本中含有转发关系的用户集合，\(V_1\)是\(V\)的子集，\(E_{uf}\)是用户与转发用户之间边的集合,这个网络隐含用户之间的关系，为了有监督的捕获用户标签的信息，我们需要定义用户标签网络,用户微博文本网络。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Tag Network：用户标签网络，记为\(G_{ut}=(V_2\bigcup T,E_{ut})\)。这个二部图网络记录了用户与用户的Tag之间的信息，包含的标记信息，用于作为标记数据。\(V_2\)是爬取的微博数据中带有标签的微博用户的集合，\(V_2\)是\(V\)的子集。\(T\)是爬取微博用户的标签的集合，\(E_{ut}\)是用户集合和标签集合之间边的集合。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Microblog Network: 用户微博文本网络,记为\(G_{um}=(V_3\bigcup T_1,E_{um})\)这个网络记录了微博文本中的标签信息,我们通过对微博文本分词并提取标签信息得到，其中\(V_3\)是通过分析用户发的微博得到用户集合，\(V_3\)是\(V\)的子集，
其中\(T_1\)是通过分析用户发的微博得到标签集合，它是\(T\)的子集,这个网络具有标记信息，可用于有监督学习。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heterogeneous User Tag Network：这个网络整合了无标记网络(User-User Network、User-Forward Network)和有标记网络(User-Tag Network、User-Microblog Network),它包含不同维度的用户信息，包含有标记数据和无标记数据两部分。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面来引入我们的研究内容：&lt;br /&gt;
Predictive User Tag embedding:针对海量微博数据(无标记的用户关系数据和有标记用户标签数据),我们通过Heterogeneous User Tag Network去得到用户以及Tag的低维表示。通过用户和Tag的低维表示，我们可以利用\( \vec{u}\bullet\vec{v}\)来排序来计算与用户最相关的Tag。&lt;/p&gt;

&lt;p&gt;下面介绍本文的训练方法：&lt;/p&gt;

&lt;p&gt;(1) Bipartite Network Embedding
LINE Model是Graph embedding的比较常用的方法，它可以训练大规模网络从而得到节点的嵌入式表示。LINE Model主要是解决同构网络的节点嵌入式表示，由于异构网络之间边的权重没有可比性，所以LINE Model不能直接应用于异构网路。我们利用了LINE Model 二阶相似的思想，有相似邻居的两个顶点是相似的，这两个顶点在低维空间中距离很近。给定一个二部图网络\(G=(V_A\bigcup{V_B},E)\)，其中\(V_A\)，\(V_B\)是两个不同类型的不相交的顶点集合，E是连接两个顶点集合之间边的集合。我们首先定义由在集合\(V_B\)中的顶点\(v_j\)生成\(V_A\)中\(v_i\)顶点的条件概率为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
  p(v_i|v_j) = {e^{\vec{u_i}\bullet\vec{u_j}}\over \sum_{i^{\prime} \in V_A} e^{\vec{u_i^{\prime}}\bullet \vec{u_j}}} 
  \label{eq:eq1}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(\vec u_i\)是\(v_i\)的嵌入式表示，\(\vec u_j\)是\(v_j\)的嵌入式表示，对于每一个在\(V_B\)中的顶点\(v_j\),方程\eqref{eq:eq1}定义了在&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_A\)&lt;/code&gt;中所有顶点上的一个条件分布&lt;code class=&quot;highlighter-rouge&quot;&gt;\(p(\bullet|v_j)\)&lt;/code&gt;。对于任意一对顶点&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_j\)&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_{j^{\prime}}\)&lt;/code&gt;,为了保留二阶相似性，我们可以使条件分布&lt;code class=&quot;highlighter-rouge&quot;&gt;\(p(\bullet|v_j)\)&lt;/code&gt;接近于
&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\hat p(\bullet|v_j)\)&lt;/code&gt;,所以我们可以通过最小化下面这个目标函数达到目标:
\begin{equation}
O = \sum_{j \in V_B} \lambda_j d(\hat p(\bullet|v_j),p(\bullet|v_j))
\label{eq:eq2}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中&lt;code class=&quot;highlighter-rouge&quot;&gt;\(d(\bullet,\bullet)\)&lt;/code&gt;是两个分布的KL距离,&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\lambda_j\)&lt;/code&gt;用来表示顶点&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_j\)&lt;/code&gt;在网络中的重要性，它可以定义为:&lt;code class=&quot;highlighter-rouge&quot;&gt;\(deg_j=\sum_i w_{ij}\)&lt;/code&gt;,经验分布
&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\hat p(\bullet|v_j) = {w_{ij}\over deg_j}\)&lt;/code&gt;忽略一些常数，目标函数\eqref{eq:eq2}可简化为：
\begin{equation}
O = - \sum_{(i,j) \in E} w_{ij} \log {p(v_j|v_i)}
\label{eq:eq3}
\end{equation}&lt;/p&gt;

&lt;p&gt;目标函数\eqref{eq:eq3}可以利用边采样[5]或者负采样[11]的随机梯度下降法进行优化求解。
我们可以将上文提到的四种单一网络中的无向边看成是两条有向边，然后&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_A\)&lt;/code&gt;可以看做源节点的集合，&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_B\)&lt;/code&gt;可以看做目的节点的集合，通过这样处理，我们可以将上文提到的四种单一网络视为二部图来处理，从而可以利用改模型进行求解&lt;/p&gt;

&lt;p&gt;(2) Heterogeneous User Tag Network由两个四个部图网络构成，其中无标记网络为User-User Network、User-Forward Network和有标记网络为User-Tag Network、User-Microblog Network。其中User节点集合被四个网络所共享，为了学习到四个网络结构的嵌入式表示，我们直觉上的方法是整体训练这四个二部图网络，即最小化下面的目标函数:
\begin{equation}
O_{total} = O_{uu} + O_{ut} + O_{uf} + O_ {um}
\label{eq:eq4}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中：
\begin{equation}
O_{uu} = - \sum_{(i,j) \in E_{uu}} \log p(v_i|v_j)
\label{eq:eq5}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{ut} = - \sum_{(i,j) \in E_{ut}} \log p(v_i|t_j)
\label{eq:eq6}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{uf} = - \sum_{(i,j) \in E_{uf}} \log p(v_i|f_j)
\label{eq:eq7}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{um} = - \sum_{(i,j) \in E_{um}} \log p(v_i|m_j)
\label{eq:eq8}
\end{equation}&lt;/p&gt;

&lt;p&gt;目标函数\eqref{eq:eq4}有多种优化方法，一种解决方式是同时训练有标记数据和无标记数据，我们称这种方式为联合训练(Joint training)；另一种方式是先训练无标记数据，得到用户的嵌入式表示，然后利用有标记数据进行调优(Pre-training + Fine-tuning)[12],下面是具体的训练过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/pte-algorithm-1-2.png&quot; alt=&quot;algorithm&quot; title=&quot;pte-algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在联合训练中，上面的四种网络（用户关注关系网络、用户转发网络、用户标签网络、用户微博文本网络）均被训练，优化\eqref{eq:eq4}的一个方案是将\(G_{uu}，G_{uf}，G_{ut}，G_{um}\)中的所有边聚集在一起，然后使用边采样来更新模型，边采样的概率正比于边的权重，然而当网络是异构的，不同的网络结构之间边的权重是不兼容的，一个更好的解决方案是从四个边的集合中交替选择边采样，如上图中的算法\(1\)所示，相似的先训练后优化的算法细节如上图中的算法\(2\)所示。&lt;/p&gt;

&lt;h2 id=&quot;5-实验结果&quot;&gt;5. 实验结果&lt;/h2&gt;

&lt;h2 id=&quot;本文总结和对未来的工作展望&quot;&gt;本文总结和对未来的工作展望&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;mathes2004folksonomies&quot;&gt;[1]A. Mathes, “Folksonomies-cooperative classification and communication through shared metadata.” December, 2004.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;JiAT2007Collaborative&quot;&gt;[2]JiAT, “Collaborative Tagging in Recommender Systems,” in &lt;i&gt;AI 2007: Advances in Artificial Intelligence&lt;/i&gt;, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;shiratsuchi2006finding&quot;&gt;[3]K. Shiratsuchi, S. Yoshii, and M. Furukawa, “Finding unknown interests utilizing the wisdom of crowds in a social bookmark service,” in &lt;i&gt;Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology&lt;/i&gt;, 2006, pp. 421–424.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;clauset2005finding&quot;&gt;[4]A. Clauset, “Finding local community structure in networks,” &lt;i&gt;Physical review E&lt;/i&gt;, vol. 72, no. 2, p. 026132, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tang2015line&quot;&gt;[5]J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in &lt;i&gt;Proceedings of the 24th International Conference on World Wide Web&lt;/i&gt;, 2015, pp. 1067–1077.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;michlmayr2007learning&quot;&gt;[6]E. Michlmayr and S. Cayzer, “Learning user profiles from tagging data and leveraging them for personal (ized) information access,” 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ecs14007&quot;&gt;[7]M. Szomszor &lt;i&gt;et al.&lt;/i&gt;, “Folksonomies, the Semantic Web, and Movie Recommendation ,” in &lt;i&gt;4th European Semantic Web Conference, Bridging the Gap between Semantic Web and Web 2.0&lt;/i&gt;, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipp2006patterns&quot;&gt;[8]M. E. I. Kipp and D. G. Campbell, “Patterns and inconsistencies in collaborative tagging systems: An examination of tagging practices,” &lt;i&gt;Proceedings of the American Society for Information Science and Technology&lt;/i&gt;, vol. 43, no. 1, pp. 1–18, 2006.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;begelman2006automated&quot;&gt;[9]G. Begelman, P. Keller, F. Smadja, and others, “Automated tag clustering: Improving search and exploration in the tag space,” in &lt;i&gt;Collaborative Web Tagging Workshop at WWW2006, Edinburgh, Scotland&lt;/i&gt;, 2006, pp. 15–33.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;王萍2010一种社会性标签聚类算法&quot;&gt;[10]王萍 and 张际平, “一种社会性标签聚类算法,” &lt;i&gt;计算机应用与软件&lt;/i&gt;, vol. 27, no. 2, pp. 126–129, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Xu:2006:CAS:2114193.2114262&quot;&gt;[11]Y. Xu, L. Zhang, and W. Liu, “Cubic Analysis of Social Bookmarking for Personalized Recommendation,” in &lt;i&gt;Proceedings of the 8th Asia-Pacific Web Conference on Frontiers of WWW Research and Development&lt;/i&gt;, Berlin, Heidelberg, 2006, pp. 733–738.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yin2013connecting&quot;&gt;[12]D. Yin, S. Guo, B. Chidlovskii, B. D. Davison, C. Archambeau, and G. Bouchard, “Connecting comments and tags: improved modeling of social tagging systems,” in &lt;i&gt;Proceedings of the sixth ACM international conference on Web search and data mining&lt;/i&gt;, 2013, pp. 547–556.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;基于社会化标签的协同过滤推荐策略研究&quot;&gt;[13]万朔 and 邱会中, “基于社会化标签的协同过滤推荐策略研究,” &lt;i&gt;电子科技大学&lt;/i&gt;, pp. 14–16, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lacic2014recommending&quot;&gt;[14]E. Lacic, D. Kowald, P. Seitlinger, C. Trattner, and D. Parra, “Recommending items in social tagging systems using tag and time information,” &lt;i&gt;arXiv preprint arXiv:1406.7727&lt;/i&gt;, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;涂存超:24&quot;&gt;[15]涂存超 孙茂松, “社会媒体用户标签的分析与推荐,” &lt;i&gt;图书情报工作&lt;/i&gt;, vol. 57, no. 23, p. 24, 2013.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kumar2014exploiting&quot;&gt;[16]H. Kumar, S. Lee, and H.-G. Kim, “Exploiting social bookmarking services to build clustered user interest profile for personalized search,” &lt;i&gt;Information Sciences&lt;/i&gt;, vol. 281, pp. 399–417, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;godoy2012one&quot;&gt;[17]D. Godoy, “One-class support vector machines for personalized tag-based resource classification in social bookmarking systems,” &lt;i&gt;Concurrency and Computation: Practice and Experience&lt;/i&gt;, vol. 24, no. 17, pp. 2193–2206, 2012.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yoshida2012improving&quot;&gt;[18]T. Yoshida, G. Irie, T. Satou, A. Kojima, and S. Higashino, “Improving item recommendation based on social tag ranking,” in &lt;i&gt;International Conference on Multimedia Modeling&lt;/i&gt;, 2012, pp. 161–172.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mikolov2013distributed&quot;&gt;[19]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;, 2013, pp. 3111–3119.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bengio2003neural&quot;&gt;[20]Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language model,” &lt;i&gt;Journal of machine learning research&lt;/i&gt;, vol. 3, no. Feb, pp. 1137–1155, 2003.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rumelhart1988learning&quot;&gt;[21]D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” &lt;i&gt;Cognitive modeling&lt;/i&gt;, vol. 5, no. 3, p. 1, 1988.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tenenbaum2000global&quot;&gt;[22]J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” &lt;i&gt;science&lt;/i&gt;, vol. 290, no. 5500, pp. 2319–2323, 2000.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;belkin2001laplacian&quot;&gt;[23]M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in &lt;i&gt;NIPS&lt;/i&gt;, 2001, vol. 14, no. 14, pp. 585–591.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;page1999pagerank&quot;&gt;[24]L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web.,” Stanford InfoLab, 1999.&lt;/span&gt;



&lt;/li&gt;&lt;/ul&gt;
</description>
      </item>
    
      <item>
        <title>大规模社交用户的标签自动生成技术研究</title>
        <link>http://localhost:4000/2017/04/10/weibo-user-profile.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/10/weibo-user-profile.html</guid>
        <pubDate>Mon, 10 Apr 2017 00:00:00 +0800</pubDate>
        <description>&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;社会标签(Social Tagging)作为一种新型网络信息组织方式,由网络信息的提供者或者用户自发为某类信息赋予一定数量的标签,
选用自由词对感兴趣的网络信息资源进行描述来实现网络信息的分类,正在改变着传统的网络信息组织模式,
被广泛应用在社交网络、商业以及科研教育领域。社会标签较传统的主题词具有更大的灵活性、易用性,同时资源描述性关键词的增加也便于
对资源进行准确查找,尤其是对多媒体资源来说,用户所标注的标签信息就更为重要。&lt;/p&gt;

&lt;p&gt;Word2Vec从提出至今，已经广泛应用在自然语言处理、机器学习、数据挖掘等领域，从它延伸出的Doc2Vec、Graph2Vec也具有广泛的应用前景，
它已经成为了深度学习在自然语言处理中的基础部件，本文尝试利用大规模微博社交网络数据，提出将异构网络（微博用户关系网络、用户标签网络、用户文本网络）
进行联合训练的方法并得到用户与标签的嵌入式表示。这项技术可用于建模用户兴趣，推荐用户感兴趣的商品、信息和好友，进行用户画像，以及寻找目标用户进行商品推广。
本文提出的联合训练方法产生的用户与标签的嵌入式表示优于分别训练用户关系网络、用户标签网络产生的用户和标签的嵌入式表示，并在预测用户标签的准确率上优于同类方法。&lt;/p&gt;

&lt;p&gt;关键字：推荐系统，图嵌入表示，大数据&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Social Tagging is a new type of organizing the information of Internet. The users of Internet will provide some tags to
some objects spontaneously.We can use these tags to sort resources that we are interested in. Social Tagging is changing
the the method of the organization of network information resources and widely used in the social network service,business
and education. Social tagging is more flexible and convenient than traditional key words. With the increase of the number of 
social tags，it is more convenient to locate resource. Especially for multi-media resource，social tagging plays an increasingly important role.&lt;/p&gt;

&lt;p&gt;Word2vec has been widely applied in the fields of NLP，machine learning and data mining since it was first proposed by Mikolov. Doc2vec
and Graph2Vec，which are all originated from Word2vec，also have wide application foreground. Word2vec has become fundamental component in 
the field of NLP. This paper tries to use huge amounts of social networks data. In this paper, a new algorithm is presented, which can be
use to train heterogeneous network(user relationship network and user tags network and user text network). Through this algorithm,we can 
get embedding of both user and tag and then we can use these embedding in the fields，such as user interests modeling，Commodity recommendation and User portrait. The embedding produced by our algorithm is better than that produced by separately training each network.
Besides,in the case of predicting user tags，our algorithm is better than other similar methods.&lt;/p&gt;

&lt;p&gt;keywords:Recommendation System,Graph Embedding,Big Data&lt;/p&gt;

&lt;h2 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h2&gt;
&lt;h3 id=&quot;11-研究背景&quot;&gt;1.1 研究背景&lt;/h3&gt;

&lt;p&gt;微博作为一种新型媒体，是一种基于草根用户的关系构建个性化用户信息的即时传递、共享和获取平台。它具有信息实时性，内容简洁性，用户交互性等特点。
微博之所以可以成为当今国内外主流的社交媒体，主要是因为其具有强大的用户实时交互性，用户在使用微博的过程中，会在微博的网络空间中结成种种关系，
比如用户之间的关注关系，社区中的好友和亲情关系，实时交互过程中因共同购买或评论产品而结成的共同评论关系等，有效分析和挖掘微博中复杂的用户关系不仅可以激发、
助推和引导社会事件的发展趋势，还可以准确高效的为关注某一兴趣和爱好的微博群体进行个性化推荐，甚至可以大大降低企业和消费者的交易成本，推动企业营销模式的不断创新。
此外，微博在凝聚民心，降低事件危害以及政务互动等方面也发挥着不可替代的积极作用。由此可见，微博的兴起赋予了社会经济活动前所未有的大众化和网络化的内涵，
极大提升了社交媒体的社会服务效能。但是急剧增长的微博用户数量和海量用户下的交互行为增加了社会、经济与生产的复杂性，使一些社会实践变得更加不可预测，难以控制，
从而为分析社会化效应带来了新的挑战，因此如何正确理解微博用户之间的关系以及用户在关系交互中所产生的行为，成为学者迫切需要研究的新方向。&lt;/p&gt;

&lt;p&gt;社会化标签（Social Tagging） 也称为collaborative tagging，指的是用户在网络中自发得分配电子标签或关键词来描述网络上的资源，并且在网络用户群体中共享这些标签。这种方法允许用户使用自己的语言、以“标签”的形式对信息资源的内外部特征进行标注,以实现资源的查找和共享。社会化标签系统与预先定义网络资源类别
 来对网络资源分类的机制相比，它可以使用户自发产生和分配标签，这对网络资源分类有积极意义。随着社会化标签的广泛应用，公众分类法（folksonomy）应运生，这种分类方法改变了传统利用专家来对网络资源分类的方式，它是从公众的角度来分类资源。公众分类法是互联网所推崇的共享与协作精神的体现,是新的互联网信息环境中一种独具特色的信息组织工具。它的产生为互联网信息组织与检索的改进提供了新方向。随着社交网络，图片分享，视频分享业务的蓬勃发展，社会化标签系统被广泛应用在互联网的各个领域，它可以对社交网络和数据挖掘提供技术支持，而且有助于改进搜索结果，提高广告投放的准确率，同时利用所有用户产生的标签数据可以挖掘出其他有价值的信息，比如用户社群，潜在目标客户等。社会化标签系统将用户产生的大量通过网络传播而聚合起来，可以实现对网络资源的合作标记和公众分类，体现网络用户的群体智慧。&lt;/p&gt;

&lt;p&gt;在web2.0中，用户不仅可以通过豆瓣来分享图书,通过优酷来分享视频，通过微博来发表博文，通过Flicker来发布照片，通过youtube上传视频等方式来创造内容，
用户这些行为有一个共同的特征，即用户会自由的选择一些词(Term)或者短语(Phrase)来标注相关网络信息资源，
我们称用户的这种行为为标注(Tagging),用户所选择的词或者词语为标签(Tag),提供标注行为的系统为社会标签系统，
本文研究的就是利用微博用户关系与用户已有标签来为每个用户推荐相关的标签，便于建模用户兴趣，为用户之间的衔接赋予更丰富的信息，
推荐用户感兴趣的商品、信息和好友，进行用户画像，以及寻找目标用户进行商品推广。&lt;/p&gt;

&lt;h3 id=&quot;12-问题的提出&quot;&gt;1.2 问题的提出&lt;/h3&gt;
&lt;p&gt;在Web2.0环境下，互联网已经成为全球最大的知识库，它在给人类的生活和工作带来革命性变化的同时，也引发了“信息泛滥”，“信息迷航”等问题，社会化标签推荐能够根据用户的需求主动的将合适的信息、商品、知识提供给用户，可以有效缓解这些问题。同时，作为由用户产生的元数据，社会化标签能够独特反应用户的需求及其变化&lt;a href=&quot;#mathes2004folksonomies&quot;&gt;[1]&lt;/a&gt;，而且“用户-资源-社会化标签”之间的关系网络能够为个性化信息推荐系统提供十分有价值的基础数据，由此部分学者对基于社会化标签的个性化知识推荐进行了密切的关注，并从以下三个方面进行了探索。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于矩阵的方法，即通过构建“用户-资源”矩阵、“用户-社会化标签”矩阵，“社会化标签-资源”矩阵实现知识推荐。Ji AT等人依据“用户-资源”矩阵，“用户-标签”矩阵，“标签-资源”矩阵构建Naive Bayesian分类器，以此为基础实现协同过滤推荐&lt;a href=&quot;#JiAT2007Collaborative&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于聚类的方法,主要包括用户、资源、社会化标签三种对象的聚类，其中基于社会化标签的聚类是当前研究的重点。一个重要的研究思路就是一句社会化标签之前的共现频率，利用k-means聚类算法、马尔科夫聚类算法等方法对社会化标签进行聚类，进而依据聚类的结果为用户提供个性化推荐。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于图论的方法。其中社会网络分析是学者们关注的重点，如Shiratsuchi等人依据用户使用的标签之间的相似性
建立用户社会网络&lt;a href=&quot;#shiratsuchi2006finding&quot;&gt;[3]&lt;/a&gt;，并利用Clauset提出的local midularity算法&lt;a href=&quot;#clauset2005finding&quot;&gt;[4]&lt;/a&gt;划分网络社区，进而实现协同过滤推荐。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;显然，学者们提出的三类方法都有其优势，但是都面临各自的问题。首先第一类方法，面临一个重要的问题就是社会化标签使用量的“幂率分布”规律，排序在前几位的社会化标签具有较大的使用量，而大量的社会化标签都处于“长尾”区域，由此相关的矩阵可能非常不规则，从而严重制约个性化推荐算法。其次第二类方法中基于社会化标签的聚类方法属于基于内容的推荐思想，难以发现用户新的兴趣，而且社会化标签使用量的“幂律分布”问题同样会制约推荐效果。最后，第三类方法中基于社会网络分析的方法属于协同过滤思想，虽然能够发现用户新兴趣，但是个性化推荐需要依据用户的特定知识需求，在一般的社会网络中，个性间的“关系互动”并不意味着在特定的知识情境下必然能产生“知识互动”由此，当前学者们提出的基于社会化标签的各种推荐方法都有自身无法克服的劣势，如何利用社会化标签是实现精准的推荐是学者研究的重要问题，本文中提出一种有监督训练社会化标签的方法，我们对海量微博数据构建用户关系网络，用户转发网络，用户文本网络，用户标签网络，其中用户转发网络与用户关系网络刻画的是用户之间的交互关系，我们通过一阶相似性和二阶相似性找到相似用户&lt;a href=&quot;#tang2015line&quot;&gt;[5]&lt;/a&gt;，在此基础上利用用户标签网络和用户文本网络为用户注入标签信息，利用这种方式我们可以得到用户与标签的嵌入式表示，在预测用户标签任务中，效果优于同类方法。&lt;/p&gt;

&lt;h3 id=&quot;13-研究内容与研究方法&quot;&gt;1.3 研究内容与研究方法&lt;/h3&gt;

&lt;p&gt;本文主要围绕以下几项工作展开研究：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;传统无监督的词嵌入方法的研究: 包括CBOW模型和Skip-gram模型的设计思想、目标函数推导、以及基于Hierarchical Softmax和Negative Sampling的优化方法研究。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;大规模消息网络嵌入式表示方法的研究: 包括大规模消息网络中局部相似性(一阶相似)和全局相似性(二阶相似)的原理，以及
对这两种相似性的建模方法和负采样法对目标函数进行优化的研究。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当前社会化标签推荐方法的研究：包括基于矩阵和协同过滤的标签推荐方法、基于社会化标签共现频率聚类的社会化标签推荐方法、基于图论的标签推荐方法，以及他们的优缺点。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;针对异构网络的有监督的社会化标签推荐方法研究：包括有监督训练异构网络的思想、目标函数的提出、利用Negative Sampling方法对目标函数进行优化;提出大规模无向图异构网络联合训练算法、大规模无向图异构网路先训练后优化算法、大规模有向图异构网络联合训练算法、大规模有向图异构网络先训练后优化算法。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于海量微博数据集的研究：包括对海量微博数据集构建无标记网络(用户关系网络、用户转发网络)和有标记网络(用户标签网络、用户微博文本网络)并利用本文提出的异构网络的有监督的社会化标签推荐方法来得到用户及其标签的嵌入式表示来
验证方法的有效性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于Spark的大数据处理平台研究：包括Hadoop、Spark、Hive等大数据处理技术的研究；对5台物理服务器构建Spark大数据处理平台方法的研究。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;14-本文的组织安排&quot;&gt;1.4 本文的组织安排&lt;/h3&gt;
&lt;p&gt;本文分为六章，具体的内容组织如下：&lt;/p&gt;

&lt;p&gt;第一章：引言。给出课题的研究目的和意义，提出论文的主要目标与主要内容&lt;/p&gt;

&lt;p&gt;第二章：国内外研究现状。 给出社交网络中几个热门研究方向及其研究进展，这几个热门研究方向包括：社会化标签系统、个性化推荐系统、社交网络社区分析、意见领袖挖掘等。&lt;/p&gt;

&lt;p&gt;第三章: 介绍传统无监督词嵌入表示方法和大规模消息网络的嵌入式表示方法的常见模型、原理、优化方法以及当今流行的大数据处理技术的架构、原理和特性。&lt;/p&gt;

&lt;p&gt;第四章: 介绍相关工作，阐述本文提出的异构网络的有监督的社会化标签推荐方法的原理、目标函数及其训练方法；详细介绍大规模无向图异构网络联合训练算法、大规模无向图异构网路先训练后优化算法、大规模有向图异构网络联合训练算法、大规模有向图异构网络先训练后优化算法；基于5台服务器搭建Spark大数据处理平台的方法总结。&lt;/p&gt;

&lt;p&gt;第五章: 基于海量微博数据训练第四章提出的4种模型，并介绍实验评价指标和分析实验结果。&lt;/p&gt;

&lt;p&gt;第六章: 论文总结，总结本文工作所取得的成果，并对下一步工作提出展望。&lt;/p&gt;

&lt;p&gt;最后部分是参考文献和致谢。&lt;/p&gt;

&lt;h2 id=&quot;2-国内外研究现状&quot;&gt;2. 国内外研究现状&lt;/h2&gt;
&lt;p&gt;本文对微博平台的使用主体也就是微博用户间的用户关系、用户标签、用户微博博文，以及微博用户转发等数据展开相关研究，针对微博用户方面的内容也有很多人进行了研究，主要从以下几个角度进行研究:&lt;/p&gt;

&lt;h3 id=&quot;21-面向微博用户关系模式信息推荐&quot;&gt;2.1 面向微博用户关系模式信息推荐&lt;/h3&gt;

&lt;p&gt;面向微博用户关系模式信息推荐的基本思想是首先建立用户和信息源之间以及用户和用户之间的对应关系，然后进行用户社群分析，建立相似用户群或兴趣共同体。当相同用户群的某个用户或者某几个用户对某信息或者商品感兴趣时，可以预测共同体的其他成员也感兴趣，从而将该信息推荐给其他成员。常用的分析方法是，通过分析用户社交网络中用户之间的相互关系，然后根据用户的不同关系进行基于内容的协同过滤推荐。在微博使用过程中，用户积极选择并参与构建个性化关系，与一些具有相似特征的用户自发的聚集到一起形成群体，用户社群分析作为用户关系挖掘的主要技术手段，他在常规复杂系统的研究中比较成熟。&lt;/p&gt;

&lt;h3 id=&quot;22-社会化标签系统&quot;&gt;2.2 社会化标签系统&lt;/h3&gt;
&lt;p&gt;标签是由用户在自由，不受约束环境下创造出来的，因此具有自由性和低限度的特点，当然标签系统的优点也往往正是它的缺点，标签具有一定的社会性和含糊性，也同时存在着例如同义词、多义词、一词多义甚至拼写错误的情况，所以导致了标签系统存在大量重复、不规范、无效的标签，我们称之为噪音。当用户对其感兴趣的资源进行标注标签行为的时候，规范、有效、高质量的标签会创造出标签系统的循环性，促进系统良性循环。&lt;/p&gt;

&lt;p&gt;社会化标签系统是Web用户利用社会化标签对Web资源进行标注的环境，它包括三个基本的实体，分别是Web用户，Web资源和社会化标签。另外，还包括一个关系集合。社会化标签系统的模型可以用一个四元组来表示\(F=(U,T,R,A)\),其中，\(U\)是Web用户的有限集合，\(T\)是社会标签的有限集合，\(R\)是Web资源的有限集合，\(A \subseteq U \times T \times R\)是一个三元关系集合，元素\(a=(u,t,r) \in A\)表示用户\(a\)使用标签\(t\)标注了资源\(r\)。标签共现分析是揭示标签语义关系的重要途径，Michlmayr和Cayzer &lt;a href=&quot;#michlmayr2007learning&quot;&gt;[6]&lt;/a&gt;指出，如果两个标签被某一用户结合或共同使用去标注某一个书签，那么这两个标签之间一定存在着某种语义关系。Szomszor等人&lt;a href=&quot;#ecs14007&quot;&gt;[7]&lt;/a&gt;通过实验表明标签共现关系的重要本质是能够用来揭示标签之间的语义关系,并利用Jaccard系数来衡量标签之间的共现关系。Kipp和Campbell &lt;a href=&quot;#kipp2006patterns&quot;&gt;[8]&lt;/a&gt;利用共词分析来抽取社会化书签服务系统delicious中的标签模型，他们发现标签的数量和使用频率之间遵循幂律分布，即只有少量的标签被经常用来标注资源而大部分标签被使用的次数较少。Begelman，Keller和Smadja等人&lt;a href=&quot;#begelman2006automated&quot;&gt;[9]&lt;/a&gt;使用标签聚类技术，提出了基于标签共现分布相似性的算法，并利用谱聚类实现了标签的聚类分析。王萍和张际平&lt;a href=&quot;#王萍2010一种社会性标签聚类算法&quot;&gt;[10]&lt;/a&gt;把标签共现定义为两个标签用来标注同一资源,并设计了一种基于标签相似性的聚类算法对标签共现网络进行分割,来建立标签聚类簇。&lt;/p&gt;

&lt;h3 id=&quot;23-社会化标签在个性化推荐领域的应用&quot;&gt;2.3 社会化标签在个性化推荐领域的应用&lt;/h3&gt;
&lt;p&gt;随着互联网技术的发展，用户的日常生活和互联网建立起了紧密的联系，与此同时，互联网上产生了海量用户数据，海量数据为个性化推荐系统创造了独一无二的优势，近几年，个性化推荐技术逐渐成为众多研究者的研究热点&lt;a href=&quot;#Xu:2006:CAS:2114193.2114262&quot;&gt;[11]&lt;/a&gt;&lt;a href=&quot;#yin2013connecting&quot;&gt;[12]&lt;/a&gt;。文献&lt;a href=&quot;#基于社会化标签的协同过滤推荐策略研究&quot;&gt;[13]&lt;/a&gt;和文献&lt;a href=&quot;#lacic2014recommending&quot;&gt;[14]&lt;/a&gt;均介绍了各种类型的成熟推荐技术，这些推荐技术各有利弊，分别适用于不同类型不同场景下的推荐系统。如Alexandrin Popescul等提出概率框架，合并基于内容和基于协同过滤的方法，加上EM算法学习的二次内容信息用于解决稀疏问题，辅助混合模型推荐。&lt;/p&gt;

&lt;p&gt;微博是Web2.0的重要应用，其中包含了丰富的网络和用户信息，在微博中标签是一种表示用户兴趣和属性的有效方式，一个用户的兴趣也通常隐藏在他/她的文本和网络中，Zhiyuan Liu提出一种概率模型，网络正则化的概率标签模型NTDM&lt;a href=&quot;#涂存超:24&quot;&gt;[15]&lt;/a&gt;,用来进行微博用户标签推荐，NTDM用来对微博个人介绍中的词和语义关系进行建模，同时将其所在的网络结构信息通过正则化的方式考虑进来，产生了很好的效果。
社会化标签技术对个性化推荐的精确、高效起到了推进作用&lt;a href=&quot;#kumar2014exploiting&quot;&gt;[16]&lt;/a&gt;，Chatti等在个人学习环境(PLE)设置不同的标签来研究基于标签的协同过滤算法，Godoy等实现以标签为基础的分类，其结果证明基于标签的分类优于那些使用文本、文档以及其他与内容相关的数据来源的推荐效果&lt;a href=&quot;#godoy2012one&quot;&gt;[17]&lt;/a&gt;；Yoshida等使用通过结合标签的排名和基于内容的过滤得出标签的相关性水平排名，从而提高项目推荐性能&lt;a href=&quot;#yoshida2012improving&quot;&gt;[18]&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;24-社交网络中关键用户挖掘分析&quot;&gt;2.4 社交网络中关键用户挖掘分析&lt;/h3&gt;
&lt;p&gt;关于微博用户所具有的结构差异特性，不同用户因其自身属性及其所在关系位置的不同，所以其在关系中所处的位置和交互方式也各不相同，并逐步形成一定的影响力，用户影响力分析主要研究如何基于用户的交互活动水平来研究用户与用户是如何相互影响以及研究用户在社交网络中影响力的大小。在社区中影响力大的用户是关键用户（或称意见领袖），能在一定程度上引导舆论，影响用户行为和政治观点等。&lt;/p&gt;

&lt;h2 id=&quot;3-模型介绍和大数据处理的相关技术&quot;&gt;3. 模型介绍和大数据处理的相关技术&lt;/h2&gt;

&lt;h3 id=&quot;31-分布式向量表示&quot;&gt;3.1 分布式向量表示&lt;/h3&gt;

&lt;h4 id=&quot;311-统计语言模型&quot;&gt;3.1.1 统计语言模型&lt;/h4&gt;
&lt;p&gt;word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;，它简单、高效，因此引起了很多人的关注，word2vec是用来生成词向量的工具，而词向量和语言模型有着密切的联系。当今的互联网迅猛发展，每天都在产生大量的文本，图片，语言和视频数据，要从这些数据处理并挖掘出有价值的信息，离不开自然语言处理（Nature Language Processing，NLP）技术，其中统计语言模型（Statistical Language Model）就是很重要的一环，它是所有NLP的基础，被广泛应用于语音识别，机器翻译，分词，词性标注和信息检索等任务。统计语言模型是用来计算一个句子的概率的概率模型，他通常基于一个语料库来构建。假设\(W = w_1^T :=(w_1,w_2,\cdots,w_T)\)表示由T个词\(w_1,w_2,\cdots,w_T\)按顺序构成一个句子，则\(w_1,w_2,\cdots,w_T\)的联合概率&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(W) = p(w_1^T) = p(w_1,w_2,\cdots,w_T)
\label{eq:eq9}
\end{equation}
就是这个句子的概率，利用Bayes公式，上式可以被链式分解为&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w_1^T) = p(w_1)\bullet p(w_2|w_1) \bullet p(w_3|w_1^2) \cdots p(w_T|w_1^{T-1})
\label{eq:eq10}
\end{equation}
其中的条件概率\(p(w_1)\bullet p(w_2|w_1) \bullet p(w_3|w_1^2) \cdots p(w_T|w_1^{T-1})\)就是语言模型的参数，那么给定一个句子\(w_1^T\)就可以很快地算出相应的\(p(w_1^T)\)了。
刚才我们考虑了一个给定长度为\(T\)的句子，就需要计算\(T\)个参数，假设对应词典\(D\)的大小(即词汇量)为\(N\),那么如果考虑长度为T的任意句子，总过就需要\(T \bullet N^T\)个参数，这些参数的量级是很大的。&lt;/p&gt;

&lt;h4 id=&quot;312-n-gram模型&quot;&gt;3.1.2 n-gram模型&lt;/h4&gt;
&lt;p&gt;n-gram模型可以用来计算上述的参数，考虑\(p(w_k|w_1^{k-1})(k &amp;gt; 1)\)的近似计算。利用Bayes公式有&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w_k|w_1^{k-1}) = {p(w_1^k) \over p(w_1^{k-1})}
\label{eq:eq11}
\end{equation}
根据大数定理，当语料库足够大时，
\begin{equation}
p(w_k|w_1^{k-1}) \approx {count(w_1^k) \over count(w_1^{k-1})}
\label{eq:eq12}
\end{equation}
其中\(count(w_1^k)\)和\(count(w_1^{k-1})\)在表示词串\(w_1^k\)和\(w_1^{k-1}\)语料中出现的次数,当k很大时，统计会很耗时。从公式\eqref{eq:eq10}可以看出，一个词出现的概率与它前面的所有词都相关。n-gram模型的基本思想是一个词出现的概率和它前面固定数目的词相关，它做了一个\(n-1\)阶的Markov假设，认为一个词出现的概率只与它前面的\(n-1\)个词相关，即
&lt;script type=&quot;math/tex&quot;&gt;p(w\_k|w\_1^{k-1}) \approx p(w\_k|w\_{k-n+1}^{k-1})&lt;/script&gt;
于是公式\eqref{eq:eq12}可以简化为
\begin{equation}
p(w_k|w_1^{k-1}) \approx {count(w_{k-n+1}^k) \over count(w_{k-n+1}^{k-1})}
\label{eq:eq13}
\end{equation}
这样简化，不仅使得参数统计变得更容易，也使得参数总数变得更少了。&lt;/p&gt;

&lt;h4 id=&quot;313-神经概率语言模型&quot;&gt;3.1.3 神经概率语言模型&lt;/h4&gt;
&lt;p&gt;本小节介绍Bengio等人提出的一种神经概率语言模型&lt;a href=&quot;#bengio2003neural&quot;&gt;[20]&lt;/a&gt;，该模型用到了一个重要的工具—词向量。对词典\(D\)中的任意词\(w\)指定一个任意长度的实值向量\(v(w) \in \Bbb R^m\),\(v(w)\)就称为\(w\)的词向量，m为词向量的长度。
图1给出了神经网络的结构示意图，它包括四个层：输入层(Input)，投影层(Projection)，隐藏层(Hidden)和输出层(Output)，其中\(W,U\)分别为投影层与隐藏层以及隐藏层和输出层之间的权值矩阵，\(p,q\)分别为隐藏层和输出层上的偏置向量。
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neuro-network.png&quot; alt=&quot;title for image&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图1 神经网络结构图&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;对于语料\(C\)中的任意一个词\(w\)，将Context(w)取为前\(n-1\)个词(类似于n-gram)，这样二元对\(Context(w),w\)就是一个训练样本了，接下来将讨论
样本\(Context(w),w\)经过如图1所示的神经网络时是如何参与运算的。一旦语料\(C\)和词向量的长度\(m\)给定后，投影层和输出层的规模就确定了，前者为\((n-1)m\),后者为\(N=|D|\),即语料C的词汇量大小，而隐藏层的规模\(n_n\)是可调参数，由用户指定。将输入层的\(n-1\)个词向量按顺序首尾相接地拼起来形成了一个长向量，其长度是\((n-1)m\),有了\(x_w\)了接下来的计算过程就很平凡了，具体为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left\{  
\begin{array}  
{l l}  
z_w &amp;=tanh(Wx_w + p)\\
y_w &amp;=Uz_w+q 
\end{array}  
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;其中tanh为双曲正切函数，用来做隐藏层的激活函数，上式中，tanh作用在向量上表示它作用在向量的每一分量上。经过上述两步计算得到的\(y_w =(y_{w,1},y_{w,2},\cdots,y_{w,N})\)只是一个长度为N的向量，其分量不能表示概率，如果想要\(y_w\)的分量\(y_{w,i}\)表示当上下文为\(Context(w)\)时下一个词恰为词典\(D\)中第i个词的概率，则还需要做一个softmax归一化，归一化后，\(p(w|Context(w))\)就可以表示为&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w|Context(w)) = {e^{y_{w,i_w}} \over \sum_{i=1}^N e^{y_{w,i}}}
\label{eq:eq14}
\end{equation}
其中\(i_w\)表示词\(w\)在词典\(D\)中的索引。
公式\eqref{eq:eq14}给出了概率\(p(w|Context(w))\)的函数表示，即找到了上一节中提到的函数\(F(w,Context(w),\theta)\),其中\(\theta\)是待确定的参数。\(theta\)有两部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词向量：\(v(w) \in \Re^m \) , \(w \in D\) 以及填充向量&lt;/li&gt;
  &lt;li&gt;神经网络参数：\(W \in \Bbb R^{n_h \times (n-1)m},p \in \Bbb R^{n_h};U \in R^{N \times n_h},q \in \Bbb R^N\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些参数均通过训练算法得到.值得一提的是，通常的机器学习算法中，输入都是已知的，而在上述神经概率语言模型中，输入\(v(w)\)也需要通过训练才能得到。&lt;/p&gt;

&lt;h4 id=&quot;314-词向量的理解&quot;&gt;3.1.4 词向量的理解&lt;/h4&gt;
&lt;p&gt;在NLP任务中，我们将自然语言交给机器学习算法来处理，但机器无法直接理解人类的语言，因此首先要做的事情就是将语言数字化，词向量提供了一种很好的方式。一种最简单的词向量是one-hot representation，他就是用一个很长的向量来表示一个词，向量的长度为词典\(D\)的大小N,向量的分量只有一个1，其余全为0,1的位置对应该词在词典中的索引。但是这种词向量表示又有一些缺点，容易受维数灾难的困扰，尤其是将其应用到Deep Learning的场景时。另一种词向量是Distributed Representation，它最早是Hinton于1986年提出的&lt;a href=&quot;#rumelhart1988learning&quot;&gt;[21]&lt;/a&gt;，可以克服one-hot representation的上述缺点，其基本思想是：通过训练将某种语言中的每一个词映射成一个固定长度的短向量，所有这些向量构成一个词向量空间，而每一个则可以视为该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断他们之间的相似性了。Word2vec采用的就是这种Distributed Representation的词向量。&lt;/p&gt;

&lt;h3 id=&quot;32-基于-hierarchical-softmax-的模型&quot;&gt;3.2 基于 Hierarchical Softmax 的模型&lt;/h3&gt;
&lt;p&gt;word2vec中用到了两个重要模型 - CBOW(Continuous Bag-of-Words Model)模型和Skip-gram模型(Continuous Skip-gram Model)，关于这两个模型，作者Tomas Mikolov在文&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;给出了如图2和图3所示的模型
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow.png&quot; alt=&quot;cbow&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图2 CBOW模型&lt;/p&gt;
    
&lt;/div&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram.png&quot; alt=&quot;skip-gram&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图3 Skip-gram模型&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;对于CBOW和Skip-gram两个模型，word2vec给出了两套框架，他们分别是基于Hierarchical Softmax和Negative Sampling来进行设计，本节介绍基于Hierarchical Softmax的CBOW和Skip-gram模型。在3.1节中我们提到基于神经网络的语言模型的目标函数通常取为如下对数似然函数&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} logP(w|Context(w))
\label{eq:eq15}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中的关键是条件概率函数\(P(w|Context(w))\)的构造，文&lt;a href=&quot;#bengio2003neural&quot;&gt;[20]&lt;/a&gt;中的模型就给出了这个函数的一种构造方法，即公式\eqref{eq:eq14}。对于word2vec中基于Hierarchical Softmax的CBOW模型，优化的目标函数也形如公式\eqref{eq:eq15}；而对于基于Hierarchical Softmax的Skip-gram模型，优化的目标函数则形如：&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} logP(Context(w)|w)
\label{eq:eq16}
\end{equation}&lt;/p&gt;

&lt;p&gt;下面将介绍\(p(w|Context(w))\)或者\(p(Context(w)|w)\)的构造。&lt;/p&gt;

&lt;h4 id=&quot;321-cbow模型&quot;&gt;3.2.1 CBOW模型&lt;/h4&gt;
&lt;p&gt;本小节介绍word2vec中的第一个模型—CBOW模型。&lt;/p&gt;
&lt;h5 id=&quot;3211-网络结构&quot;&gt;3.2.1.1 网络结构&lt;/h5&gt;
&lt;p&gt;图四给出了CBOW模型的网路结构，它包括三层:输入层、投影层、输出层。下面以样本\(Context(w),w\)为例(这里假设Context(w)由w前后各c个词构成)，下面对这三个层作简要说明。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;输入层&lt;/strong&gt;: 包含Context(w)中的2c个词的词向量\(v(Context(w)_1),v(Context(w)_2),\cdots,v(Context(w)_{2c}) \in R^m\),这里,m的含义同上表示词向量的长度。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;投影层&lt;/strong&gt;: 将输入层的2c个向量做求和累加，即\(x_w = \sum_{i=1}^2c v(Context(w)_i) \in R^m\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;输出层&lt;/strong&gt;: 输出层对应一棵二叉树，它是以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权值构造出来Huffman树，在这颗Huffman树中，叶子节点
共N(=|D|)个，分别对应词典D中的词，非叶子节点N-1个(图中标成黄色的那些顶点)。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对比神经概率语言模型的网络图(见图2和图3)和CBOW模型的结构图(见图4)，易知它们主要有以下三处不同:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(从输入层到投影层的操作) 前者是通过拼接，后者通过累加求和。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(隐藏层) 前者有隐藏层，后者无隐藏层。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(输出层) 前者是线性结构，后者树形结构。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow-net.png&quot; alt=&quot;cbow-net&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图4 CBOW模型的网络结构&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;在3.1.3节介绍的神经概率语言模型中，我们指出，模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算。而从上面的对比中可见，CBOW模型对这些计算复杂度高的地方有针对性的进行了改变，首先去掉了隐藏层，其次，输出层改用Huffman树，从而为利用Hierarchical softmax技术奠定了基础。&lt;/p&gt;

&lt;h5 id=&quot;3212-梯度计算&quot;&gt;3.2.1.2 梯度计算&lt;/h5&gt;

&lt;p&gt;Hierarchical Softmax是word2vec中用于提高性能的一项关键技术，为了描述方便起见，在具体介绍这个技术之前，先引入若干相关记号。考虑Huffman树中的某个叶子节点，假设它对应词典D中的词w，记&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(p^w\)：从根结点出发到达\(w\)对应&lt;/li&gt;
  &lt;li&gt;\(l^w\)：路径\(p^w\)中包含的结点的个数&lt;/li&gt;
  &lt;li&gt;\(p_q^w,p_2^w,\cdots,p_{l^w}^w\): 路径\(p^w\)中的\(l^w\)个结点,其中\(p_1^w\)表示根节点,\(p_{l^w}^w\)表示词w对应的结点。&lt;/li&gt;
  &lt;li&gt;\(d_2^w,d_3^w,\cdots,d_{l^w}^w \in {0,1}\)：词w的Huffman编码，它由\(l^w-1\)位编码构成，\(d_j^w\)表示路径\(p^w\)中第j个结点对应的编码(根节点不对应编码)。&lt;/li&gt;
  &lt;li&gt;\(\theta_1^w,\theta_2^w,\cdots,\theta \in R^m\)：路径\(p^w\)中非叶子结点对应的向量,\(\theta_j^w\)表示路径\(p^w\)中第j个非叶子结点对应的向量。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;下面介绍如何利用\(x_w \in R^m\) 以及Huffman树来定义函数\(p(w|Context(w))\),我们从二分类的角度考虑问题，那么对于每一个非叶子结点，就需要为其左右孩子结点指定一个类别，即哪一个是正类(标签为1)，哪一个是负类(标签为0)。碰巧，除了根节点以外，树中每个结点都对应了一个取值为0或1的Huffman编码。因此，一种最自然的做法就是将Huffman编码为0的结点定义为正类，编码为1的结点定义为
负类，word2vec选用的这个约定：&lt;/p&gt;

&lt;p&gt;\begin{equation}
Lable(P_i^w) = 1 - d_i^w, i = 2,3,4,\cdots,l^w
\label{eq:eq17}
\end{equation}
所以根据逻辑回归，一个结点分为正类的概率是\(\sigma(x_w^T \theta) = {1 \over 1 + e^{-x_w^T \theta}}\),被分为负类的概率为\(1-\sigma(x_w^T \theta)\),其中\(\theta\)是待定参数，非叶子结点对应的那些向量\(\theta_i^w\)就可以扮演参数\(\theta\)的角色。对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径\(p^w\)（且这条路径是唯一的）。路径\(p^w\)上存在\(l^w-1\)个分支，将每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来，就是所需的\(P(w|Context(w))\)。
条件概率&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w|Context(w)) = \prod_{j=2}^{l^w}p(d_j^w|X_w,\theta_{j-1}^w)
\label{eq:eq18}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(p(d_j^w|X_w,\theta_{j-1}^w) = [\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \bullet [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}\)
将公式\eqref{eq:eq18}带入公式\eqref{eq:eq15}得到&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{j=2}^{l^w}{(1-d_j^w)log[\sigma(X_w^T\theta_{j-1}^w)]+d_j^wlog[1-\sigma(X_w^T\theta_{j-1}^w)]}
\label{eq:eq19}
\end{equation}&lt;/p&gt;

&lt;p&gt;至此，已经推导出了对数似然函数\eqref{eq:eq19}，这个就是CBOW模型的目标函数，下面利用随机梯度下降法来优化这个目标函数，观察目标函数\eqref{eq:eq19}易知，该函数中的参数包括向量\(X_w,\theta_{j-1}^w,w \in C,j = 2,\cdots,l^w\)。通过求导可得&lt;/p&gt;

&lt;p&gt;\begin{equation}
\theta_{j-1}^w := \theta_{j-1}^w + \eta[1 - d_j^w - \sigma(X_w^T\theta_{j-1}^w)]X_w
\label{eq:eq20}
\end{equation}
其中\(\eta\)表示学习率，下同。&lt;/p&gt;

&lt;p&gt;\begin{equation}
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_{j=2}^{l^w} {\partial L(w,j) \over \partial X_w }, \widetilde{w} \in Context(w)
\label{eq:eq21}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中
\[
{\partial L(w,j) \over \partial X_w } = [1-d_j^w -\sigma(X_w^T\theta_{j-1}^w)]\theta_{j-1}^w
\]
下面以样本(Context(w),w)为例，给出了CBOW模型中采用随机梯度下降法更新各参数的伪代码
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow-pseudocode.png&quot; alt=&quot;cbow-pseudocode.png&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图5 CBOW模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;322-skip-gram模型&quot;&gt;3.2.2 Skip-gram模型&lt;/h4&gt;
&lt;p&gt;本小结介绍word2vec中的另一个模型—Skip-gram模型，由于推导过程与CBOW大同小异，因此会沿用上节引入的记号。&lt;/p&gt;
&lt;h5 id=&quot;3221-网络结构&quot;&gt;3.2.2.1 网络结构&lt;/h5&gt;

&lt;p&gt;图6给出了Skip-gram模型的网络结构，同CBOW模型网络结构也一样，它也包括三层：输入层，投影层，输出层，下面以样本\(w,Context(w)\)为例，对这三个层做简要说明。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;输入层&lt;/strong&gt;：只含有当前样本中心词\(w\)的词向量\(v(w) \in R^m\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;投影层&lt;/strong&gt;: 这是个恒等投影，把\(v(w)\)投影到\(v(w)\)。因此，这个投影层其实是多余的，这里之所以保留投影层只要是方便和CBOW模型的网络结构做对比。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;: 和CBOW模型一样，输出层也是一颗Huffman树&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram-net.png&quot; alt=&quot;skip-gram-net&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图6 Skip-gram模型的网络结构示意图&lt;/p&gt;
    
&lt;/div&gt;

&lt;h5 id=&quot;3222-梯度计算&quot;&gt;3.2.2.2 梯度计算&lt;/h5&gt;
&lt;p&gt;对于Skip-gram模型，已知的是当前词w，需要对其上下文Context(w)中的词进行预测，因此目标函数应该形如公式\eqref{eq:eq16},且关键是条件概率函数\(p(Context(w)|w)\)的构造，
Skip-gram模型将其定义为
\[
p(Context(w)|w) = \prod_{u \in Context(w)} p(u|w)
\]
上式中的\(p(u|w)\)可以按照上一小节介绍的Hierarchical Softmax思想，写为
\[
p(u|w) = \prod_{j=2}^{l^u} p(d_j^u|v(w),\theta_{j-1}^u)
\]
其中&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(d_j^u|v(w),\theta_{j-1}^u) = [\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}[1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}
\label{eq:eq22}
\end{equation}&lt;/p&gt;

&lt;p&gt;将公式\eqref{eq:eq22}依次带回目标函数\eqref{eq:eq16}可以得到对数似然函数的具体表达式&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{u \in Context(w)} \sum_{j=2}^{l^u}  (1-d_j^u)log[\sigma(v(w)^T\theta_{j-1}^u)]+d_j^ulog[1-\sigma(v(w)^T\theta_{j-1}^u)]
\label{eq:eq23}
\end{equation}
下面对目标函数求导后可知参数的更新公式为：
\[
\theta_{j-1}^u := \theta_{j-1}^u + \eta[1-d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]v(w)
\]&lt;/p&gt;

&lt;p&gt;\[
v(w) := v(w) + \eta \sum_{u \in Context(w)} \sum_{j=2}^{l^u} {\partial L(w,u,j) \over \partial v(w)}
\]&lt;/p&gt;

&lt;p&gt;其中
\[
 {\partial L(w,u,j) \over \partial v(w)} = [1 -d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]\theta_{j-1}^u
\]
下面以样本\(w,Context(w)\)为例，给出Skip-gram模型中采用随机梯度下降法更新各参数的伪代码
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram-pseudocode.png&quot; alt=&quot;skip-gram-pseudocode&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图7 Skip-gram模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h3 id=&quot;33-基于negative-sampling的模型&quot;&gt;3.3 基于Negative Sampling的模型&lt;/h3&gt;
&lt;p&gt;本节将介绍基于Negative Sampling的CBOW和Skip-gram模型。Negative Sampling(简称NEG)是Tomas Mikolov等人在&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;中提出的，它是NCE(Noise Contrastive Estimation)的一个简化版本，目的是用来提高训练速度并改善所得词向量的质量。与Hierarchical Softmax相比，NEG不再使用复杂的Huffman树，而是利用(相对简单的)随机负采样，能大幅提高性能，因此可作为Hierarchical Softmax的一种替代。&lt;/p&gt;

&lt;h4 id=&quot;331-cbow模型&quot;&gt;3.3.1 CBOW模型&lt;/h4&gt;
&lt;p&gt;在CBOW模型中，已知词w的上下文Context(w),需要预测w,因此对于给定的Context(w)，词w就是一个正样本，其他词就是负样本，现在假定已经选好了一个关于w的负样本子集\(NEG(w) \neq \emptyset \)
且对\(\forall \widetilde{w} \in D\),定义&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(n) =  
\begin{cases}  
1, &amp;\text{$\widetilde{w} = w$} \\[2ex]
0, &amp;\text{$\widetilde{w} \neq w$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;表示词\(\widetilde{w}\)的标签，即正样本标签为1，负样本标签为0.&lt;/p&gt;

&lt;p&gt;给定一个正样本(Context(w),w),我们希望最大化&lt;/p&gt;

&lt;p&gt;\begin{equation}
g(w) = \prod_{u \in {w}\bigcup NEG(w)} p(u|Context(w))
\label{eq:eq24}
\end{equation}
其中&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(u|Context(w)) =  
\begin{cases}  
\sigma(X_w^T\theta^u), &amp;\text{$L^w(u) =1$} \\[2ex]
1-\sigma(X_w^T\theta^u), &amp;\text{$L^w(u)=0$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;这里\(X_w\)仍表示Context(w)中各词的词向量之和，而\(\theta^u \in R^m\)表示词u对应的一个辅助向量，为待训练参数。负采样的思想是增大正样本的概率同时降低负样本的概率，于是，
对于一个给定的语料库\(C\),函数&lt;/p&gt;

&lt;p&gt;\[
G = \prod _{w \in C}g(w)
\]
就可以作为整体优化目标，为了计算方便，对G取对数，最终的目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{u \in {w} \bigcup NEG(w) }L^w(u)log[\sigma(x_w^T \theta^u)]+[1-L^w(u)]log[1- \sigma(x_w^T \theta^u)]
\label{eq:eq25}
\end{equation}&lt;/p&gt;

&lt;p&gt;利用随机梯度下降来计算参数的更新公式：
\[
\theta^u := \theta^u + \eta[L^w(u) -\sigma(x_w^T\theta^u)]x_w
\]&lt;/p&gt;

&lt;p&gt;\[
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_{u \in {w} \bigcup NEG(w)} {\partial L(w,u) \over \partial x_w} ,\widetilde{w} \in Context(w)
\]&lt;/p&gt;

&lt;p&gt;其中
\[
{\partial L(w,u) \over \partial x_w} = [L^w(u) - \sigma(x_w^T\theta^u)]\theta^u
\]
下面以样本(Context(w),w)为例，给出基于Negtive Sampling的CBOW模型中采用随机梯度下降法更新各参数的伪代码&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-cbow-code.png&quot; alt=&quot;neg-cbow-code&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图8 基于负采样的CBOW模型训练方法&lt;/p&gt;
    
&lt;/div&gt;
&lt;h4 id=&quot;332-skip-gram模型&quot;&gt;3.3.2 Skip-gram模型&lt;/h4&gt;
&lt;p&gt;本小节介绍基于Negative Sampling的Skip-gram模型，将CBOW下的目标函数改写为&lt;/p&gt;

&lt;p&gt;\begin{equation}
G = \prod_{w \in C} \prod_{u \in Context(w)} g(u)
\label{eq:eq26}
\end{equation}&lt;/p&gt;

&lt;p&gt;这里\(\prod_{u \in Context(w)} g(u)\)表示对于一个给定的样本(w,Context(w)),我们希望最大化的量，\(g(u)\)类似于上一节的\(g(w)\),定义为&lt;/p&gt;

&lt;p&gt;\begin{equation}
g(u) = \prod_ {z \in u \bigcup NEG(u)} p(z|w)
\label{eq:eq27}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中NEG(u)表示处理词u时生成的负样本子集，条件概率&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(z|w) =  
\begin{cases}  
\sigma(v(w)^T\theta^z), &amp;\text{$L^u(z) =1;$} \\[2ex]
1-\sigma(v(w)^T\theta^z), &amp;\text{$L^u(z)=0$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以最终的目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = logG = \sum_{w \in C} \sum_{\widetilde{w} \in Context(w)} \sum_ {u \in {w} \bigcup NEG^{\widetilde{w}}(w)}{L^w(u)log[\sigma(v(\widetilde{w})^T \theta^u)]+[1-L^w(u)]log[1-\sigma(v(\widetilde{w})^T \theta^u)]} 
\label{eq:eq28}
\end{equation}&lt;/p&gt;

&lt;p&gt;于是\(\theta^u\)的更新公式可写为
\[
\theta^u := \theta^u + \eta[L^w(u) - \sigma(v(\widetilde{w})^T\theta^u)]v(\widetilde{w})
\]&lt;/p&gt;

&lt;p&gt;\(v(\widetilde{w})\)的更新公式可以写为&lt;/p&gt;

&lt;p&gt;\[
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_ {u \in {w} \bigcup NEG^{\widetilde{w}}(w)} {\partial L(w,\widetilde{w},u) \over \partial v(\widetilde{w})}
\]
下面以样本(w,Context(w))为例,给出基于Negative Sampling的Skip-gram模型中采用随机梯度下降法更新各参数的伪代码&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-skip-gram-code.png&quot; alt=&quot;neg-skip-gram-code&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图9 基于负采样的Skip-gram模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;333-负采样算法&quot;&gt;3.3.3 负采样算法&lt;/h4&gt;
&lt;p&gt;在基于Negative Sampling的CBOW和Skip-gram模型中，负采样是个很重要的环节，对于一个给定的词\(w\)，如何生成\(NEG(w)\)呢？&lt;/p&gt;

&lt;p&gt;词典\(D\)中的词在语料\(C\)中出现的次数有高有低，对于那些高频词，被选为负样本的概率就比较大，反之，对于那些低频词，被其选中的概率就应该比较小，这就是采样过程中的一个大致要求，本质上是一个带权采样问题，在word2vec中，记\(l_0=0,l_k = \sum_{j=1}^klen(w_j),k=1,2,\cdots,N\),这里\(w_j\)表示词典D中的第j个词，则以\(\{l_j\}_{j=0}^N\)为剖结点可以得到区间\([0,1]\)上的一个非等距剖分，\(I_i = (l_{i-1},l_ i],i = 1,2,\cdots,N\)为其N个剖分区间。进一步引入区间\([0,1]\)上的一个等距离剖分，剖分结点为\(\{m_j\}_{j=0}^M\),其中M \(\gg\)N,具体见图10给出的示意图：&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-sample.png&quot; alt=&quot;neg-sample&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图10 Table(·)映射的建立示意图&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;将内部剖分节点\(\{m_j\}_{j=1}^{M-1}\)投影到非等距剖分上，如图10中虚线所示，则可建立\(\{m_j\}_{j=1}^{M-1}\)与区间\(\{I_j\}_{j=1}^N\)(或者说\(\{w_j\}_{j-1}^{M-1}\))的映射关系是
\[
Table(i) = w_ k, m_ i \in I_ k, i = 1,2,\cdots,M-1
\]
有了这个映射，采样的方法为：每次生成一个\([1,M-1]\)间的随机整数r，Table(r)就是一个样本，如果对\(w_i\)进行负采样时，如果碰巧选到\(w_i\)时，需要重新负采样，直到选非自身的值为止。&lt;/p&gt;

&lt;h3 id=&quot;34-大规模消息网络嵌入式表示&quot;&gt;3.4 大规模消息网络嵌入式表示&lt;/h3&gt;

&lt;p&gt;大规模消息网路嵌入式表示(Large-scale Information Network Embeding，简记LINE)主要是用来研究大规模消息网络结点间的关系，并用低维向量空间来表示该网络结构，这项技术广泛应用在多个领域，比如：数据可视化，结点分类，链接预测。LINE相比于
当前存在的图嵌入技术&lt;a href=&quot;#tenenbaum2000global&quot;&gt;[22]&lt;/a&gt; &lt;a href=&quot;#belkin2001laplacian&quot;&gt;[23]&lt;/a&gt;的主要优点是它可以适用于现实世界中的真实网络(拥有上百万顶点，上千万边的网络)。LINE适用于任意类型的消息网络，比如说，无向网络，有向网络，
带权重的网络。这种方法通过优化保留了局部和全局网络结构的目标函数来刻画消息网络的特征从而得到每一个结点的低维向量表示。局部网络结构，又称一阶相似性，捕获的是网络中两个顶点的链接关系，大部分图嵌入方法均可以保留一阶相似性，比如IsoMap&lt;a href=&quot;#tenenbaum2000global&quot;&gt;[22]&lt;/a&gt;。由于在现实世界的真实网络中，很多合理的链接并没有被捕获，仅仅通过一阶相似性不足以表示全局网络结构，文&lt;a href=&quot;#tang2015line&quot;&gt;[5]&lt;/a&gt;中提出了结点间的二阶相似性。二阶相似性通过判断两个结点间是否共享邻居来判断这两个结点是否相似，这个思想和我们直观上的想法—“我们可以通过某人的朋友来了解一个人”。&lt;/p&gt;

&lt;p&gt;下面举例说明，在图10中的消息网络中，边可以是有向的，无向的或者带权重的。顶点6和顶点7由于存在边直接连接，所以顶点6和顶点7存在一阶相似性。顶点5和顶点6由于共享相同的邻居(顶点1，顶点2，顶点3，顶点4),所以顶点5和顶点6具有二阶相似性。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/line-2-prox.png&quot; alt=&quot;line-2-prox&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图11 网络中的二阶相似性&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;下面具体来用数学公式来刻画一阶相似性和二阶相似性。&lt;/p&gt;

&lt;h4 id=&quot;341-一阶相似性&quot;&gt;3.4.1 一阶相似性&lt;/h4&gt;

&lt;p&gt;一阶相似性指的是网络中两个顶点的局部成对相似性，为了对一阶相似性建模，对于任意无向边\((i,j)\),我们定义了顶点\(v_i\)和\(v_j\)的联合概率如下：&lt;/p&gt;

&lt;p&gt;\begin{equation}
p_ 1(v_ i,v_ j) = {1 \over 1+exp(-\vec{u_ i}^T \cdot \vec{u _ j})}
\label{eq:eq29}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(\vec{u_ i} \in R^d\)是顶点\(v_ i\)的低维向量表示，\eqref{eq:eq29}定义了在\(V \times V\)空间上的一个概率分布\(p(\cdot,\cdot)\),它的经验分布为
\(\hat{p_ 1} = {w_ {ij} \over W}\),其中\(W = \sum _{(i,j) \in E} w_{ij}\),为了保留一阶相似性，一个最直接的想法就是最小化前面两个分布的距离，即目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_ 1 = - \sum _ {(i,j) \in E} w_ {ij} \log p_ 1(v_ i,v_ j) 
\label{eq:eq30}
\end{equation}&lt;/p&gt;

&lt;p&gt;一阶相似性只适合于无向图，不适合于有向图，通过找到\( \{\vec{u_ i}  \}_{i = 1,\cdots,|V|}\)来最小化目标函数\eqref{eq:eq30}，我们便可以得到每个顶点的\(d\)维向量表示。&lt;/p&gt;

&lt;h4 id=&quot;342-二阶相似性&quot;&gt;3.4.2 二阶相似性&lt;/h4&gt;
&lt;p&gt;二阶相似性适用于无向图和有向图，为了不损失一般性，我们考虑一个有向图网络(无向边可以看做两个具有相同权重但是方向相反的有向边)，二阶相似性认为如果两个顶点共享邻居结点的话，那么这两个结点相似。在这种情况下，每一个顶点被指定一个上下文(Context)，如果顶点在上下文上具有相似的分布，那么认为这两个顶点是相似的。
因此，一个顶点担任两个角色，第一，顶点本身；第二，其他顶点的上下文(Context)。所以我们引入两个向量\(\vec{u_i} 和 {\vec{u_i}}^{\prime}\)，其中\(\vec{u_i}\)是\(v_i\)的顶点表示，\({\vec{u_i}}^{\prime}\)是\(v_i\)的上下文表示，对于每一个有向边(i,j)，我们定义通过顶点\(v_i\)生成上下文\(v_j\)的概率如下&lt;/p&gt;

&lt;p&gt;\begin{equation}
p_2(v_ j|v_ i) = {exp({\vec{u_ j}^{\prime}}^T \cdot \vec{u_ i} ) \over \sum _ {k=1} ^{|V|} exp({\vec{u_ k}^{\prime}}^T \cdot \vec{u_ i} )}
\label{eq:eq31}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(|V|\)是顶点或者上下文的数量，对于每一个顶点\(v_i\)，公式\eqref{eq:eq31}定义了在\(v_i\)在上下文下的条件概率分布\(p_2(\cdot|v_ i)\),正如上文所说，二阶相似性表示的是，如果顶点在上下文上具有相似的概率分布，那么这些顶点相似。为了保留二阶相似性，我们应使\(p_2(\cdot|v_i)\)与经验分布\(\hat{p_2}(\cdot|v_i)\)距离最近,因此我们定义如下目标函数。&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_2 = \sum_{i \in V} \lambda_ i d(p_2(\cdot|v_i),\hat{p_2}(\cdot|v_i))
\label{eq:eq32}
\end{equation}
其中\(d(\cdot,\cdot)\)表示两个概率分布之间的距离，由于网络中顶点之间的重要性不同，\(\lambda_i\)表示每个顶点的重要性，它可以用每个顶点的度或者PageRank&lt;a href=&quot;#page1999pagerank&quot;&gt;[24]&lt;/a&gt;,其中\(\hat{p_2}(v_j|v_ i) = {w_{ij} \over d_ i}\),\(w_{ij}\)是边(i,j)的权重，\(d_i\)是顶点i的出度，我们通过KL距离来计算\(d(\cdot,\cdot)\)并且\(\lambda_i = d_i\),目标函数可以简化为
\begin{equation}
O_2 = - \sum _{(i,j) \in E} w_{ij}\log p_2(v_j|v_i)
\label{eq:eq33}
\end{equation}&lt;/p&gt;

&lt;p&gt;通过学习出\(\{\vec{u_ i}\}_{i = 1 \cdots |V|}\) 和\(\{\vec{u_ i}^{\prime}\}_{i = 1 \cdots |V|}\)使目标函数达到最小，
我们便可以用一个d维向量\(\vec{u_ i}\)表示每一个顶点\(v_i\)。&lt;/p&gt;

&lt;h3 id=&quot;35-spark&quot;&gt;3.5 Spark&lt;/h3&gt;

&lt;p&gt;Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。2009年由加州大学伯克利分校AMPLab实验室开发，并于2010年成为Apache的开源项目之一。Spark与Hadoop，Storm和MapReduce等其他大数据技术相比，Spark有很多优势，首先，Spark为我们提供了一个全面、统一的框架用于管理各种不同性质(文本数据、图表数据等)的数据集和数据源(批量数据或实时流数据)的大数据处理的需求。Spark可以将运行在Hadoop集群中的应用在内存中运行并且其速度提升100倍，甚至可以将应用在磁盘上的运行速度提升10倍。开发者可以快速的使用Java、Scala或者Python编写程序并部署在Spark集群中，它本身自带了一个超过80个高阶操作符集合，可以用它在shell中交互式的查询数据。Spark除了Map和Reduce操作之外，还支持SQL查询、流数据、机器学习、和图表数据处理，开发者可以在一个数据管道用例中单独使用某一操作符或者将这些操作符结合在一起使用。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/spark-structure.png&quot; alt=&quot;spark-structure&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图12 Spark体系架构模型中各个组件&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;351-hadoop和spark&quot;&gt;3.5.1 Hadoop和Spark&lt;/h4&gt;
&lt;p&gt;Hadoop这项大数据处理技术已有十多年历史，而且被看做是首选的大数据集合处理的解决方案。MapReduce是一路计算的优秀解决方案，不过对于需要多路计算的算法来说，并非十分高效。数据处理流程中的每一步都需要一个Map阶段和一个Reduce阶段，而且如果要利用这一解决方案，需要将所有用例都转换成MapReduce模式。在下一步开始之前，上一步的作业输出数据必须要存储到分布式文件系统中。因此，复制和磁盘存储会导致这种方式速度变慢。另外Hadoop解决方案中通常会包含难以安装和管理的集群。而且为了处理不同的大数据用例，还需要集成多种不同的工具，比如用于机器学习的Mahout和流数据处理的Storm。如果想要完成比较复杂的工作，就必须将一系列的MapReduce作业串联起来然后顺序执行这些作业。每一个作业都是高时延的，而且只有在前一个作业完成之后下一个作业才能开始启动。&lt;/p&gt;

&lt;p&gt;Spark则允许程序开发者使用有向无环图（DAG）开发复杂的多步数据管道。而且还支持跨有向无环图的内存数据共享，以便不同的作业可以共同处理同一个数据。Spark运行在现有的Hadoop分布式文件系统基础之上（HDFS）提供额外的增强功能。它支持将Spark应用部署到现存的Hadoop v1集群（with SIMR – Spark-Inside-MapReduce）或Hadoop v2 YARN集群甚至是Apache Mesos之中。我们应该将Spark看作是Hadoop MapReduce的一个替代品而不是Hadoop的替代品。其意图并非是替代Hadoop，而是为了提供一个管理不同的大数据用例和需求的全面且统一的解决方案。&lt;/p&gt;

&lt;h4 id=&quot;352-spark的特性&quot;&gt;3.5.2 Spark的特性&lt;/h4&gt;
&lt;p&gt;Spark通过在数据处理过程中成本更低的洗牌（Shuffle）方式，将MapReduce提升到一个更高的层次。利用内存数据存储和接近实时的处理能力，Spark比其他的大数据处理技术的性能要快很多倍。
Spark还支持大数据查询的延迟计算，这可以帮助优化大数据处理流程中的处理步骤。Spark还提供高级的API以提升开发者的生产力，除此之外还为大数据解决方案提供一致的体系架构模型。
Spark将中间结果保存在内存中而不是将其写入磁盘，当需要多次处理同一数据集时，这一点特别实用。Spark的设计初衷就是既可以在内存中又可以在磁盘上工作的执行引擎。当内存中的数据不适用时，Spark操作符就会执行外部操作。Spark可以用于处理大于集群内存容量总和的数据集。
Spark会尝试在内存中存储尽可能多的数据然后将其写入磁盘。它可以将某个数据集的一部分存入内存而剩余部分存入磁盘。开发者需要根据数据和用例评估对内存的需求。Spark的性能优势得益于这种内存中的数据存储。&lt;/p&gt;

&lt;p&gt;Spark的其他特性包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;支持比Map和Reduce更多的函数。&lt;/li&gt;
  &lt;li&gt;优化任意操作算子图（operator graphs）。&lt;/li&gt;
  &lt;li&gt;可以帮助优化整体数据处理流程的大数据查询的延迟计算。&lt;/li&gt;
  &lt;li&gt;提供简明、一致的Scala，Java和Python API。&lt;/li&gt;
  &lt;li&gt;提供交互式Scala和Python Shell。目前暂不支持Java。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;353-spark生态系统&quot;&gt;3.5.3 Spark生态系统&lt;/h4&gt;
&lt;p&gt;除了Spark核心API之外，Spark生态系统中还包括其他附加库，可以在大数据分析和机器学习领域提供更多的能力。这些库包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark Streaming: Spark Streaming基于微批量方式的计算和处理，可以用于处理实时的流数据。它使用DStream，简单来说就是一个弹性分布式数据集（RDD）系列，处理实时数据&lt;/li&gt;
  &lt;li&gt;Spark SQL: Spark SQL可以通过JDBC API将Spark数据集暴露出去，而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询。用户还可以用Spark SQL对不同格式的数据（如JSON，Parquet以及数据库等）执行ETL，将其转化，然后暴露给特定的查询。&lt;/li&gt;
  &lt;li&gt;Spark MLlib: MLlib是一个可扩展的Spark机器学习库，由通用的学习算法和工具组成，包括二元分类、线性回归、聚类、协同过滤、梯度下降以及底层优化原语。&lt;/li&gt;
  &lt;li&gt;Spark GraphX: GraphX是用于图计算和并行图计算的新的（alpha）Spark API。通过引入弹性分布式属性图（Resilient Distributed Property Graph），一种顶点和边都带有属性的有向多重图，扩展了Spark RDD。为了支持图计算，GraphX暴露了一个基础操作符集合（如subgraph，joinVertices和aggregateMessages）和一个经过优化的Pregel API变体。此外，GraphX还包括一个持续增长的用于简化图分析任务的图算法和构建器集合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除了这些库以外，还有一些其他的库，如BlinkDB和Tachyon。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BlinkDB&lt;/strong&gt;是一个近似查询引擎，用于在海量数据上执行交互式SQL查询。BlinkDB可以通过牺牲数据精度来提升查询响应时间。通过在数据样本上执行查询并展示包含有意义的错误线注解的结果，操作大数据集合。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tachyon&lt;/strong&gt;是一个以内存为中心的分布式文件系统，能够提供内存级别速度的跨集群框架（如Spark和MapReduce）的可信文件共享。它将工作集文件缓存在内存中，从而避免到磁盘中加载需要经常读取的数据集。通过这一机制，不同的作业/查询和框架可以以内存级的速度访问缓存的文件。
此外，还有一些用于与其他产品集成的适配器，如Cassandra（Spark Cassandra 连接器）和R（SparkR）。Cassandra Connector可用于访问存储在Cassandra数据库中的数据并在这些数据上执行数据分析。&lt;/p&gt;

&lt;h3 id=&quot;36-hive&quot;&gt;3.6 HIVE&lt;/h3&gt;
&lt;p&gt;Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。&lt;/p&gt;

&lt;p&gt;下面是Hive的架构图：&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/hive-structure.png&quot; alt=&quot;hive-structure&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图13 HIVE体系架构&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;由上图可知，hadoop和mapreduce是hive架构的根基。Hive架构包括如下组件：CLI（command line interface）、JDBC/ODBC、Thrift Server、WEB GUI、metastore和Driver(Complier、Optimizer和Executor)，这些组件可以分为两大类：服务端组件和客户端组件。&lt;/p&gt;

&lt;p&gt;服务端组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Driver组件：该组件包括Complier、Optimizer和Executor，它的作用是将我们写的HiveQL（类SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的mapreduce计算框架。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Metastore组件：元数据服务组件，这个组件存储hive的元数据，hive的元数据存储在关系数据库里，hive支持的关系数据库有derby、mysql。元数据对于hive十分重要，因此hive支持把metastore服务独立出来，安装到远程的服务器集群里，从而解耦hive服务和metastore服务，保证hive运行的健壮性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thrift服务：Thrift是facebook开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，hive集成了该服务，能让不同的编程语言调用hive的接口。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;客户端组件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CLI：command line interface，命令行接口。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Thrift客户端：上面的架构图里没有写上Thrift客户端，但是hive架构的许多客户端接口是建立在thrift客户端之上，包括JDBC和ODBC接口。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WEBGUI：hive客户端提供了一种通过网页的方式访问hive所提供的服务。这个接口对应hive的hwi组件（hive web interface），使用前要启动hwi服务。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-主要工作&quot;&gt;4. 主要工作&lt;/h2&gt;

&lt;h3 id=&quot;41-spark大数据处理平台的搭建&quot;&gt;4.1 Spark大数据处理平台的搭建&lt;/h3&gt;

&lt;p&gt;为处理海量微博数据，我申请了五台机器搭建了Spark大数据处理集群，集群中机器配置如下，&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;机器IP&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;结点类别&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;内存&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;硬盘&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;CPU&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.33&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;master&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.34&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.35&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.36&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;183.xxx.xxx.37&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;slave&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;70G&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12T&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;24核&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在该集群中，我选用的Hadoop的版本为2.6.1,Spark版本为1.5.0，Hive版本为1.2.1，主要利用分布式文件存储(Hdfs)存储海量微博数据，Hive的调度引擎是基于Yarn的，可以通过Hive解析器将查询语句转化成基于MapReduce的分布式程序，
本文中大部分数据处理工作(数据清洗、抽取、变形)利用Hive,大大提高了的效率，由于实验室机房大部分端口未开放，所以采用
Nginx反向代理技术将监控页面开放给用户.&lt;/p&gt;

&lt;p&gt;图14为Spark大数据平台监控界面，用户可以在此平台观察程序运行结果，各阶段运行时间，管理用户程序。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/spark-page.png&quot; alt=&quot;spark-page&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图14 Spark平台监控界面&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;图15为Hadoop大数据平台监控界面，用户可以在此平台查看基于MapReduce的程序的执行状态，执行结果，执行时间等相关信息。
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/yarn-page.png&quot; alt=&quot;yarn-page&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图15 Hadoop平台监控界面&lt;/p&gt;
    
&lt;/div&gt;

&lt;h3 id=&quot;42-异构网络的有监督的社会化标签推荐方法&quot;&gt;4.2 异构网络的有监督的社会化标签推荐方法&lt;/h3&gt;
&lt;p&gt;传统的无监督文本嵌入方法，比如Skip-gram，Paragraph Vectors可以学习到比较通用微博用户向量表示，但是这种方法对指定的任务不会产生很高的准确率，我们提出一种有监督的学习方法，它可以利用打标签的数据(用户-标签网络)和无标签数据(用户-用户网络)来产生对指定分类任务更有针对性的标签，从而提升用户分类的准确率。为了达到这个目标，我们需要对有标记数据和无标记数据进行统一表示。
下面来定义五个网络:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;User-User Network:用户关注关系网络,记为\(G_{uu}=(V,E_{uu})\)这个网络记录了用户之间的关注关系，\(V\)是爬取的微博用户的集合，\(E_{uu}\)是边的集合，表现的是用户之间的关注关系，如果两个用户之间有关注关系，这两个用户之间有一条边。用户-用户网络捕获了用户之间的关注关系，这个是用户嵌入表示，比如Skip-Gram，所需要的重要信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Forward Network：用户转发网络,记为\(G_{uf} = (V_1,E_{uf})\),这个网络记录了微博用户的转发关系，其中的转发特征是根据微博文本中的&lt;code class=&quot;highlighter-rouge&quot;&gt;//@&lt;/code&gt;来确定的，
\(V_1\)是爬取数据微博文本中含有转发关系的用户集合，\(V_1\)是\(V\)的子集，\(E_{uf}\)是用户与转发用户之间边的集合,这个网络隐含用户之间的关系，为了有监督的捕获用户标签的信息，我们需要定义用户标签网络,用户微博文本网络。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Tag Network：用户标签网络，记为\(G_{ut}=(V_2\bigcup T,E_{ut})\)。这个二部图网络记录了用户与用户的Tag之间的信息，包含的标记信息，用于作为标记数据。\(V_2\)是爬取的微博数据中带有标签的微博用户的集合，\(V_2\)是\(V\)的子集。\(T\)是爬取微博用户的标签的集合，\(E_{ut}\)是用户集合和标签集合之间边的集合。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Microblog Network: 用户微博文本网络,记为\(G_{um}=(V_3\bigcup T_1,E_{um})\)这个网络记录了微博文本中的标签信息,我们通过对微博文本分词并提取标签信息得到，其中\(V_3\)是通过分析用户发的微博得到用户集合，\(V_3\)是\(V\)的子集，
其中\(T_1\)是通过分析用户发的微博得到标签集合，它是\(T\)的子集,这个网络具有标记信息，可用于有监督学习。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heterogeneous User Tag Network：这个网络整合了无标记网络(User-User Network、User-Forward Network)和有标记网络(User-Tag Network、User-Microblog Network),它包含不同维度的用户信息，包含有标记数据和无标记数据两部分。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面来引入我们的研究内容：&lt;br /&gt;
Predictive User Tag embedding:针对海量微博数据(无标记的用户关系数据和有标记用户标签数据),我们通过Heterogeneous User Tag Network去得到用户以及Tag的低维表示。通过用户和Tag的低维表示，我们可以利用\( \vec{u}\bullet\vec{v}\)来排序来计算与用户最相关的Tag。&lt;/p&gt;

&lt;p&gt;下面介绍本文的训练方法：&lt;/p&gt;

&lt;p&gt;(1) Bipartite Network Embedding
LINE Model是Graph embedding的比较常用的方法，它可以训练大规模网络从而得到节点的嵌入式表示。LINE Model主要是解决同构网络的节点嵌入式表示，由于异构网络之间边的权重没有可比性，所以LINE Model不能直接应用于异构网路。我们利用了LINE Model 二阶相似的思想，有相似邻居的两个顶点是相似的，这两个顶点在低维空间中距离很近。给定一个二部图网络\(G=(V_A\bigcup{V_B},E)\)，其中\(V_A\)，\(V_B\)是两个不同类型的不相交的顶点集合，E是连接两个顶点集合之间边的集合。我们首先定义由在集合\(V_B\)中的顶点\(v_j\)生成\(V_A\)中\(v_i\)顶点的条件概率为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
  p(v_i|v_j) = {e^{\vec{u_i}\bullet\vec{u_j}}\over \sum_{i^{\prime} \in V_A} e^{\vec{u_i^{\prime}}\bullet \vec{u_j}}} 
  \label{eq:eq1}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(\vec u_i\)是\(v_i\)的嵌入式表示，\(\vec u_j\)是\(v_j\)的嵌入式表示，对于每一个在\(V_B\)中的顶点\(v_j\),方程\eqref{eq:eq1}定义了在&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_A\)&lt;/code&gt;中所有顶点上的一个条件分布&lt;code class=&quot;highlighter-rouge&quot;&gt;\(p(\bullet|v_j)\)&lt;/code&gt;。对于任意一对顶点&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_j\)&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_{j^{\prime}}\)&lt;/code&gt;,为了保留二阶相似性，我们可以使条件分布&lt;code class=&quot;highlighter-rouge&quot;&gt;\(p(\bullet|v_j)\)&lt;/code&gt;接近于
&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\hat p(\bullet|v_j)\)&lt;/code&gt;,所以我们可以通过最小化下面这个目标函数达到目标:
\begin{equation}
O = \sum_{j \in V_B} \lambda_j d(\hat p(\bullet|v_j),p(\bullet|v_j))
\label{eq:eq2}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中&lt;code class=&quot;highlighter-rouge&quot;&gt;\(d(\bullet,\bullet)\)&lt;/code&gt;是两个分布的KL距离,&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\lambda_j\)&lt;/code&gt;用来表示顶点&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_j\)&lt;/code&gt;在网络中的重要性，它可以定义为:&lt;code class=&quot;highlighter-rouge&quot;&gt;\(deg_j=\sum_i w_{ij}\)&lt;/code&gt;,经验分布
&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\hat p(\bullet|v_j) = {w_{ij}\over deg_j}\)&lt;/code&gt;忽略一些常数，目标函数\eqref{eq:eq2}可简化为：
\begin{equation}
O = - \sum_{(i,j) \in E} w_{ij} \log {p(v_j|v_i)}
\label{eq:eq3}
\end{equation}&lt;/p&gt;

&lt;p&gt;目标函数\eqref{eq:eq3}可以利用边采样[5]或者负采样[11]的随机梯度下降法进行优化求解。
我们可以将上文提到的四种单一网络中的无向边看成是两条有向边，然后&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_A\)&lt;/code&gt;可以看做源节点的集合，&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_B\)&lt;/code&gt;可以看做目的节点的集合，通过这样处理，我们可以将上文提到的四种单一网络视为二部图来处理，从而可以利用改模型进行求解&lt;/p&gt;

&lt;p&gt;(2) Heterogeneous User Tag Network由两个四个部图网络构成，其中无标记网络为User-User Network、User-Forward Network和有标记网络为User-Tag Network、User-Microblog Network。其中User节点集合被四个网络所共享，为了学习到四个网络结构的嵌入式表示，我们直觉上的方法是整体训练这四个二部图网络，即最小化下面的目标函数:
\begin{equation}
O_{total} = O_{uu} + O_{ut} + O_{uf} + O_ {um}
\label{eq:eq4}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中：
\begin{equation}
O_{uu} = - \sum_{(i,j) \in E_{uu}} \log p(v_i|v_j)
\label{eq:eq5}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{ut} = - \sum_{(i,j) \in E_{ut}} \log p(v_i|t_j)
\label{eq:eq6}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{uf} = - \sum_{(i,j) \in E_{uf}} \log p(v_i|f_j)
\label{eq:eq7}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{um} = - \sum_{(i,j) \in E_{um}} \log p(v_i|m_j)
\label{eq:eq8}
\end{equation}&lt;/p&gt;

&lt;p&gt;目标函数\eqref{eq:eq4}有多种优化方法，一种解决方式是同时训练有标记数据和无标记数据，我们称这种方式为联合训练(Joint training)；另一种方式是先训练无标记数据，得到用户的嵌入式表示，然后利用有标记数据进行调优(Pre-training + Fine-tuning)[12],下面是具体的训练过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/pte-algorithm-1-2.png&quot; alt=&quot;algorithm&quot; title=&quot;pte-algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在联合训练中，上面的四种网络（用户关注关系网络、用户转发网络、用户标签网络、用户微博文本网络）均被训练，优化\eqref{eq:eq4}的一个方案是将\(G_{uu}，G_{uf}，G_{ut}，G_{um}\)中的所有边聚集在一起，然后使用边采样来更新模型，边采样的概率正比于边的权重，然而当网络是异构的，不同的网络结构之间边的权重是不兼容的，一个更好的解决方案是从四个边的集合中交替选择边采样，如上图中的算法\(1\)所示，相似的先训练后优化的算法细节如上图中的算法\(2\)所示。&lt;/p&gt;

&lt;h2 id=&quot;5-实验结果&quot;&gt;5. 实验结果&lt;/h2&gt;

&lt;h2 id=&quot;本文总结和对未来的工作展望&quot;&gt;本文总结和对未来的工作展望&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;mathes2004folksonomies&quot;&gt;[1]A. Mathes, “Folksonomies-cooperative classification and communication through shared metadata.” December, 2004.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;JiAT2007Collaborative&quot;&gt;[2]JiAT, “Collaborative Tagging in Recommender Systems,” in &lt;i&gt;AI 2007: Advances in Artificial Intelligence&lt;/i&gt;, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;shiratsuchi2006finding&quot;&gt;[3]K. Shiratsuchi, S. Yoshii, and M. Furukawa, “Finding unknown interests utilizing the wisdom of crowds in a social bookmark service,” in &lt;i&gt;Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology&lt;/i&gt;, 2006, pp. 421–424.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;clauset2005finding&quot;&gt;[4]A. Clauset, “Finding local community structure in networks,” &lt;i&gt;Physical review E&lt;/i&gt;, vol. 72, no. 2, p. 026132, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tang2015line&quot;&gt;[5]J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in &lt;i&gt;Proceedings of the 24th International Conference on World Wide Web&lt;/i&gt;, 2015, pp. 1067–1077.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;michlmayr2007learning&quot;&gt;[6]E. Michlmayr and S. Cayzer, “Learning user profiles from tagging data and leveraging them for personal (ized) information access,” 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ecs14007&quot;&gt;[7]M. Szomszor &lt;i&gt;et al.&lt;/i&gt;, “Folksonomies, the Semantic Web, and Movie Recommendation ,” in &lt;i&gt;4th European Semantic Web Conference, Bridging the Gap between Semantic Web and Web 2.0&lt;/i&gt;, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipp2006patterns&quot;&gt;[8]M. E. I. Kipp and D. G. Campbell, “Patterns and inconsistencies in collaborative tagging systems: An examination of tagging practices,” &lt;i&gt;Proceedings of the American Society for Information Science and Technology&lt;/i&gt;, vol. 43, no. 1, pp. 1–18, 2006.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;begelman2006automated&quot;&gt;[9]G. Begelman, P. Keller, F. Smadja, and others, “Automated tag clustering: Improving search and exploration in the tag space,” in &lt;i&gt;Collaborative Web Tagging Workshop at WWW2006, Edinburgh, Scotland&lt;/i&gt;, 2006, pp. 15–33.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;王萍2010一种社会性标签聚类算法&quot;&gt;[10]王萍 and 张际平, “一种社会性标签聚类算法,” &lt;i&gt;计算机应用与软件&lt;/i&gt;, vol. 27, no. 2, pp. 126–129, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Xu:2006:CAS:2114193.2114262&quot;&gt;[11]Y. Xu, L. Zhang, and W. Liu, “Cubic Analysis of Social Bookmarking for Personalized Recommendation,” in &lt;i&gt;Proceedings of the 8th Asia-Pacific Web Conference on Frontiers of WWW Research and Development&lt;/i&gt;, Berlin, Heidelberg, 2006, pp. 733–738.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yin2013connecting&quot;&gt;[12]D. Yin, S. Guo, B. Chidlovskii, B. D. Davison, C. Archambeau, and G. Bouchard, “Connecting comments and tags: improved modeling of social tagging systems,” in &lt;i&gt;Proceedings of the sixth ACM international conference on Web search and data mining&lt;/i&gt;, 2013, pp. 547–556.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;基于社会化标签的协同过滤推荐策略研究&quot;&gt;[13]万朔 and 邱会中, “基于社会化标签的协同过滤推荐策略研究,” &lt;i&gt;电子科技大学&lt;/i&gt;, pp. 14–16, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lacic2014recommending&quot;&gt;[14]E. Lacic, D. Kowald, P. Seitlinger, C. Trattner, and D. Parra, “Recommending items in social tagging systems using tag and time information,” &lt;i&gt;arXiv preprint arXiv:1406.7727&lt;/i&gt;, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;涂存超:24&quot;&gt;[15]涂存超 孙茂松, “社会媒体用户标签的分析与推荐,” &lt;i&gt;图书情报工作&lt;/i&gt;, vol. 57, no. 23, p. 24, 2013.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kumar2014exploiting&quot;&gt;[16]H. Kumar, S. Lee, and H.-G. Kim, “Exploiting social bookmarking services to build clustered user interest profile for personalized search,” &lt;i&gt;Information Sciences&lt;/i&gt;, vol. 281, pp. 399–417, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;godoy2012one&quot;&gt;[17]D. Godoy, “One-class support vector machines for personalized tag-based resource classification in social bookmarking systems,” &lt;i&gt;Concurrency and Computation: Practice and Experience&lt;/i&gt;, vol. 24, no. 17, pp. 2193–2206, 2012.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yoshida2012improving&quot;&gt;[18]T. Yoshida, G. Irie, T. Satou, A. Kojima, and S. Higashino, “Improving item recommendation based on social tag ranking,” in &lt;i&gt;International Conference on Multimedia Modeling&lt;/i&gt;, 2012, pp. 161–172.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mikolov2013distributed&quot;&gt;[19]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;, 2013, pp. 3111–3119.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bengio2003neural&quot;&gt;[20]Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language model,” &lt;i&gt;Journal of machine learning research&lt;/i&gt;, vol. 3, no. Feb, pp. 1137–1155, 2003.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rumelhart1988learning&quot;&gt;[21]D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” &lt;i&gt;Cognitive modeling&lt;/i&gt;, vol. 5, no. 3, p. 1, 1988.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tenenbaum2000global&quot;&gt;[22]J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” &lt;i&gt;science&lt;/i&gt;, vol. 290, no. 5500, pp. 2319–2323, 2000.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;belkin2001laplacian&quot;&gt;[23]M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in &lt;i&gt;NIPS&lt;/i&gt;, 2001, vol. 14, no. 14, pp. 585–591.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;page1999pagerank&quot;&gt;[24]L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web.,” Stanford InfoLab, 1999.&lt;/span&gt;



&lt;/li&gt;&lt;/ul&gt;
</description>
      </item>
    
      <item>
        <title>大规模社交用户的标签自动生成技术研究</title>
        <link>http://localhost:4000/2017/04/07/weibo-user-profile.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/04/07/weibo-user-profile.html</guid>
        <pubDate>Fri, 07 Apr 2017 00:00:00 +0800</pubDate>
        <description>&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;社会标签(Social Tagging)作为一种新型网络信息组织方式,由网络信息的提供者或者用户自发为某类信息赋予一定数量的标签,
选用自由词对感兴趣的网络信息资源进行描述来实现网络信息的分类,正在改变着传统的网络信息组织模式,
被广泛应用在社交网络、商业以及科研教育领域。社会标签较传统的主题词具有更大的灵活性、易用性,同时资源描述性关键词的增加也便于
对资源进行准确查找,尤其是对多媒体资源来说,用户所标注的标签信息就更为重要。&lt;/p&gt;

&lt;p&gt;Word2Vec从提出至今，已经广泛应用在自然语言处理、机器学习、数据挖掘等领域，从它延伸出的Doc2Vec、Graph2Vec也具有广泛的应用前景，
它已经成为了深度学习在自然语言处理中的基础部件，本文尝试利用大规模微博社交网络数据，提出将异构网络（微博用户关系网络、用户标签网络、用户文本网络）
进行联合训练的方法并得到用户与标签的嵌入式表示。这项技术可用于建模用户兴趣，推荐用户感兴趣的商品、信息和好友，进行用户画像，以及寻找目标用户进行商品推广。
本文提出的联合训练方法产生的用户与标签的嵌入式表示优于分别训练用户关系网络、用户标签网络产生的用户和标签的嵌入式表示，并在预测用户标签的准确率上优于同类方法。&lt;/p&gt;

&lt;p&gt;关键字：推荐系统，图嵌入表示，大数据&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Social Tagging is a new type of organizing the information of Internet. The users of Internet will provide some tags to
some objects spontaneously.We can use these tags to sort resources that we are interested in. Social Tagging is changing
the the method of the organization of network information resources and widely used in the social network service,business
and education. Social tagging is more flexible and convenient than traditional key words. With the increase of the number of 
social tags，it is more convenient to locate resource. Especially for multi-media resource，social tagging plays an increasingly important role.&lt;/p&gt;

&lt;p&gt;Word2vec has been widely applied in the fields of NLP，machine learning and data mining since it was first proposed by Mikolov. Doc2vec
and Graph2Vec，which are all originated from Word2vec，also have wide application foreground. Word2vec has become fundamental component in 
the field of NLP. This paper tries to use huge amounts of social networks data. In this paper, a new algorithm is presented, which can be
use to train heterogeneous network(user relationship network and user tags network and user text network). Through this algorithm,we can 
get embedding of both user and tag and then we can use these embedding in the fields，such as user interests modeling，Commodity recommendation and User portrait. The embedding produced by our algorithm is better than that produced by separately training each network.
Besides,in the case of predicting user tags，our algorithm is better than other similar methods.&lt;/p&gt;

&lt;p&gt;keywords:Recommendation System,Graph Embedding,Big Data&lt;/p&gt;

&lt;h2 id=&quot;1-引言&quot;&gt;1. 引言&lt;/h2&gt;
&lt;h3 id=&quot;11-研究背景&quot;&gt;1.1 研究背景&lt;/h3&gt;

&lt;p&gt;微博作为一种新型媒体，是一种基于草根用户的关系构建个性化用户信息的即时传递、共享和获取平台。它具有信息实时性，内容简洁性，用户交互性等特点。
微博之所以可以成为当今国内外主流的社交媒体，主要是因为其具有强大的用户实时交互性，用户在使用微博的过程中，会在微博的网络空间中结成种种关系，
比如用户之间的关注关系，社区中的好友和亲情关系，实时交互过程中因共同购买或评论产品而结成的共同评论关系等，有效分析和挖掘微博中复杂的用户关系不仅可以激发、
助推和引导社会事件的发展趋势，还可以准确高效的为关注某一兴趣和爱好的微博群体进行个性化推荐，甚至可以大大降低企业和消费者的交易成本，推动企业营销模式的不断创新。
此外，微博在凝聚民心，降低事件危害以及政务互动等方面也发挥着不可替代的积极作用。由此可见，微博的兴起赋予了社会经济活动前所未有的大众化和网络化的内涵，
极大提升了社交媒体的社会服务效能。但是急剧增长的微博用户数量和海量用户下的交互行为增加了社会、经济与生产的复杂性，使一些社会实践变得更加不可预测，难以控制，
从而为分析社会化效应带来了新的挑战，因此如何正确理解微博用户之间的关系以及用户在关系交互中所产生的行为，成为学者迫切需要研究的新方向。&lt;/p&gt;

&lt;p&gt;社会化标签（Social Tagging） 也称为collaborative tagging，指的是用户在网络中自发得分配电子标签或关键词来描述网络上的资源，并且在网络用户群体中共享这些标签。这种方法允许用户使用自己的语言、以“标签”的形式对信息资源的内外部特征进行标注,以实现资源的查找和共享。社会化标签系统与预先定义网络资源类别
 来对网络资源分类的机制相比，它可以使用户自发产生和分配标签，这对网络资源分类有积极意义。随着社会化标签的广泛应用，公众分类法（folksonomy）应运生，这种分类方法改变了传统利用专家来对网络资源分类的方式，它是从公众的角度来分类资源。公众分类法是互联网所推崇的共享与协作精神的体现,是新的互联网信息环境中一种独具特色的信息组织工具。它的产生为互联网信息组织与检索的改进提供了新方向。随着社交网络，图片分享，视频分享业务的蓬勃发展，社会化标签系统被广泛应用在互联网的各个领域，它可以对社交网络和数据挖掘提供技术支持，而且有助于改进搜索结果，提高广告投放的准确率，同时利用所有用户产生的标签数据可以挖掘出其他有价值的信息，比如用户社群，潜在目标客户等。社会化标签系统将用户产生的大量通过网络传播而聚合起来，可以实现对网络资源的合作标记和公众分类，体现网络用户的群体智慧。&lt;/p&gt;

&lt;p&gt;在web2.0中，用户不仅可以通过豆瓣来分享图书,通过优酷来分享视频，通过微博来发表博文，通过Flicker来发布照片，通过youtube上传视频等方式来创造内容，
用户这些行为有一个共同的特征，即用户会自由的选择一些词(Term)或者短语(Phrase)来标注相关网络信息资源，
我们称用户的这种行为为标注(Tagging),用户所选择的词或者词语为标签(Tag),提供标注行为的系统为社会标签系统，
本文研究的就是利用微博用户关系与用户已有标签来为每个用户推荐相关的标签，便于建模用户兴趣，为用户之间的衔接赋予更丰富的信息，
推荐用户感兴趣的商品、信息和好友，进行用户画像，以及寻找目标用户进行商品推广。&lt;/p&gt;

&lt;h3 id=&quot;12-问题的提出&quot;&gt;1.2 问题的提出&lt;/h3&gt;
&lt;p&gt;在Web2.0环境下，互联网已经成为全球最大的知识库，它在给人类的生活和工作带来革命性变化的同时，也引发了“信息泛滥”，“信息迷航”等问题，社会化标签推荐能够根据用户的需求主动的将合适的信息、商品、知识提供给用户，可以有效缓解这些问题。同时，作为由用户产生的元数据，社会化标签能够独特反应用户的需求及其变化&lt;a href=&quot;#mathes2004folksonomies&quot;&gt;[1]&lt;/a&gt;，而且“用户-资源-社会化标签”之间的关系网络能够为个性化信息推荐系统提供十分有价值的基础数据，由此部分学者对基于社会化标签的个性化知识推荐进行了密切的关注，并从以下三个方面进行了探索。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基于矩阵的方法，即通过构建“用户-资源”矩阵、“用户-社会化标签”矩阵，“社会化标签-资源”矩阵实现知识推荐。Ji AT等人依据“用户-资源”矩阵，“用户-标签”矩阵，“标签-资源”矩阵构建Naive Bayesian分类器，以此为基础实现协同过滤推荐&lt;a href=&quot;#JiAT2007Collaborative&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于聚类的方法,主要包括用户、资源、社会化标签三种对象的聚类，其中基于社会化标签的聚类是当前研究的重点。一个重要的研究思路就是一句社会化标签之前的共现频率，利用k-means聚类算法、马尔科夫聚类算法等方法对社会化标签进行聚类，进而依据聚类的结果为用户提供个性化推荐。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;基于图论的方法。其中社会网络分析是学者们关注的重点，如Shiratsuchi等人依据用户使用的标签之间的相似性
建立用户社会网络&lt;a href=&quot;#shiratsuchi2006finding&quot;&gt;[3]&lt;/a&gt;，并利用Clauset提出的local midularity算法&lt;a href=&quot;#clauset2005finding&quot;&gt;[4]&lt;/a&gt;划分网络社区，进而实现协同过滤推荐。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;显然，学者们提出的三类方法都有其优势，但是都面临各自的问题。首先第一类方法，面临一个重要的问题就是社会化标签使用量的“幂率分布”规律，排序在前几位的社会化标签具有较大的使用量，而大量的社会化标签都处于“长尾”区域，由此相关的矩阵可能非常不规则，从而严重制约个性化推荐算法。其次第二类方法中基于社会化标签的聚类方法属于基于内容的推荐思想，难以发现用户新的兴趣，而且社会化标签使用量的“幂律分布”问题同样会制约推荐效果。最后，第三类方法中基于社会网络分析的方法属于协同过滤思想，虽然能够发现用户新兴趣，但是个性化推荐需要依据用户的特定知识需求，在一般的社会网络中，个性间的“关系互动”并不意味着在特定的知识情境下必然能产生“知识互动”由此，当前学者们提出的基于社会化标签的各种推荐方法都有自身无法克服的劣势，如何利用社会化标签是实现精准的推荐是学者研究的重要问题，本文中提出一种有监督训练社会化标签的方法，我们对海量微博数据构建用户关系网络，用户转发网络，用户文本网络，用户标签网络，其中用户转发网络与用户关系网络刻画的是用户之间的交互关系，我们通过一阶相似性和二阶相似性找到相似用户&lt;a href=&quot;#tang2015line&quot;&gt;[5]&lt;/a&gt;，在此基础上利用用户标签网络和用户文本网络为用户注入标签信息，利用这种方式我们可以得到用户与标签的嵌入式表示，在预测用户标签任务中，效果优于同类方法。&lt;/p&gt;

&lt;h3 id=&quot;13-研究内容与研究方法&quot;&gt;1.3 研究内容与研究方法&lt;/h3&gt;

&lt;h4 id=&quot;131-研究内容&quot;&gt;1.3.1 研究内容&lt;/h4&gt;

&lt;p&gt;传统的无监督文本嵌入方法，比如Skip-gram，Paragraph Vectors可以学习到比较通用微博用户向量表示，但是这种方法对指定的任务不会产生很高的准确率，我们提出一种有监督的学习方法，它可以利用打标签的数据(用户-标签网络)和无标签数据(用户-用户网络)来产生对指定分类任务更有针对性的标签，从而提升用户分类的准确率。为了达到这个目标，我们需要对有标记数据和无标记数据进行统一表示。
下面来定义五个网络:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;User-User Network:用户关注关系网络,记为\(G_{uu}=(V,E_{uu})\)这个网络记录了用户之间的关注关系，\(V\)是爬取的微博用户的集合，\(E_{uu}\)是边的集合，表现的是用户之间的关注关系，如果两个用户之间有关注关系，这两个用户之间有一条边。用户-用户网络捕获了用户之间的关注关系，这个是用户嵌入表示，比如Skip-Gram，所需要的重要信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Forward Network：用户转发网络,记为\(G_{uf} = (V_1,E_{uf})\),这个网络记录了微博用户的转发关系，其中的转发特征是根据微博文本中的&lt;code class=&quot;highlighter-rouge&quot;&gt;//@&lt;/code&gt;来确定的，
\(V_1\)是爬取数据微博文本中含有转发关系的用户集合，\(V_1\)是\(V\)的子集，\(E_{uf}\)是用户与转发用户之间边的集合,这个网络隐含用户之间的关系，为了有监督的捕获用户标签的信息，我们需要定义用户标签网络,用户微博文本网络。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Tag Network：用户标签网络，记为\(G_{ut}=(V_2\bigcup T,E_{ut})\)。这个二部图网络记录了用户与用户的Tag之间的信息，包含的标记信息，用于作为标记数据。\(V_2\)是爬取的微博数据中带有标签的微博用户的集合，\(V_2\)是\(V\)的子集。\(T\)是爬取微博用户的标签的集合，\(E_{ut}\)是用户集合和标签集合之间边的集合。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;User-Microblog Network: 用户微博文本网络,记为\(G_{um}=(V_3\bigcup T_1,E_{um})\)这个网络记录了微博文本中的标签信息,我们通过对微博文本分词并提取标签信息得到，其中\(V_3\)是通过分析用户发的微博得到用户集合，\(V_3\)是\(V\)的子集，
其中\(T_1\)是通过分析用户发的微博得到标签集合，它是\(T\)的子集,这个网络具有标记信息，可用于有监督学习。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heterogeneous User Tag Network：这个网络整合了无标记网络(User-User Network、User-Forward Network)和有标记网络(User-Tag Network、User-Microblog Network),它包含不同维度的用户信息，包含有标记数据和无标记数据两部分。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面来引入我们的研究内容：&lt;br /&gt;
Predictive User Tag embedding:针对海量微博数据(无标记的用户关系数据和有标记用户标签数据),我们通过Heterogeneous User Tag Network去得到用户以及Tag的低维表示。通过用户和Tag的低维表示，我们可以利用\( \vec{u}\bullet\vec{v}\)来排序来计算与用户最相关的Tag。&lt;/p&gt;

&lt;h4 id=&quot;132-研究方法&quot;&gt;1.3.2 研究方法&lt;/h4&gt;
&lt;p&gt;下面介绍本文的训练方法：&lt;/p&gt;

&lt;p&gt;(1)	Bipartite Network Embedding
LINE Model是Graph embedding的比较常用的方法，它可以训练大规模网络从而得到节点的嵌入式表示。LINE Model主要是解决同构网络的节点嵌入式表示，由于异构网络之间边的权重没有可比性，所以LINE Model不能直接应用于异构网路。我们利用了LINE Model 二阶相似的思想，有相似邻居的两个顶点是相似的，这两个顶点在低维空间中距离很近。给定一个二部图网络\(G=(V_A\bigcup{V_B},E)\)，其中\(V_A\)，\(V_B\)是两个不同类型的不相交的顶点集合，E是连接两个顶点集合之间边的集合。我们首先定义由在集合\(V_B\)中的顶点\(v_j\)生成\(V_A\)中\(v_i\)顶点的条件概率为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
  p(v_i|v_j) = {e^{\vec{u_i}\bullet\vec{u_j}}\over \sum_{i^{\prime} \in V_A} e^{\vec{u_i^{\prime}}\bullet \vec{u_j}}} 
  \label{eq:eq1}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(\vec u_i\)是\(v_i\)的嵌入式表示，\(\vec u_j\)是\(v_j\)的嵌入式表示，对于每一个在\(V_B\)中的顶点\(v_j\),方程\eqref{eq:eq1}定义了在&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_A\)&lt;/code&gt;中所有顶点上的一个条件分布&lt;code class=&quot;highlighter-rouge&quot;&gt;\(p(\bullet|v_j)\)&lt;/code&gt;。对于任意一对顶点&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_j\)&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_{j^{\prime}}\)&lt;/code&gt;,为了保留二阶相似性，我们可以使条件分布&lt;code class=&quot;highlighter-rouge&quot;&gt;\(p(\bullet|v_j)\)&lt;/code&gt;接近于
&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\hat p(\bullet|v_j)\)&lt;/code&gt;,所以我们可以通过最小化下面这个目标函数达到目标:
\begin{equation}
O = \sum_{j \in V_B} \lambda_j d(\hat p(\bullet|v_j),p(\bullet|v_j))
\label{eq:eq2}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中&lt;code class=&quot;highlighter-rouge&quot;&gt;\(d(\bullet,\bullet)\)&lt;/code&gt;是两个分布的KL距离,&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\lambda_j\)&lt;/code&gt;用来表示顶点&lt;code class=&quot;highlighter-rouge&quot;&gt;\(v_j\)&lt;/code&gt;在网络中的重要性，它可以定义为:&lt;code class=&quot;highlighter-rouge&quot;&gt;\(deg_j=\sum_i w_{ij}\)&lt;/code&gt;,经验分布
&lt;code class=&quot;highlighter-rouge&quot;&gt;\(\hat p(\bullet|v_j) = {w_{ij}\over deg_j}\)&lt;/code&gt;忽略一些常数，目标函数\eqref{eq:eq2}可简化为：
\begin{equation}
O = - \sum_{(i,j) \in E} w_{ij} \log {p(v_j|v_i)}
\label{eq:eq3}
\end{equation}&lt;/p&gt;

&lt;p&gt;目标函数\eqref{eq:eq3}可以利用边采样[5]或者负采样[11]的随机梯度下降法进行优化求解。
我们可以将上文提到的四种单一网络中的无向边看成是两条有向边，然后&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_A\)&lt;/code&gt;可以看做源节点的集合，&lt;code class=&quot;highlighter-rouge&quot;&gt;\(V_B\)&lt;/code&gt;可以看做目的节点的集合，通过这样处理，我们可以将上文提到的四种单一网络视为二部图来处理，从而可以利用改模型进行求解&lt;/p&gt;

&lt;p&gt;(2)	Heterogeneous User Tag Network由两个四个部图网络构成，其中无标记网络为User-User Network、User-Forward Network和有标记网络为User-Tag Network、User-Microblog Network。其中User节点集合被四个网络所共享，为了学习到四个网络结构的嵌入式表示，我们直觉上的方法是整体训练这四个二部图网络，即最小化下面的目标函数:
\begin{equation}
O_{total} = O_{uu} + O_{ut} + O_{uf} + O_ {um}
\label{eq:eq4}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中：
\begin{equation}
O_{uu} = - \sum_{(i,j) \in E_{uu}} \log p(v_i|v_j)
\label{eq:eq5}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{ut} = - \sum_{(i,j) \in E_{ut}} \log p(v_i|t_j)
\label{eq:eq6}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{uf} = - \sum_{(i,j) \in E_{uf}} \log p(v_i|f_j)
\label{eq:eq7}
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_{um} = - \sum_{(i,j) \in E_{um}} \log p(v_i|m_j)
\label{eq:eq8}
\end{equation}&lt;/p&gt;

&lt;p&gt;目标函数\eqref{eq:eq4}有多种优化方法，一种解决方式是同时训练有标记数据和无标记数据，我们称这种方式为联合训练(Joint training)；另一种方式是先训练无标记数据，得到用户的嵌入式表示，然后利用有标记数据进行调优(Pre-training + Fine-tuning)[12],下面是具体的训练过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/pte-algorithm-1-2.png&quot; alt=&quot;algorithm&quot; title=&quot;pte-algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在联合训练中，上面的四种网络（用户关注关系网络、用户转发网络、用户标签网络、用户微博文本网络）均被训练，优化\eqref{eq:eq4}的一个方案是将\(G_{uu}，G_{uf}，G_{ut}，G_{um}\)中的所有边聚集在一起，然后使用边采样来更新模型，边采样的概率正比于边的权重，然而当网络是异构的，不同的网络结构之间边的权重是不兼容的，一个更好的解决方案是从四个边的集合中交替选择边采样，如上图中的算法\(1\)所示，相似的先训练后优化的算法细节如上图中的算法\(2\)所示。&lt;/p&gt;

&lt;h2 id=&quot;2-国内外研究现状&quot;&gt;2. 国内外研究现状&lt;/h2&gt;
&lt;p&gt;本文对微博平台的使用主体也就是微博用户间的用户关系，用户标签以及用户发微博情况，以及微博用户转发情况展开相关研究，针对微博用户方面的内容也有很多人进行了研究，主要从以下几个角度进行研究:&lt;/p&gt;

&lt;p&gt;标签是由用户认为自由，不受约束环境下创造出来的，因此具有自由性和低限度的特点，当然标签系统的优点也往往正是它的缺点，标签具有一定的社会性和含糊性，也同时存在着例如同义词，多义词等一词多义，甚至拼写错误的情况，所以导致了标签系统存在大量重复、不规范、无效的标签，我们称之为噪音。当用户对其感兴趣的资源进行标注标签行为的时候，规范、有效、质量高的标签则会创造出标签系统的循环性，促进系统良性循环。&lt;/p&gt;

&lt;p&gt;面向微博用户关系模式信息推荐的基本思想是首先建立用户和信息源之间以及用户和用户之间的对应关系，然后进行用户社群分析，建立相似用户群或兴趣共同体。当相同用户群的某个用户或者某几个用户对某信息或者商品感兴趣时，可以预测共同体的其他成员也感兴趣，从而将该信息推荐给其他成员。常用的分析方法时，通过分析用户社交网络中用户之间的相互关系情况，然后根据用户的不同关系进行基于内容的协同过滤推荐。在微博使用实践中，用户积极选择并参与构建个性化关系，与一些具有相似特征的用户自发的聚集到一起形成群体，用户社群分析作为用户关系挖掘的主要技术手段，他在常规复杂系统的研究中比较成熟。&lt;/p&gt;

&lt;p&gt;社会化标签系统是Web用户利用社会化标签对Web资源进行标注的环境，它包括三个基本的实体，分别是Web用户，Web资源和社会化标签。另外，还包括一个关系集合。社会化标签系统的模型可以用一个四元组来表示\(F=(U,T,R,A)\),其中，\(U\)是Web用户的有限集合，\(T\)是社会标签的有限集合，\(R\)是Web资源的有限集合，\(A \subseteq U \times T \times R\)是一个三元关系集合，元素\(a=(u,t,r) \in A\)表示用户\(a\)使用标签\(t\)标注了资源\(r\)。标签共现分析是揭示标签语义关系的重要途径，Michlmayr和Cayzer &lt;a href=&quot;#michlmayr2007learning&quot;&gt;[6]&lt;/a&gt;指出，如果两个标签被某一用户结合或共同使用去标注某一个书签，那么这两个标签之间一定存在着某种语义关系。Szomszor等人&lt;a href=&quot;#ecs14007&quot;&gt;[7]&lt;/a&gt;通过实验表明标签共现
关系的重要本质是能够用来揭示标签之间的语义关系,并利用Jaccard系数来衡量标签之间的共现关系。Kipp和Campbell &lt;a href=&quot;#kipp2006patterns&quot;&gt;[8]&lt;/a&gt;利用共词分析来抽取社会化书签服务系统delicious中的标签模型，他们发现标签的数量和使用频率之间遵循幂律分布，即只有少量的标签被经常用来标注资源而大部分标签被使用的次数较少。Begelman，Keller和Smadja等人&lt;a href=&quot;#begelman2006automated&quot;&gt;[9]&lt;/a&gt;使用标签聚类技术，提出了基于标签共现分布相似性的算法，并利用谱聚类实现了标签的聚类分析。王萍和张际平&lt;a href=&quot;#王萍2010一种社会性标签聚类算法&quot;&gt;[10]&lt;/a&gt;把标签共现定义为两个标签用来标注同一资源,并设计了一种基于标签相似性的聚类算法对标签共现网络进行分割,来建立标签聚类簇。&lt;/p&gt;

&lt;p&gt;随着互联网技术的发展，用户的日常生活和互联网建立起了紧密的联系，与此同时，互联网上产生了海量用户数据，海量数据为个性化推荐系统创造了独一无二的优势，近几年，个性化推荐技术逐渐成为众多研究者的研究热点&lt;a href=&quot;#Xu:2006:CAS:2114193.2114262&quot;&gt;[11]&lt;/a&gt;&lt;a href=&quot;#yin2013connecting&quot;&gt;[12]&lt;/a&gt;。文献&lt;a href=&quot;#基于社会化标签的协同过滤推荐策略研究&quot;&gt;[13]&lt;/a&gt;和文献&lt;a href=&quot;#lacic2014recommending&quot;&gt;[14]&lt;/a&gt;均介绍了各种类型的成熟推荐技术，这些推荐技术各有利弊，分别适用于不同类型不同场景下的推荐系统。如Alexandrin Popescul等提出概率框架，合并基于内容和基于协同过滤的方法，加上EM算法学习的二次内容信息用于解决稀疏问题，辅助混合模型推荐。&lt;/p&gt;

&lt;p&gt;微博是Web2.0的重要应用，其中包含了丰富的网络和用户信息，在微博中标签是一种表示用户兴趣和属性的有效方式，一个用户的兴趣也通常隐藏在他/她的文本和网络中，Zhiyuan Liu提出一种概率模型，网络正则化的概率标签模型NTDM&lt;a href=&quot;#涂存超:24&quot;&gt;[15]&lt;/a&gt;,用来进行微博用户标签推荐，NTDM用来对微博个人介绍中的词和语义关系进行建模，同时将其所在的网络结构信息通过正则化的方式考虑进来，产生了很好的效果。
社会化标签技术对个性化推荐的精确、高效起到了推进作用&lt;a href=&quot;#kumar2014exploiting&quot;&gt;[16]&lt;/a&gt;，Chatti等在个人学习环境(PLE)设置不同的标签来研究基于标签的协同过滤算法，Godoy等实现以标签为基础的分类结果证明基于标签的分类优于那些使用文本的文档以及其他内容相关的来源的推荐效果&lt;a href=&quot;#godoy2012one&quot;&gt;[17]&lt;/a&gt;；Yoshida等使用通过结合标签的排名和基于内容的过滤得出标签的相关性水平排名，从而提高项目推荐性能&lt;a href=&quot;#yoshida2012improving&quot;&gt;[18]&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;关于微微博用户具有结构差异特性，不同用户因其自身属性及其所在关系位置的不同，所以其在关系中所处的位置和交互方式也各不相同，并逐步形成一定的影响力，用户影响力分析主要研究如何基于用户的交互活动水平来研究用户与用户是如何相互影响以及研究用户在社交网络中影响力的大小。在社区中影响力大的用户是关键用户（或称意见领袖），能在一定程度上引导舆论，影响用户行为和政治观点等。&lt;/p&gt;

&lt;h2 id=&quot;3-介绍模型和大数据处理的相关技术&quot;&gt;3. 介绍模型和大数据处理的相关技术&lt;/h2&gt;

&lt;h3 id=&quot;31-分布式向量表示&quot;&gt;3.1 分布式向量表示&lt;/h3&gt;

&lt;h4 id=&quot;311-统计语言模型&quot;&gt;3.1.1 统计语言模型&lt;/h4&gt;
&lt;p&gt;word2vec 是 Google 于 2013 年开源推出的一个用于获取 word vector 的工具包&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;，它简单、高效，因此引起了很多人的关注，word2vec是用来生成词向量的工具，而词向量和语言模型有着密切的联系。当今的互联网迅猛发展，每天都在产生大量的文本，图片，语言和视频数据，要从这些数据处理并挖掘出有价值的信息，离不开自然语言处理（Nature Language Processing，NLP）技术，其中统计语言模型（Statistical Language Model）就是很重要的一环，它是所有NLP的基础，被广泛应用于语音识别，机器翻译，分词，词性标注和信息检索等任务。统计语言模型是用来计算一个句子的概率的概率模型，他通常基于一个语料库来构建。假设\(W = w_1^T :=(w_1,w_2,\cdots,w_T)\)表示由T个词\(w_1,w_2,\cdots,w_T\)按顺序构成一个句子，则\(w_1,w_2,\cdots,w_T\)的联合概率&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(W) = p(w_1^T) = p(w_1,w_2,\cdots,w_T)
\label{eq:eq9}
\end{equation}
就是这个句子的概率，利用Bayes公式，上式可以被链式分解为&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w_1^T) = p(w_1)\bullet p(w_2|w_1) \bullet p(w_3|w_1^2) \cdots p(w_T|w_1^{T-1})
\label{eq:eq10}
\end{equation}
其中的条件概率\(p(w_1)\bullet p(w_2|w_1) \bullet p(w_3|w_1^2) \cdots p(w_T|w_1^{T-1})\)就是语言模型的参数，那么给定一个句子\(w_1^T\)就可以很快地算出相应的\(p(w_1^T)\)了。
刚才我们考虑了一个给定长度为\(T\)的句子，就需要计算\(T\)个参数，假设对应词典\(D\)的大小(即词汇量)为\(N\),那么如果考虑长度为T的任意句子，总过就需要\(T \bullet N^T\)个参数，这些参数的量级是很大的。&lt;/p&gt;

&lt;h4 id=&quot;312-n-gram模型&quot;&gt;3.1.2 n-gram模型&lt;/h4&gt;
&lt;p&gt;n-gram模型可以用来计算上述的参数，考虑\(p(w_k|w_1^{k-1})(k &amp;gt; 1)\)的近似计算。利用Bayes公式有&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w_k|w_1^{k-1}) = {p(w_1^k) \over p(w_1^{k-1})}
\label{eq:eq11}
\end{equation}
根据大数定理，当语料库足够大时，
\begin{equation}
p(w_k|w_1^{k-1}) \approx {count(w_1^k) \over count(w_1^{k-1})}
\label{eq:eq12}
\end{equation}
其中\(count(w_1^k)\)和\(count(w_1^{k-1})\)在表示词串\(w_1^k\)和\(w_1^{k-1}\)语料中出现的次数,当k很大时，统计会很耗时。从公式\eqref{eq:eq10}可以看出，一个词出现的概率与它前面的所有词都相关。n-gram模型的基本思想是一个词出现的概率和它前面固定数目的词相关，它做了一个\(n-1\)阶的Markov假设，认为一个词出现的概率只与它前面的\(n-1\)个词相关，即
&lt;script type=&quot;math/tex&quot;&gt;p(w\_k|w\_1^{k-1}) \approx p(w\_k|w\_{k-n+1}^{k-1})&lt;/script&gt;
于是公式\eqref{eq:eq12}可以简化为
\begin{equation}
p(w_k|w_1^{k-1}) \approx {count(w_{k-n+1}^k) \over count(w_{k-n+1}^{k-1})}
\label{eq:eq13}
\end{equation}
这样简化，不仅使得参数统计变得更容易，也使得参数总数变得更少了。&lt;/p&gt;

&lt;h4 id=&quot;313-神经概率语言模型&quot;&gt;3.1.3 神经概率语言模型&lt;/h4&gt;
&lt;p&gt;本小节介绍Bengio等人提出的一种神经概率语言模型&lt;a href=&quot;#bengio2003neural&quot;&gt;[20]&lt;/a&gt;，该模型用到了一个重要的工具—词向量。对词典\(D\)中的任意词\(w\)指定一个任意长度的实值向量\(v(w) \in \Bbb R^m\),\(v(w)\)就称为\(w\)的词向量，m为词向量的长度。
图1给出了神经网络的结构示意图，它包括四个层：输入层(Input)，投影层(Projection)，隐藏层(Hidden)和输出层(Output)，其中\(W,U\)分别为投影层与隐藏层以及隐藏层和输出层之间的权值矩阵，\(p,q\)分别为隐藏层和输出层上的偏置向量。
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neuro-network.png&quot; alt=&quot;title for image&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图1 神经网络结构图&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;对于语料\(C\)中的任意一个词\(w\)，将Context(w)取为前\(n-1\)个词(类似于n-gram)，这样二元对\(Context(w),w\)就是一个训练样本了，接下来将讨论
样本\(Context(w),w\)经过如图1所示的神经网络时是如何参与运算的。一旦语料\(C\)和词向量的长度\(m\)给定后，投影层和输出层的规模就确定了，前者为\((n-1)m\),后者为\(N=|D|\),即语料C的词汇量大小，而隐藏层的规模\(n_n\)是可调参数，由用户指定。将输入层的\(n-1\)个词向量按顺序首尾相接地拼起来形成了一个长向量，其长度是\((n-1)m\),有了\(x_w\)了接下来的计算过程就很平凡了，具体为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left\{  
\begin{array}  
{l l}  
z_w &amp;=tanh(Wx_w + p)\\
y_w &amp;=Uz_w+q 
\end{array}  
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;其中tanh为双曲正切函数，用来做隐藏层的激活函数，上式中，tanh作用在向量上表示它作用在向量的每一分量上。经过上述两步计算得到的\(y_w =(y_{w,1},y_{w,2},\cdots,y_{w,N})\)只是一个长度为N的向量，其分量不能表示概率，如果想要\(y_w\)的分量\(y_{w,i}\)表示当上下文为\(Context(w)\)时下一个词恰为词典\(D\)中第i个词的概率，则还需要做一个softmax归一化，归一化后，\(p(w|Context(w))\)就可以表示为&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w|Context(w)) = {e^{y_{w,i_w}} \over \sum_{i=1}^N e^{y_{w,i}}}
\label{eq:eq14}
\end{equation}
其中\(i_w\)表示词\(w\)在词典\(D\)中的索引。
公式\eqref{eq:eq14}给出了概率\(p(w|Context(w))\)的函数表示，即找到了上一节中提到的函数\(F(w,Context(w),\theta)\),其中\(\theta\)是待确定的参数。\(theta\)有两部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;词向量：\(v(w) \in \Re^m \) , \(w \in D\) 以及填充向量&lt;/li&gt;
  &lt;li&gt;神经网络参数：\(W \in \Bbb R^{n_h \times (n-1)m},p \in \Bbb R^{n_h};U \in R^{N \times n_h},q \in \Bbb R^N\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些参数均通过训练算法得到.值得一提的是，通常的机器学习算法中，输入都是已知的，而在上述神经概率语言模型中，输入\(v(w)\)也需要通过训练才能得到。&lt;/p&gt;

&lt;h4 id=&quot;314-词向量的理解&quot;&gt;3.1.4 词向量的理解&lt;/h4&gt;
&lt;p&gt;在NLP任务中，我们将自然语言交给机器学习算法来处理，但机器无法直接理解人类的语言，因此首先要做的事情就是将语言数字化，词向量提供了一种很好的方式。一种最简单的词向量是one-hot representation，他就是用一个很长的向量来表示一个词，向量的长度为词典\(D\)的大小N,向量的分量只有一个1，其余全为0,1的位置对应该词在词典中的索引。但是这种词向量表示又有一些缺点，容易受维数灾难的困扰，尤其是将其应用到Deep Learning的场景时。另一种词向量是Distributed Representation，它最早是Hinton于1986年提出的&lt;a href=&quot;#rumelhart1988learning&quot;&gt;[21]&lt;/a&gt;，可以克服one-hot representation的上述缺点，其基本思想是：通过训练将某种语言中的每一个词映射成一个固定长度的短向量，所有这些向量构成一个词向量空间，而每一个则可以视为该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断他们之间的相似性了。Word2vec采用的就是这种Distributed Representation的词向量。&lt;/p&gt;

&lt;h3 id=&quot;32-基于-hierarchical-softmax-的模型&quot;&gt;3.2 基于 Hierarchical Softmax 的模型&lt;/h3&gt;
&lt;p&gt;word2vec中用到了两个重要模型 - CBOW(Continuous Bag-of-Words Model)模型和Skip-gram模型(Continuous Skip-gram Model)，关于这两个模型，作者Tomas Mikolov在文&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;给出了如图2和图3所示的模型
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow.png&quot; alt=&quot;cbow&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图2 CBOW模型&lt;/p&gt;
    
&lt;/div&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram.png&quot; alt=&quot;skip-gram&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图3 Skip-gram模型&lt;/p&gt;
    
&lt;/div&gt;
&lt;p&gt;对于CBOW和Skip-gram两个模型，word2vec给出了两套框架，他们分别是基于Hierarchical Softmax和Negative Sampling来进行设计，本节介绍基于Hierarchical Softmax的CBOW和Skip-gram模型。在3.1节中我们提到基于神经网络的语言模型的目标函数通常取为如下对数似然函数&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} logP(w|Context(w))
\label{eq:eq15}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中的关键是条件概率函数\(P(w|Context(w))\)的构造，文&lt;a href=&quot;#bengio2003neural&quot;&gt;[20]&lt;/a&gt;中的模型就给出了这个函数的一种构造方法，即公式\eqref{eq:eq14}。对于word2vec中基于Hierarchical Softmax的CBOW模型，优化的目标函数也形如公式\eqref{eq:eq15}；而对于基于Hierarchical Softmax的Skip-gram模型，优化的目标函数则形如：&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} logP(Context(w)|w)
\label{eq:eq16}
\end{equation}&lt;/p&gt;

&lt;p&gt;下面将介绍\(p(w|Context(w))\)或者\(p(Context(w)|w)\)的构造。&lt;/p&gt;

&lt;h4 id=&quot;321-cbow模型&quot;&gt;3.2.1 CBOW模型&lt;/h4&gt;
&lt;p&gt;本小节介绍word2vec中的第一个模型—CBOW模型。&lt;/p&gt;
&lt;h5 id=&quot;3211-网络结构&quot;&gt;3.2.1.1 网络结构&lt;/h5&gt;
&lt;p&gt;图四给出了CBOW模型的网路结构，它包括三层:输入层、投影层、输出层。下面以样本\(Context(w),w\)为例(这里假设Context(w)由w前后各c个词构成)，下面对这三个层作简要说明。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;输入层&lt;/strong&gt;: 包含Context(w)中的2c个词的词向量\(v(Context(w)_1),v(Context(w)_2),\cdots,v(Context(w)_{2c}) \in R^m\),这里,m的含义同上表示词向量的长度。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;投影层&lt;/strong&gt;: 将输入层的2c个向量做求和累加，即\(x_w = \sum_{i=1}^2c v(Context(w)_i) \in R^m\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;输出层&lt;/strong&gt;: 输出层对应一棵二叉树，它是以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权值构造出来Huffman树，在这颗Huffman树中，叶子节点
共N(=|D|)个，分别对应词典D中的词，非叶子节点N-1个(图中标成黄色的那些顶点)。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对比神经概率语言模型的网络图(见图2和图3)和CBOW模型的结构图(见图4)，易知它们主要有以下三处不同:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(从输入层到投影层的操作) 前者是通过拼接，后者通过累加求和。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(隐藏层) 前者有隐藏层，后者无隐藏层。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(输出层) 前者是线性结构，后者树形结构。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow-net.png&quot; alt=&quot;cbow-net&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图4 CBOW模型的网络结构&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;在3.1.3节介绍的神经概率语言模型中，我们指出，模型的大部分计算集中在隐藏层和输出层之间的矩阵向量运算，以及输出层上的softmax归一化运算。而从上面的对比中可见，CBOW模型对这些计算复杂度高的地方有针对性的进行了改变，首先去掉了隐藏层，其次，输出层改用Huffman树，从而为利用Hierarchical softmax技术奠定了基础。&lt;/p&gt;

&lt;h5 id=&quot;3212-梯度计算&quot;&gt;3.2.1.2 梯度计算&lt;/h5&gt;

&lt;p&gt;Hierarchical Softmax是word2vec中用于提高性能的一项关键技术，为了描述方便起见，在具体介绍这个技术之前，先引入若干相关记号。考虑Huffman树中的某个叶子节点，假设它对应词典D中的词w，记&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(p^w\)：从根结点出发到达\(w\)对应&lt;/li&gt;
  &lt;li&gt;\(l^w\)：路径\(p^w\)中包含的结点的个数&lt;/li&gt;
  &lt;li&gt;\(p_q^w,p_2^w,\cdots,p_{l^w}^w\): 路径\(p^w\)中的\(l^w\)个结点,其中\(p_1^w\)表示根节点,\(p_{l^w}^w\)表示词w对应的结点。&lt;/li&gt;
  &lt;li&gt;\(d_2^w,d_3^w,\cdots,d_{l^w}^w \in {0,1}\)：词w的Huffman编码，它由\(l^w-1\)位编码构成，\(d_j^w\)表示路径\(p^w\)中第j个结点对应的编码(根节点不对应编码)。&lt;/li&gt;
  &lt;li&gt;\(\theta_1^w,\theta_2^w,\cdots,\theta \in R^m\)：路径\(p^w\)中非叶子结点对应的向量,\(\theta_j^w\)表示路径\(p^w\)中第j个非叶子结点对应的向量。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;下面介绍如何利用\(x_w \in R^m\) 以及Huffman树来定义函数\(p(w|Context(w))\),我们从二分类的角度考虑问题，那么对于每一个非叶子结点，就需要为其左右孩子结点指定一个类别，即哪一个是正类(标签为1)，哪一个是负类(标签为0)。碰巧，除了根节点以外，树中每个结点都对应了一个取值为0或1的Huffman编码。因此，一种最自然的做法就是将Huffman编码为0的结点定义为正类，编码为1的结点定义为
负类，word2vec选用的这个约定：&lt;/p&gt;

&lt;p&gt;\begin{equation}
Lable(P_i^w) = 1 - d_i^w, i = 2,3,4,\cdots,l^w
\label{eq:eq17}
\end{equation}
所以根据逻辑回归，一个结点分为正类的概率是\(\sigma(x_w^T \theta) = {1 \over 1 + e^{-x_w^T \theta}}\),被分为负类的概率为\(1-\sigma(x_w^T \theta)\),其中\(\theta\)是待定参数，非叶子结点对应的那些向量\(\theta_i^w\)就可以扮演参数\(\theta\)的角色。对于词典D中的任意词w，Huffman树中必存在一条从根结点到词w对应结点的路径\(p^w\)（且这条路径是唯一的）。路径\(p^w\)上存在\(l^w-1\)个分支，将每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来，就是所需的\(P(w|Context(w))\)。
条件概率&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(w|Context(w)) = \prod_{j=2}^{l^w}p(d_j^w|X_w,\theta_{j-1}^w)
\label{eq:eq18}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(p(d_j^w|X_w,\theta_{j-1}^w) = [\sigma(X_w^T\theta_{j-1}^w)]^{1-d_j^w} \bullet [1-\sigma(X_w^T\theta_{j-1}^w)]^{d_j^w}\)
将公式\eqref{eq:eq18}带入公式\eqref{eq:eq15}得到&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{j=2}^{l^w}{(1-d_j^w)log[\sigma(X_w^T\theta_{j-1}^w)]+d_j^wlog[1-\sigma(X_w^T\theta_{j-1}^w)]}
\label{eq:eq19}
\end{equation}&lt;/p&gt;

&lt;p&gt;至此，已经推导出了对数似然函数\eqref{eq:eq19}，这个就是CBOW模型的目标函数，下面利用随机梯度下降法来优化这个目标函数，观察目标函数\eqref{eq:eq19}易知，该函数中的参数包括向量\(X_w,\theta_{j-1}^w,w \in C,j = 2,\cdots,l^w\)。通过求导可得&lt;/p&gt;

&lt;p&gt;\begin{equation}
\theta_{j-1}^w := \theta_{j-1}^w + \eta[1 - d_j^w - \sigma(X_w^T\theta_{j-1}^w)]X_w
\label{eq:eq20}
\end{equation}
其中\(\eta\)表示学习率，下同。&lt;/p&gt;

&lt;p&gt;\begin{equation}
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_{j=2}^{l^w} {\partial L(w,j) \over \partial X_w }, \widetilde{w} \in Context(w)
\label{eq:eq21}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中
\[
{\partial L(w,j) \over \partial X_w } = [1-d_j^w -\sigma(X_w^T\theta_{j-1}^w)]\theta_{j-1}^w
\]
下面以样本(Context(w),w)为例，给出了CBOW模型中采用随机梯度下降法更新各参数的伪代码
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/cbow-pseudocode.png&quot; alt=&quot;cbow-pseudocode.png&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图5 CBOW模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h4 id=&quot;322-skip-gram模型&quot;&gt;3.2.2 Skip-gram模型&lt;/h4&gt;
&lt;p&gt;本小结介绍word2vec中的另一个模型—Skip-gram模型，由于推导过程与CBOW大同小异，因此会沿用上节引入的记号。&lt;/p&gt;
&lt;h5 id=&quot;3221-网络结构&quot;&gt;3.2.2.1 网络结构&lt;/h5&gt;

&lt;p&gt;图6给出了Skip-gram模型的网络结构，同CBOW模型网络结构也一样，它也包括三层：输入层，投影层，输出层，下面以样本\(w,Context(w)\)为例，对这三个层做简要说明。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;输入层&lt;/strong&gt;：只含有当前样本中心词\(w\)的词向量\(v(w) \in R^m\)。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;投影层&lt;/strong&gt;: 这是个恒等投影，把\(v(w)\)投影到\(v(w)\)。因此，这个投影层其实是多余的，这里之所以保留投影层只要是方便和CBOW模型的网络结构做对比。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;: 和CBOW模型一样，输出层也是一颗Huffman树&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram-net.png&quot; alt=&quot;skip-gram-net&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图6 Skip-gram模型的网络结构示意图&lt;/p&gt;
    
&lt;/div&gt;

&lt;h5 id=&quot;3222-梯度计算&quot;&gt;3.2.2.2 梯度计算&lt;/h5&gt;
&lt;p&gt;对于Skip-gram模型，已知的是当前词w，需要对其上下文Context(w)中的词进行预测，因此目标函数应该形如公式\eqref{eq:eq16},且关键是条件概率函数\(p(Context(w)|w)\)的构造，
Skip-gram模型将其定义为
\[
p(Context(w)|w) = \prod_{u \in Context(w)} p(u|w)
\]
上式中的\(p(u|w)\)可以按照上一小节介绍的Hierarchical Softmax思想，写为
\[
p(u|w) = \prod_{j=2}^{l^u} p(d_j^u|v(w),\theta_{j-1}^u)
\]
其中&lt;/p&gt;

&lt;p&gt;\begin{equation}
p(d_j^u|v(w),\theta_{j-1}^u) = [\sigma(v(w)^T\theta_{j-1}^u)]^{1-d_j^u}[1-\sigma(v(w)^T\theta_{j-1}^u)]^{d_j^u}
\label{eq:eq22}
\end{equation}&lt;/p&gt;

&lt;p&gt;将公式\eqref{eq:eq22}依次带回目标函数\eqref{eq:eq16}可以得到对数似然函数的具体表达式&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{u \in Context(w)} \sum_{j=2}^{l^u}  (1-d_j^u)log[\sigma(v(w)^T\theta_{j-1}^u)]+d_j^ulog[1-\sigma(v(w)^T\theta_{j-1}^u)]
\label{eq:eq23}
\end{equation}
下面对目标函数求导后可知参数的更新公式为：
\[
\theta_{j-1}^u := \theta_{j-1}^u + \eta[1-d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]v(w)
\]&lt;/p&gt;

&lt;p&gt;\[
v(w) := v(w) + \eta \sum_{u \in Context(w)} \sum_{j=2}^{l^u} {\partial L(w,u,j) \over \partial v(w)}
\]&lt;/p&gt;

&lt;p&gt;其中
\[
 {\partial L(w,u,j) \over \partial v(w)} = [1 -d_j^u - \sigma(v(w)^T\theta_{j-1}^u)]\theta_{j-1}^u
\]
下面以样本\(w,Context(w)\)为例，给出Skip-gram模型中采用随机梯度下降法更新各参数的伪代码
&lt;!-- _includes/image.html --&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/skip-gram-pseudocode.png&quot; alt=&quot;skip-gram-pseudocode&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图7 Skip-gram模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h3 id=&quot;33-基于negative-sampling的模型&quot;&gt;3.3 基于Negative Sampling的模型&lt;/h3&gt;
&lt;p&gt;本节将介绍基于Negative Sampling的CBOW和Skip-gram模型。Negative Sampling(简称NEG)是Tomas Mikolov等人在&lt;a href=&quot;#mikolov2013distributed&quot;&gt;[19]&lt;/a&gt;中提出的，它是NCE(Noise Contrastive Estimation)的一个简化版本，目的是用来提高训练速度并改善所得词向量的质量。与Hierarchical Softmax相比，NEG不再使用复杂的Huffman树，而是利用(相对简单的)随机负采样，能大幅提高性能，因此可作为Hierarchical Softmax的一种替代。&lt;/p&gt;

&lt;h4 id=&quot;331-cbow模型&quot;&gt;3.3.1 CBOW模型&lt;/h4&gt;
&lt;p&gt;在CBOW模型中，已知词w的上下文Context(w),需要预测w,因此对于给定的Context(w)，词w就是一个正样本，其他词就是负样本，现在假定已经选好了一个关于w的负样本子集\(NEG(w) \neq \emptyset \)
且对\(\forall \widetilde{w} \in D\),定义&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(n) =  
\begin{cases}  
1, &amp;\text{$\widetilde{w} = w$} \\[2ex]
0, &amp;\text{$\widetilde{w} \neq w$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;表示词\(\widetilde{w}\)的标签，即正样本标签为1，负样本标签为0.&lt;/p&gt;

&lt;p&gt;给定一个正样本(Context(w),w),我们希望最大化&lt;/p&gt;

&lt;p&gt;\begin{equation}
g(w) = \prod_{u \in {w}\bigcup NEG(w)} p(u|Context(w))
\label{eq:eq24}
\end{equation}
其中&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(u|Context(w)) =  
\begin{cases}  
\sigma(X_w^T\theta^u), &amp;\text{$L^w(u) =1$} \\[2ex]
1-\sigma(X_w^T\theta^u), &amp;\text{$L^w(u)=0$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;这里\(X_w\)仍表示Context(w)中各词的词向量之和，而\(\theta^u \in R^m\)表示词u对应的一个辅助向量，为待训练参数。负采样的思想是增大正样本的概率同时降低负样本的概率，于是，
对于一个给定的语料库\(C\),函数&lt;/p&gt;

&lt;p&gt;\[
G = \prod _{w \in C}g(w)
\]
就可以作为整体优化目标，为了计算方便，对G取对数，最终的目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = \sum_{w \in C} \sum_{u \in {w} \bigcup NEG(w) }L^w(u)log[\sigma(x_w^T \theta^u)]+[1-L^w(u)]log[1- \sigma(x_w^T \theta^u)]
\label{eq:eq25}
\end{equation}&lt;/p&gt;

&lt;p&gt;利用随机梯度下降来计算参数的更新公式：
\[
\theta^u := \theta^u + \eta[L^w(u) -\sigma(x_w^T\theta^u)]x_w
\]&lt;/p&gt;

&lt;p&gt;\[
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_{u \in {w} \bigcup NEG(w)} {\partial L(w,u) \over \partial x_w} ,\widetilde{w} \in Context(w)
\]&lt;/p&gt;

&lt;p&gt;其中
\[
{\partial L(w,u) \over \partial x_w} = [L^w(u) - \sigma(x_w^T\theta^u)]\theta^u
\]
下面以样本(Context(w),w)为例，给出基于Negtive Sampling的CBOW模型中采用随机梯度下降法更新各参数的伪代码&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-cbow-code.png&quot; alt=&quot;neg-cbow-code&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图8 基于负采样的CBOW模型训练方法&lt;/p&gt;
    
&lt;/div&gt;
&lt;h4 id=&quot;332-skip-gram模型&quot;&gt;3.3.2 Skip-gram模型&lt;/h4&gt;
&lt;p&gt;本小节介绍基于Negative Sampling的Skip-gram模型，将CBOW下的目标函数改写为&lt;/p&gt;

&lt;p&gt;\begin{equation}
G = \prod_{w \in C} \prod_{u \in Context(w)} g(u)
\label{eq:eq26}
\end{equation}&lt;/p&gt;

&lt;p&gt;这里\(\prod_{u \in Context(w)} g(u)\)表示对于一个给定的样本(w,Context(w)),我们希望最大化的量，\(g(u)\)类似于上一节的\(g(w)\),定义为&lt;/p&gt;

&lt;p&gt;\begin{equation}
g(u) = \prod_ {z \in u \bigcup NEG(u)} p(z|w)
\label{eq:eq27}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中NEG(u)表示处理词u时生成的负样本子集，条件概率&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p(z|w) =  
\begin{cases}  
\sigma(v(w)^T\theta^z), &amp;\text{$L^u(z) =1;$} \\[2ex]
1-\sigma(v(w)^T\theta^z), &amp;\text{$L^u(z)=0$}  
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以最终的目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
L = logG = \sum_{w \in C} \sum_{\widetilde{w} \in Context(w)} \sum_ {u \in {w} \bigcup NEG^{\widetilde{w}}(w)}{L^w(u)log[\sigma(v(\widetilde{w})^T \theta^u)]+[1-L^w(u)]log[1-\sigma(v(\widetilde{w})^T \theta^u)]} 
\label{eq:eq28}
\end{equation}&lt;/p&gt;

&lt;p&gt;于是\(\theta^u\)的更新公式可写为
\[
\theta^u := \theta^u + \eta[L^w(u) - \sigma(v(\widetilde{w})^T\theta^u)]v(\widetilde{w})
\]&lt;/p&gt;

&lt;p&gt;\(v(\widetilde{w})\)的更新公式可以写为&lt;/p&gt;

&lt;p&gt;\[
v(\widetilde{w}) := v(\widetilde{w}) + \eta \sum_ {u \in {w} \bigcup NEG^{\widetilde{w}}(w)} {\partial L(w,\widetilde{w},u) \over \partial v(\widetilde{w})}
\]
下面以样本(w,Context(w))为例,给出基于Negative Sampling的Skip-gram模型中采用随机梯度下降法更新各参数的伪代码&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/neg-skip-gram-code.png&quot; alt=&quot;neg-skip-gram-code&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图9 基于负采样的Skip-gram模型训练方法&lt;/p&gt;
    
&lt;/div&gt;

&lt;h3 id=&quot;34-大规模消息网络嵌入式表示&quot;&gt;3.4 大规模消息网络嵌入式表示&lt;/h3&gt;

&lt;p&gt;大规模消息网路嵌入式表示(Large-scale Information Network Embeding，简记LINE)主要是用来研究大规模消息网络结点间的关系，并用低维向量空间来表示该网络结构，这项技术广泛应用在多个领域，比如：数据可视化，结点分类，链接预测。LINE相比于
当前存在的图嵌入技术&lt;a href=&quot;#tenenbaum2000global&quot;&gt;[22]&lt;/a&gt; &lt;a href=&quot;#belkin2001laplacian&quot;&gt;[23]&lt;/a&gt;的主要优点是它可以适用于现实世界中的真实网络(拥有上百万顶点，上千万边的网络)。LINE适用于任意类型的消息网络，比如说，无向网络，有向网络，
带权重的网络。这种方法通过优化保留了局部和全局网络结构的目标函数来刻画消息网络的特征从而得到每一个结点的低维向量表示。局部网络结构，又称一阶相似性，捕获的是网络中两个顶点的链接关系，大部分图嵌入方法均可以保留一阶相似性，比如IsoMap&lt;a href=&quot;#tenenbaum2000global&quot;&gt;[22]&lt;/a&gt;。由于在现实世界的真实网络中，很多合理的链接并没有被捕获，仅仅通过一阶相似性不足以表示全局网络结构，文&lt;a href=&quot;#tang2015line&quot;&gt;[5]&lt;/a&gt;中提出了结点间的二阶相似性。二阶相似性通过判断两个结点间是否共享邻居来判断这两个结点是否相似，这个思想和我们直观上的想法—“我们可以通过某人的朋友来了解一个人”。&lt;/p&gt;

&lt;p&gt;下面举例说明，在图10中的消息网络中，边可以是有向的，无向的或者带权重的。顶点6和顶点7由于存在边直接连接，所以顶点6和顶点7存在一阶相似性。顶点5和顶点6由于共享相同的邻居(顶点1，顶点2，顶点3，顶点4),所以顶点5和顶点6具有二阶相似性。&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/line-2-prox.png&quot; alt=&quot;line-2-prox&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;图10 网络中的二阶相似性&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;下面具体来用数学公式来刻画一阶相似性和二阶相似性。&lt;/p&gt;

&lt;h4 id=&quot;341-一阶相似性&quot;&gt;3.4.1 一阶相似性&lt;/h4&gt;

&lt;p&gt;一阶相似性指的是网络中两个顶点的局部成对相似性，为了对一阶相似性建模，对于任意无向边\((i,j)\),我们定义了顶点\(v_i\)和\(v_j\)的联合概率如下：&lt;/p&gt;

&lt;p&gt;\begin{equation}
p_ 1(v_ i,v_ j) = {1 \over 1+exp(-\vec{u_ i}^T \cdot \vec{u _ j})}
\label{eq:eq29}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(\vec{u_ i} \in R^d\)是顶点\(v_ i\)的低维向量表示，\eqref{eq:eq29}定义了在\(V \times V\)空间上的一个概率分布\(p(\cdot,\cdot)\),它的经验分布为
\(\hat{p_ 1} = {w_ {ij} \over W}\),其中\(W = \sum _{(i,j) \in E} w_{ij}\),为了保留一阶相似性，一个最直接的想法就是最小化前面两个分布的距离，即目标函数为&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_ 1 = - \sum _ {(i,j) \in E} w_ {ij} \log p_ 1(v_ i,v_ j) 
\label{eq:eq30}
\end{equation}&lt;/p&gt;

&lt;p&gt;一阶相似性只适合于无向图，不适合于有向图，通过找到\( \{\vec{u_ i}  \}_{i = 1,\cdots,|V|}\)来最小化目标函数\eqref{eq:eq30}，我们便可以得到每个顶点的\(d\)维向量表示。&lt;/p&gt;

&lt;h4 id=&quot;342-二阶相似性&quot;&gt;3.4.2 二阶相似性&lt;/h4&gt;
&lt;p&gt;二阶相似性适用于无向图和有向图，为了不损失一般性，我们考虑一个有向图网络(无向边可以看做两个具有相同权重但是方向相反的有向边)，二阶相似性认为如果两个顶点共享邻居结点的话，那么这两个结点相似。在这种情况下，每一个顶点被指定一个上下文(Context)，如果顶点在上下文上具有相似的分布，那么认为这两个顶点是相似的。
因此，一个顶点担任两个角色，第一，顶点本身；第二，其他顶点的上下文(Context)。所以我们引入两个向量\(\vec{u_i} 和 {\vec{u_i}}^{\prime}\)，其中\(\vec{u_i}\)是\(v_i\)的顶点表示，\({\vec{u_i}}^{\prime}\)是\(v_i\)的上下文表示，对于每一个有向边(i,j)，我们定义通过顶点\(v_i\)生成上下文\(v_j\)的概率如下&lt;/p&gt;

&lt;p&gt;\begin{equation}
p_2(v_ j|v_ i) = {exp({\vec{u_ j}^{\prime}}^T \cdot \vec{u_ i} ) \over \sum _ {k=1} ^{|V|} exp({\vec{u_ k}^{\prime}}^T \cdot \vec{u_ i} )}
\label{eq:eq31}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中\(|V|\)是顶点或者上下文的数量，对于每一个顶点\(v_i\)，公式\eqref{eq:eq31}定义了在\(v_i\)在上下文下的条件概率分布\(p_2(\cdot|v_ i)\),正如上文所说，二阶相似性表示的是，如果顶点在上下文上具有相似的概率分布，那么这些顶点相似。为了保留二阶相似性，我们应使\(p_2(\cdot|v_i)\)与经验分布\(\hat{p_2}(\cdot|v_i)\)距离最近,因此我们定义如下目标函数。&lt;/p&gt;

&lt;p&gt;\begin{equation}
O_2 = \sum_{i \in V} \lambda_ i d(p_2(\cdot|v_i),\hat{p_2}(\cdot|v_i))
\label{eq:eq32}
\end{equation}
其中\(d(\cdot,\cdot)\)表示两个概率分布之间的距离，由于网络中顶点之间的重要性不同，\(\lambda_i\)表示每个顶点的重要性，它可以用每个顶点的度或者PageRank&lt;a href=&quot;#page1999pagerank&quot;&gt;[24]&lt;/a&gt;,其中\(\hat{p_2}(v_j|v_ i) = {w_{ij} \over d_ i}\),\(w_{ij}\)是边(i,j)的权重，\(d_i\)是顶点i的出度，我们通过KL距离来计算\(d(\cdot,\cdot)\)并且\(\lambda_i = d_i\),目标函数可以简化为
\begin{equation}
O_2 = - \sum _{(i,j) \in E} w_{ij}\log p_2(v_j|v_i)
\label{eq:eq33}
\end{equation}&lt;/p&gt;

&lt;p&gt;通过学习出\(\{\vec{u_ i}\}_{i = 1 \cdots |V|}\) 和\(\{\vec{u_ i}^{\prime}\}_{i = 1 \cdots |V|}\)使目标函数达到最小，
我们便可以用一个d维向量\(\vec{u_ i}\)表示每一个顶点\(v_i\)。&lt;/p&gt;

&lt;h2 id=&quot;4-主要工作&quot;&gt;4. 主要工作&lt;/h2&gt;

&lt;h2 id=&quot;5-实验结果&quot;&gt;5. 实验结果&lt;/h2&gt;

&lt;h2 id=&quot;本文总结和对未来的工作展望&quot;&gt;本文总结和对未来的工作展望&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;mathes2004folksonomies&quot;&gt;[1]A. Mathes, “Folksonomies-cooperative classification and communication through shared metadata.” December, 2004.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;JiAT2007Collaborative&quot;&gt;[2]JiAT, “Collaborative Tagging in Recommender Systems,” in &lt;i&gt;AI 2007: Advances in Artificial Intelligence&lt;/i&gt;, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;shiratsuchi2006finding&quot;&gt;[3]K. Shiratsuchi, S. Yoshii, and M. Furukawa, “Finding unknown interests utilizing the wisdom of crowds in a social bookmark service,” in &lt;i&gt;Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology&lt;/i&gt;, 2006, pp. 421–424.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;clauset2005finding&quot;&gt;[4]A. Clauset, “Finding local community structure in networks,” &lt;i&gt;Physical review E&lt;/i&gt;, vol. 72, no. 2, p. 026132, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tang2015line&quot;&gt;[5]J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, “Line: Large-scale information network embedding,” in &lt;i&gt;Proceedings of the 24th International Conference on World Wide Web&lt;/i&gt;, 2015, pp. 1067–1077.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;michlmayr2007learning&quot;&gt;[6]E. Michlmayr and S. Cayzer, “Learning user profiles from tagging data and leveraging them for personal (ized) information access,” 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ecs14007&quot;&gt;[7]M. Szomszor &lt;i&gt;et al.&lt;/i&gt;, “Folksonomies, the Semantic Web, and Movie Recommendation ,” in &lt;i&gt;4th European Semantic Web Conference, Bridging the Gap between Semantic Web and Web 2.0&lt;/i&gt;, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipp2006patterns&quot;&gt;[8]M. E. I. Kipp and D. G. Campbell, “Patterns and inconsistencies in collaborative tagging systems: An examination of tagging practices,” &lt;i&gt;Proceedings of the American Society for Information Science and Technology&lt;/i&gt;, vol. 43, no. 1, pp. 1–18, 2006.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;begelman2006automated&quot;&gt;[9]G. Begelman, P. Keller, F. Smadja, and others, “Automated tag clustering: Improving search and exploration in the tag space,” in &lt;i&gt;Collaborative Web Tagging Workshop at WWW2006, Edinburgh, Scotland&lt;/i&gt;, 2006, pp. 15–33.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;王萍2010一种社会性标签聚类算法&quot;&gt;[10]王萍 and 张际平, “一种社会性标签聚类算法,” &lt;i&gt;计算机应用与软件&lt;/i&gt;, vol. 27, no. 2, pp. 126–129, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Xu:2006:CAS:2114193.2114262&quot;&gt;[11]Y. Xu, L. Zhang, and W. Liu, “Cubic Analysis of Social Bookmarking for Personalized Recommendation,” in &lt;i&gt;Proceedings of the 8th Asia-Pacific Web Conference on Frontiers of WWW Research and Development&lt;/i&gt;, Berlin, Heidelberg, 2006, pp. 733–738.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yin2013connecting&quot;&gt;[12]D. Yin, S. Guo, B. Chidlovskii, B. D. Davison, C. Archambeau, and G. Bouchard, “Connecting comments and tags: improved modeling of social tagging systems,” in &lt;i&gt;Proceedings of the sixth ACM international conference on Web search and data mining&lt;/i&gt;, 2013, pp. 547–556.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;基于社会化标签的协同过滤推荐策略研究&quot;&gt;[13]万朔 and 邱会中, “基于社会化标签的协同过滤推荐策略研究,” &lt;i&gt;电子科技大学&lt;/i&gt;, pp. 14–16, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lacic2014recommending&quot;&gt;[14]E. Lacic, D. Kowald, P. Seitlinger, C. Trattner, and D. Parra, “Recommending items in social tagging systems using tag and time information,” &lt;i&gt;arXiv preprint arXiv:1406.7727&lt;/i&gt;, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;涂存超:24&quot;&gt;[15]涂存超 孙茂松, “社会媒体用户标签的分析与推荐,” &lt;i&gt;图书情报工作&lt;/i&gt;, vol. 57, no. 23, p. 24, 2013.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kumar2014exploiting&quot;&gt;[16]H. Kumar, S. Lee, and H.-G. Kim, “Exploiting social bookmarking services to build clustered user interest profile for personalized search,” &lt;i&gt;Information Sciences&lt;/i&gt;, vol. 281, pp. 399–417, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;godoy2012one&quot;&gt;[17]D. Godoy, “One-class support vector machines for personalized tag-based resource classification in social bookmarking systems,” &lt;i&gt;Concurrency and Computation: Practice and Experience&lt;/i&gt;, vol. 24, no. 17, pp. 2193–2206, 2012.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yoshida2012improving&quot;&gt;[18]T. Yoshida, G. Irie, T. Satou, A. Kojima, and S. Higashino, “Improving item recommendation based on social tag ranking,” in &lt;i&gt;International Conference on Multimedia Modeling&lt;/i&gt;, 2012, pp. 161–172.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mikolov2013distributed&quot;&gt;[19]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;, 2013, pp. 3111–3119.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bengio2003neural&quot;&gt;[20]Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language model,” &lt;i&gt;Journal of machine learning research&lt;/i&gt;, vol. 3, no. Feb, pp. 1137–1155, 2003.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rumelhart1988learning&quot;&gt;[21]D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” &lt;i&gt;Cognitive modeling&lt;/i&gt;, vol. 5, no. 3, p. 1, 1988.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tenenbaum2000global&quot;&gt;[22]J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” &lt;i&gt;science&lt;/i&gt;, vol. 290, no. 5500, pp. 2319–2323, 2000.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;belkin2001laplacian&quot;&gt;[23]M. Belkin and P. Niyogi, “Laplacian eigenmaps and spectral techniques for embedding and clustering,” in &lt;i&gt;NIPS&lt;/i&gt;, 2001, vol. 14, no. 14, pp. 585–591.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;page1999pagerank&quot;&gt;[24]L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web.,” Stanford InfoLab, 1999.&lt;/span&gt;



&lt;/li&gt;&lt;/ul&gt;
</description>
      </item>
    
      <item>
        <title>用jekyll写博客时猜过的一些坑</title>
        <link>http://localhost:4000/2017/03/31/learn-jekyll-markdown.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/31/learn-jekyll-markdown.html</guid>
        <pubDate>Fri, 31 Mar 2017 14:42:29 +0800</pubDate>
        <description>&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{x^2}{\sqrt{y+1}}&lt;/script&gt;

&lt;h3 id=&quot;2015&quot;&gt;2015&lt;/h3&gt;

&lt;ul class=&quot;bibliography&quot;&gt;&lt;/ul&gt;

&lt;h3 id=&quot;2014&quot;&gt;2014&lt;/h3&gt;

&lt;ul class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;michlmayr2007learning&quot;&gt;[1]E. Michlmayr and S. Cayzer, “Learning user profiles from tagging data and leveraging them for personal (ized) information access,” 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kipp2006patterns&quot;&gt;[2]M. E. I. Kipp and D. G. Campbell, “Patterns and inconsistencies in collaborative tagging systems: An examination of tagging practices,” &lt;i&gt;Proceedings of the American Society for Information Science and Technology&lt;/i&gt;, vol. 43, no. 1, pp. 1–18, 2006.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;基于社会化标签的协同过滤推荐策略研究&quot;&gt;[3]万朔 and 邱会中, “基于社会化标签的协同过滤推荐策略研究,” &lt;i&gt;电子科技大学&lt;/i&gt;, pp. 14–16, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;王萍2010一种社会性标签聚类算法&quot;&gt;[4]王萍 and 张际平, “一种社会性标签聚类算法,” &lt;i&gt;计算机应用与软件&lt;/i&gt;, vol. 27, no. 2, pp. 126–129, 2010.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lacic2014recommending&quot;&gt;[5]E. Lacic, D. Kowald, P. Seitlinger, C. Trattner, and D. Parra, “Recommending items in social tagging systems using tag and time information,” &lt;i&gt;arXiv preprint arXiv:1406.7727&lt;/i&gt;, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kumar2014exploiting&quot;&gt;[6]H. Kumar, S. Lee, and H.-G. Kim, “Exploiting social bookmarking services to build clustered user interest profile for personalized search,” &lt;i&gt;Information Sciences&lt;/i&gt;, vol. 281, pp. 399–417, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chatti2013tag&quot;&gt;[7]M. A. Chatti, S. Dakova, H. Thus, and U. Schroeder, “Tag-based collaborative filtering recommendation in personal learning environments,” &lt;i&gt;IEEE Transactions on Learning Technologies&lt;/i&gt;, vol. 6, no. 4, pp. 337–349, 2013.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;涂存超:24&quot;&gt;[8]涂存超 孙茂松, “社会媒体用户标签的分析与推荐,” &lt;i&gt;图书情报工作&lt;/i&gt;, vol. 57, no. 23, p. 24, 2013.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;godoy2012one&quot;&gt;[9]D. Godoy, “One-class support vector machines for personalized tag-based resource classification in social bookmarking systems,” &lt;i&gt;Concurrency and Computation: Practice and Experience&lt;/i&gt;, vol. 24, no. 17, pp. 2193–2206, 2012.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tenenbaum2000global&quot;&gt;[10]J. B. Tenenbaum, V. De Silva, and J. C. Langford, “A global geometric framework for nonlinear dimensionality reduction,” &lt;i&gt;science&lt;/i&gt;, vol. 290, no. 5500, pp. 2319–2323, 2000.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;clauset2005finding&quot;&gt;[11]A. Clauset, “Finding local community structure in networks,” &lt;i&gt;Physical review E&lt;/i&gt;, vol. 72, no. 2, p. 026132, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bengio2003neural&quot;&gt;[12]Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language model,” &lt;i&gt;Journal of machine learning research&lt;/i&gt;, vol. 3, no. Feb, pp. 1137–1155, 2003.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rumelhart1988learning&quot;&gt;[13]D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” &lt;i&gt;Cognitive modeling&lt;/i&gt;, vol. 5, no. 3, p. 1, 1988.&lt;/span&gt;



&lt;/li&gt;&lt;/ul&gt;

&lt;ul class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;lacic2014recommending&quot;&gt;[1]E. Lacic, D. Kowald, P. Seitlinger, C. Trattner, and D. Parra, “Recommending items in social tagging systems using tag and time information,” &lt;i&gt;arXiv preprint arXiv:1406.7727&lt;/i&gt;, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kumar2014exploiting&quot;&gt;[2]H. Kumar, S. Lee, and H.-G. Kim, “Exploiting social bookmarking services to build clustered user interest profile for personalized search,” &lt;i&gt;Information Sciences&lt;/i&gt;, vol. 281, pp. 399–417, 2014.&lt;/span&gt;



&lt;/li&gt;&lt;/ul&gt;

&lt;h2 id=&quot;如何利用_drafts&quot;&gt;如何利用_drafts&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;在根目录下创建_drafts文件夹，然后可以在其中创建无时间命名的markdown文档，比如a-draft-article.md,
如果想提前预览，可以输入&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;jekyll build –drafts&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;jekyll会自动连草稿一起编译，草稿的日期为当前的日期。 写完草稿想发布了，把这个文件改成year-month-day-title.md格式放到_posts目录即可。
最后commit&amp;amp;push到GitHub上去即可看到博客内容拉。&lt;/p&gt;

&lt;h2 id=&quot;the-liquid-language&quot;&gt;the Liquid Language&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;Jekyll使用Liquid模板，因此，想要用好Jekyll，需要了解一些关于Liquid相关的知识.
Liquid中存在两种类型的标签: Output and Tag 。其中Output会被解析成文本&lt;/p&gt;

&lt;h2 id=&quot;2-rake&quot;&gt;2. rake&lt;/h2&gt;

&lt;h3 id=&quot;21-何为rake&quot;&gt;2.1 何为Rake&lt;/h3&gt;

&lt;p&gt;即Ruby Make，一个用ruby开发的代码构建工具&lt;/p&gt;

&lt;h3 id=&quot;22-常用命令&quot;&gt;2.2 常用命令&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rake post[&quot;Title&quot;]   直接写博文
rake draft[&quot;Title&quot;]  写草稿
rake publish         将草稿发表
rake page[&quot;Title&quot;]   做页面
    rake page[&quot;Title&quot;,&quot;Path/to/folder&quot;]
rake build
rake watch
    rake watch[number]
    rake watch[&quot;drafts&quot;]
rake preview
rake deploy[&quot;Commit message&quot;]  写完后发布
rake transfer
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&quot;如何配置seo&quot;&gt;如何配置SEO&lt;/h2&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;如何配置analytics&quot;&gt;如何配置Analytics&lt;/h2&gt;

&lt;h2 id=&quot;jekyll里的watch参数是什么含义&quot;&gt;jekyll里的watch参数是什么含义&lt;/h2&gt;

&lt;p&gt;jekyll 2.4版本后，&lt;em&gt;jekyll build&lt;/em&gt; 和 &lt;em&gt;jekyll serve&lt;/em&gt; 默认包含jekyll参数，这个参数将监控博客
文件的改变，并重新自动生成博客。&lt;/p&gt;

&lt;h2 id=&quot;博客中如何插入图片&quot;&gt;博客中如何插入图片&lt;/h2&gt;
&lt;h3 id=&quot;图片的存放&quot;&gt;图片的存放&lt;/h3&gt;
&lt;p&gt;我将需要插入的图片都放置到/public/img下了&lt;/p&gt;
&lt;h3 id=&quot;插入图片的命令&quot;&gt;插入图片的命令&lt;/h3&gt;
&lt;p&gt;在sublime +Markdown Extended编辑模式下只需输入mdi + tab 然后回车(mdl + tab可以插入链接)&lt;/p&gt;

&lt;p&gt;利用markdown插入图片命令&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;![图片](/public/img/horse.jpg &quot;Optional title&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;插入图片不可以控制尺寸，有时图片太大用户体验不好。
我们可以采用html的&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;img&amp;gt;&lt;/code&gt;标签插入图片&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;img src=&quot;/public/img/horse.jpg&quot; width=&quot;50%&quot; height=&quot;50%&quot;&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/horse.jpg&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;atom--markdown-writer--git-plus书写博客的流程&quot;&gt;Atom + Markdown Writer + Git plus书写博客的流程&lt;/h2&gt;

&lt;h2 id=&quot;jekyll-serve-中的-detach参数&quot;&gt;jekyll serve 中的 –detach参数&lt;/h2&gt;
&lt;p&gt;功能和&lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;命令相同，但是会脱离终端在后台运行。
如果你想关闭服务器，可以使用&lt;code class=&quot;highlighter-rouge&quot;&gt;kill -9 1234&lt;/code&gt;命令，”1234” 是进程号（PID）。
如果你找不到进程号，那么就用&lt;code class=&quot;highlighter-rouge&quot;&gt;ps aux | grep jekyll&lt;/code&gt;命令来查看，然后关闭服务器。&lt;/p&gt;

&lt;h2 id=&quot;jekyll-语法高亮配置&quot;&gt;jekyll 语法高亮配置&lt;/h2&gt;

&lt;h2 id=&quot;网上看到的一个发布博客的脚本&quot;&gt;网上看到的一个发布博客的脚本&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;
do_commit&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;git commit -a -m&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$log&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$cmd&lt;/span&gt;
    git add .;
    git commit -a -m&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$log&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
    git push;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$# &lt;/span&gt;-gt 0 &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;do
  case&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;
    -commit &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;-u&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;shift&lt;/span&gt;; &lt;span class=&quot;nv&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;; do_commit; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;0;;
  &lt;span class=&quot;k&quot;&gt;esac
  &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;shift
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;jekyll-输入表格&quot;&gt;jekyll 输入表格&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Default aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Left aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Center aligned&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Right aligned&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;First body part&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Second cell&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Third cell&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fourth cell&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Second line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;foo&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;strong&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;baz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Third line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;quux&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;baz&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;bar&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Second body&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2 line&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tfoot&gt;
    &lt;tr&gt;
      &lt;td&gt;Footer row&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;

&lt;h2 id=&quot;markdown-原样抄写&quot;&gt;markdown 原样抄写&lt;/h2&gt;
&lt;p&gt;在 Markdown 中，我们可以用 backtip ( ` ) 来做原样抄写（Verbatim）。也就是说，包含在两个 ` 中间的内容，会被 Markdown 原样保留下来。&lt;/p&gt;

&lt;h2 id=&quot;markdown-插入数学公式&quot;&gt;markdown 插入数学公式&lt;/h2&gt;
&lt;h3 id=&quot;安装mathjax&quot;&gt;安装mathjax&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;首先通过&lt;a href=&quot;https://cloud.github.com/downloads/timonwong/OmniMarkupPreviewer/mathjax.zip&quot;&gt;mathjax.zip&lt;/a&gt;下载mathjax,并将
其他解压到&lt;code class=&quot;highlighter-rouge&quot;&gt;~/Library/Application Support/Sublime Text 3/Packages/OmniMarkupPreviewer/public&lt;/code&gt;下&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/Library/Application Support/Sublime Text 3/Packages/OmniMarkupPreviewer/&lt;/code&gt;创建空白文件&lt;code class=&quot;highlighter-rouge&quot;&gt;MATHJAX.DOWNLOADED&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;打开 Sublime Text -&amp;gt; Preference -&amp;gt; Package Setting -&amp;gt; OmniMarkupPreviewer -&amp;gt; Settings Default，找到下面一行将
将false 改为true保存&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/mathjax.png&quot; alt=&quot;mathjax&quot; title=&quot;修改配置文件&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在Markdown中插入数学公式的语法是 &lt;code class=&quot;highlighter-rouge&quot;&gt;$数学公式$&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;$$数学公式$$&lt;/code&gt;。
行内公式是可以让公式在文中与文字或其他东西混编，不独占一行。&lt;/p&gt;

&lt;h2 id=&quot;jekyll中插入数学公式&quot;&gt;jekyll中插入数学公式&lt;/h2&gt;
&lt;h3 id=&quot;安装mathjax-1&quot;&gt;安装mathjax&lt;/h3&gt;
&lt;p&gt;你需要将一段script加入到_layout/post.html 里面&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// mathjax 
&amp;lt;script&amp;gt;
  (function () {
    var script = document.createElement(&quot;script&quot;);
    script.type = &quot;text/javascript&quot;;
    script.src  = &quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;;
    document.getElementsByTagName(&quot;head&quot;)[0].appendChild(script);
  })();
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;和在sublime中编写公式的不同之处&quot;&gt;和在sublime中编写公式的不同之处&lt;/h3&gt;
&lt;p&gt;需要说明的是这里引发渲染的与markdown里面的略有区别：&lt;code class=&quot;highlighter-rouge&quot;&gt;$公式$&lt;/code&gt;在这个里面是不能用的; &lt;code class=&quot;highlighter-rouge&quot;&gt;$$ 公式 $$&lt;/code&gt; 在markdown里面是居中的公式，而在这里是Inline公式；这里的居中另起一行的是 &lt;code class=&quot;highlighter-rouge&quot;&gt;\\[ 公式 \\]&lt;/code&gt;；也可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;\\( 公式 \\)&lt;/code&gt;来实现Inline公式。&lt;/p&gt;

&lt;h3 id=&quot;举例&quot;&gt;举例&lt;/h3&gt;

&lt;p&gt;例1：
质能方程 &lt;script type=&quot;math/tex&quot;&gt;E = mc^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;例2：
质能方程 \[E = mc^2\]&lt;/p&gt;

&lt;p&gt;例3:
When \(a \ne 0\), there are two solutions to (\(ax^2 + bx + c = 0\)) and they are:
\[x = {-b \pm \sqrt{b^2-4ac} \over 2a}.\]&lt;/p&gt;

&lt;p&gt;例4：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^2 + b^2 = c^2&lt;/script&gt;

&lt;p&gt;\( sin(x^2) \)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathsf{Data = PCs} \times \mathsf{Loadings}&lt;/script&gt;

&lt;p&gt;\(\bigcap\Delta\Gamma\Lambda\mathsf{\Delta\Gamma\Lambda}\)&lt;/p&gt;

&lt;p&gt;\(G_{ut}=(V\bigcup{T},E_{ut})\)&lt;/p&gt;

&lt;p&gt;\begin{equation}
   E = mc^2
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
   e^{\pi i} + 1 = 0
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\[
\int_0^\infty \frac{x^3}{e^x-1}\,dx = \frac{\pi^4}{15}
\notag
\]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\[
P(E)   = {n \choose k} p^k (1-p)^{ n-k}
\tag{Eq-x}
\]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In equation \eqref{eq:sample}, we find the value of an
interesting integral:&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \int_0^\infty \frac{x^3}{e^x-1}\,dx = \frac{\pi^4}{15}
  \label{eq:sample}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;jekyll-书写伪代码&quot;&gt;jekyll 书写伪代码&lt;/h2&gt;

&lt;div class=&quot;pseudo&quot;&gt;&lt;span class=&quot;indent&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;Function&lt;/span&gt; &lt;span class=&quot;function&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;old, new&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;
  &lt;/span&gt;remaining &lt;span class=&quot;op&quot;&gt;&amp;larr;&lt;/span&gt; quorumSize
&lt;span class=&quot;indent&quot;&gt;  
  &lt;/span&gt;success &lt;span class=&quot;op&quot;&gt;&amp;larr;&lt;/span&gt; &lt;span class=&quot;symbol&quot;&gt;False&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;For&lt;/span&gt; &lt;span class=&quot;symbol&quot;&gt;Each&lt;/span&gt; host
&lt;span class=&quot;indent&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;function&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;&amp;#65339;&lt;/span&gt;host&lt;span class=&quot;op&quot;&gt;&amp;#65341;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;&amp;larr;&lt;/span&gt; &lt;span class=&quot;function&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;host, &lt;span class=&quot;function&quot;&gt;propose&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;old, new&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;function&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;&amp;#65339;&lt;/span&gt;host&lt;span class=&quot;op&quot;&gt;&amp;#65341;&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;ok&quot;&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;      &lt;/span&gt;remaining&lt;span class=&quot;op&quot;&gt;--&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;If&lt;/span&gt; remaining &lt;span class=&quot;op&quot;&gt;&amp;#12297;&lt;/span&gt; 1&lt;span class=&quot;op&quot;&gt;+&lt;/span&gt;quorumSize/2
&lt;span class=&quot;indent&quot;&gt;    &lt;/span&gt;success &lt;span class=&quot;op&quot;&gt;&amp;larr;&lt;/span&gt; &lt;span class=&quot;symbol&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;For&lt;/span&gt; &lt;span class=&quot;symbol&quot;&gt;Each&lt;/span&gt; result
&lt;span class=&quot;indent&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;If&lt;/span&gt; success
&lt;span class=&quot;indent&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;function&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;host, &lt;span class=&quot;function&quot;&gt;confirm&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;old, new&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;symbol&quot;&gt;Else&lt;/span&gt;
&lt;span class=&quot;indent&quot;&gt;      &lt;/span&gt;&lt;span class=&quot;function&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;host, &lt;span class=&quot;function&quot;&gt;cancel&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;(&lt;/span&gt;old, new&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;)&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;jekyll-配置论文引用&quot;&gt;jekyll 配置论文引用&lt;/h2&gt;

&lt;p&gt;@ARTICLE{boulet_etal_N2008,
  author = {Boulet, R. and Jouve, B. and Rossi, F. and Villa, N.},
  title = {Batch kernel {SOM} and related {L}aplacian methods for social network analysis},
  journal = {Neurocomputing},
  year = {2008},
  volume = {71},
  pages = {1257-1273},
  number = {7-9},
  doi = {doi:10.1016/j.neucom.2007.12.026},
  keywords ={self-organizing maps, social network},
  webnote = {Comments upon this article can be found on Nature web site.},
  website = {http://www.elsevier.com/wps/find/journaldescription.cws_home/505628/description}
}&lt;/p&gt;

&lt;p&gt;@CONFERENCE{rossi_etal_DD2011,
  author = {Rossi, F. and Villa-Vialaneix, N. and Hautefeuille, F.},
  title = {Exploration of a large database of French notarial acts with social network methods},
  booktitle = {Digital Diplomatics 2011},
  year = {2011},
  address = {Napoli, Italy},
  eventdate = {2011-09-29/2011-10-01},
  keywords = {social network},
  poster = {yes},
  website = {http://www.cei.lmu.de/digdipl11/}
}&lt;/p&gt;

&lt;h2 id=&quot;9-参考资料&quot;&gt;9. 参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://frank19900731.github.io/blog/2015/04/13/zai-sublime-zhong-pei-zhi-markdown-huan-jing/#monokai-extended--markdown-extended&quot;&gt;在 Sublime 中配置 Markdown 环境&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jzqt.github.io/2015/06/30/Markdown%E4%B8%AD%E5%86%99%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/&quot;&gt;Markdown中写数学公式&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.kamidox.com/write-math-formula-with-mathjax.html&quot;&gt;使用 Markdown + MathJax 在博客里插入数学公式&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://brucezhaor.github.io/blog/2016/01/07/Mathjax-with-jekyll/&quot;&gt;MathJax with Jekyll&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://wulfric.me/2014/09/jekyll-plugs/&quot;&gt;Jekyll 博客的一些优化插件与配置&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://noyobo.com/2014/10/19/jekyll-kramdown-highlight.html&quot;&gt;jekyll kramdown 语法高亮配置&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://waterbolik.github.io/jekyll/2015/10/14/MathJax%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3&quot;&gt;MathJax语法详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://gastonsanchez.com/visually-enforced/opinion/2014/02/16/Mathjax-with-jekyll/&quot;&gt;MathJax with Jekyll&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.zybuluo.com/yangfch3/note/267947&quot;&gt;MathJax 总结&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://kyl.farbox.com/post/farbox/2016-03-30&quot;&gt;使用MathJax插入带编号的数学公式并通过编号引用数学公式&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://baige5117.github.io/blog/mathjax_in_jekyll.html&quot;&gt;利用 MathJax 在 Jekyll 网页中插入 Tex/LaTex 数学公式&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.rubydoc.info/gems/jekyll-pseudo/0.1.6&quot;&gt;jekyll-pseudo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://crispgm.com/page/48-tips-for-jekyll-you-should-know.html&quot;&gt;48 个你需要知道的 Jekyll 使用技巧&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://zale.site/articles/2016/05/Academia-Writing-Using-Markdown-and-Pandoc.html&quot;&gt;使用 Pandoc Markdown 进行学术论文写作&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bl.ocks.org/Teino1978-Corp/325442fda3e3776f49e0&quot;&gt;Jekyll Scholar - bibliographies &amp;amp; citations.Jekyll-Scholar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://tuxette.nathalievilla.org/?p=1426&quot;&gt;Utiliser Jekyll avec LaTeX et BibTeXUse Jekyll with LaTeX and BibTeX&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar/&quot;&gt;jekyll-scholar 官方文档&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jeffjade.com/2016/03/26/2016-03-26-rakefile-for-jekyll/&quot;&gt;Rake让Jekyll写博更优雅&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.grayghostvisuals.com/workflow/deploying-jekyll-with-rake/&quot;&gt;Deploying a Jekyll site with a Rakefile&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mazhuang.org/2016/02/04/switch-to-kramdown-from-redcarpet/&quot;&gt;将 GitHub Pages 从 Redcarpet 切换到 kramdown&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://gohom.win/2015/11/06/Kramdown-note/#ID1&quot;&gt;kramdown和markdown较大的差异比较&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jokinkuang/jokinkuang.github.io/blob/master/_posts/jekyll/2016-9-18-what-did-i-do-for-the-blog.md&quot;&gt;Jekyll文章分类问题&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/Shopify/liquid/wiki/Liquid-for-Designers&quot;&gt;Liquid&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://xiajian.github.io/rhg-zh/zh/liquid/&quot;&gt;liquid模板语言&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://superdevresources.com/image-caption-jekyll/&quot;&gt;Display Image caption in Posts on Jekyll blogs&lt;/a&gt;&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>第三方JavaScript-介绍</title>
        <link>http://localhost:4000/2017/03/31/third-part-js-intro.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/31/third-part-js-intro.html</guid>
        <pubDate>Fri, 31 Mar 2017 13:13:02 +0800</pubDate>
        <description>&lt;h3 id=&quot;1-第三方javascript定义&quot;&gt;1. 第三方JavaScript定义&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;第三方JavaScript是一种JS的编程模式，可以用来创建高度分布式的Web应用，我们遇到最多的应用就是广告脚本，它可以在发布者网站上生成，定向投放广告，事实上几乎所有的广告都是从单独的广告服务器加载第三方脚本。Facebook和Twitter开发了数十个社交微件展示在发布者网站上，这些微件可以在他们应用系统之外拓展用户规模。&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>Big Endian Little Endian</title>
        <link>http://localhost:4000/2017/03/31/big-endian-little-endian.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/31/big-endian-little-endian.html</guid>
        <pubDate>Fri, 31 Mar 2017 11:56:11 +0800</pubDate>
        <description>
</description>
      </item>
    
      <item>
        <title>二进制文件和文本文件的区别</title>
        <link>http://localhost:4000/2017/03/31/difference-between-binary-textfile.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/31/difference-between-binary-textfile.html</guid>
        <pubDate>Fri, 31 Mar 2017 00:00:00 +0800</pubDate>
        <description>&lt;h3 id=&quot;1-起因&quot;&gt;1. 起因&lt;/h3&gt;
&lt;p&gt;今天在实验室谈论到bjson数据格式，从而引出是binary和textfile的区别，我第一感觉是binary文件省空间，但是又说不清为什么…&lt;/p&gt;

&lt;h3 id=&quot;2实验&quot;&gt;2.实验&lt;/h3&gt;

&lt;p&gt;首先定义结构体&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;struct Student
{
    int num;
    char name[20];
    float score;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将stud写入到二进制文件&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void write_to_binary_file(){
    char test_file_name[100] = &quot;test1.dat&quot;;
    FILE *fo = fopen(test_file_name, &quot;wb&quot;);
    struct Student stdu;
    stdu.num = 111;
    strcpy(stdu.name,&quot;lei&quot;);
    stdu.score = 80.0f;
    fwrite(&amp;amp;stdu, sizeof(struct Student), 1, fo);
    fclose(fo);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;将stud写入到文本文件&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void write_to_text_file(){
    char test_file_name[100] = &quot;test2.dat&quot;;
    FILE *fo = fopen(test_file_name, &quot;wb&quot;);
    struct Student stdu;
    stdu.num = 111;
    strcpy(stdu.name,&quot;lei&quot;);
    stdu.score = 80.0f;
    fprintf(fo, &quot;%d%s%f&quot;,stdu.num,stdu.name,stdu.score);
    fclose(fo);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;3-使用hexdump分析实验结果&quot;&gt;3. 使用HEXDUMP分析实验结果&lt;/h3&gt;
&lt;p&gt;hexdump命令一般用来查看“二进制”文件的十六进制编码，但实际上它的用途不止如此，手册页上的说法是“ascii, decimal, hexadecimal, octal dump”，它能查看任何文件，而不只限于二进制文件。另外还有 xxd 和 od 也可以做类似的事情。在程序输出二进制格式的文件时，常用hexdump来检查输出是否正确。当然也可以使用Windows上的UltraEdit32之类的工具查看文件的十六进制编码，但Linux上有现成的工具，何不拿来用呢&lt;/p&gt;

&lt;h4 id=&quot;31-常用参数&quot;&gt;3.1 常用参数&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;hexdump -C -n length -s skip file_name&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;-C 定义了导出的格式；-s skip 指定了从文件头跳过多少字节，或者说是偏移量，默认是十进制，如果是0x开头，则是十六进制；-n 指定了导出多少长度&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果要看到较理想的结果，推荐使用-C参数，显示结果分为三列（文件偏移量、字节的十六进制、ASCII字符）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;32-实验结果和结论&quot;&gt;3.2 实验结果和结论&lt;/h4&gt;

&lt;!-- _includes/image.html --&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img src=&quot;http://localhost:4000//public/img/binary-text.png&quot; alt=&quot;title for image&quot; /&gt;
    
    
        &lt;p class=&quot;image-caption&quot;&gt;二进制和文本文件结果&lt;/p&gt;
    
&lt;/div&gt;

&lt;p&gt;二进制文件里面将111编码成6f，1个字节，这刚好是111的16进制表示，而文本文件中则写成31，31，31用了3个字节，表示111。6c   65   69  表示lei，之后2进制文件里是几个连续的00，而文本文件中是38   30……文本文件将浮点数80.000000用了38(表示8)   30(表示0)  2E(表示.)   30(表示0)   30(表示0)   30(表示0)   30(表示0)   30(表示0)   30(表示0)，二进制文件用了4个字节表示浮点数00   00   A0   42
通过这里我们可以初见端倪了，二进制将数据在内存中的样子原封不动的搬到文件中，文本格式则是将每一个数据转换成字符写入到文件中，他们在大小上，布局上都有着区别。由此可以看出，2进制文件可以从读出来直接用，但是文本文件还多一个“翻译”的过程。&lt;/p&gt;

&lt;h3 id=&quot;4-资料&quot;&gt;4. 资料&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/19971994&quot;&gt;文本文件和二进制文件的区别？请举例说明。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/zhangjiankun/archive/2011/11/27/2265184.html&quot;&gt;文本文件与二进制文件区别&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cppblog.com/yg2362/archive/2012/07/12/182956.html&quot;&gt;浅谈二进制文件读写和文本文件读写的区别&lt;/a&gt;&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>paxos和raft是什么？</title>
        <link>http://localhost:4000/2017/03/28/introduction-paxos-raft.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/28/introduction-paxos-raft.html</guid>
        <pubDate>Tue, 28 Mar 2017 20:50:24 +0800</pubDate>
        <description>
</description>
      </item>
    
      <item>
        <title>A Blogging Scholar</title>
        <link>http://localhost:4000/2017/03/28/scholar.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/28/scholar.html</guid>
        <pubDate>Tue, 28 Mar 2017 16:01:36 +0800</pubDate>
        <description>&lt;h1 id=&quot;a-blogging-scholar&quot;&gt;A Blogging Scholar&lt;/h1&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis ‘aute irure dolor in reprehenderit in voluptate’ (missing reference)
velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat
cupidatat non proident, ‘sunt in culpa qui officia deserunt mollit anim id est
laborum’ (missing reference).&lt;/p&gt;

&lt;p&gt;Duis ‘aute irure dolor in reprehenderit in voluptate’ (missing reference)
velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat
cupidatat non proident, ‘sunt in culpa qui officia deserunt mollit anim id est
laborum’ (missing reference).&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul class=&quot;bibliography&quot;&gt;&lt;/ul&gt;
</description>
      </item>
    
      <item>
        <title>mac系统常用软件的快捷键</title>
        <link>http://localhost:4000/2017/03/28/hot-key-for-mac.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/28/hot-key-for-mac.html</guid>
        <pubDate>Tue, 28 Mar 2017 15:10:31 +0800</pubDate>
        <description>&lt;h2 id=&quot;1sublime-text&quot;&gt;1.Sublime Text&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Ctrl+ `&lt;/em&gt; 可以调出sublime控制台&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2chrome&quot;&gt;2.Chrome&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Command + R&lt;/em&gt; 刷新页面&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Command + Shift +R&lt;/em&gt; 强制刷新页面&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Command + 9&lt;/em&gt; 跳转到最后一个标签页&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Command + Shift +t&lt;/em&gt; 重新打开最后关闭的标签页，并跳转到该标签页&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;21-强制刷新刷新重新载入的区别&quot;&gt;2.1 强制刷新、刷新、重新载入的区别？&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/22087172&quot;&gt;浏览器中刷新、强制刷新和重新载入的区别是什么？&lt;/a&gt;&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>进程迁移技术研究</title>
        <link>http://localhost:4000/2017/03/27/process-migration.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/27/process-migration.html</guid>
        <pubDate>Mon, 27 Mar 2017 22:06:32 +0800</pubDate>
        <description>
</description>
      </item>
    
      <item>
        <title>利用Letax写伪代码的Demo</title>
        <link>http://localhost:4000/2017/03/27/write-pseudo-code-with-latex.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/27/write-pseudo-code-with-latex.html</guid>
        <pubDate>Mon, 27 Mar 2017 19:52:55 +0800</pubDate>
        <description>&lt;h2 id=&quot;模板&quot;&gt;模板：&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;\documentclass&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[11pt]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;article&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;fontspec&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\setromanfont&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;STSong&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;\setmonofont&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Courier New&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;\XeTeXlinebreaklocale&lt;/span&gt; &quot;zh&quot;
&lt;span class=&quot;k&quot;&gt;\XeTeXlinebreakskip&lt;/span&gt; = 0pt plus 1pt

&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;CJK&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[top=2cm, bottom=2cm, left=2cm, right=2cm]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;geometry&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;algorithm&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;algorithmicx&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;algpseudocode&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;amsmath&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;\floatname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;algorithm&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;算法&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\renewcommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\algorithmicrequire&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\textbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;输入:&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\renewcommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\algorithmicensure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\textbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;输出:&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\begin{document}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{CJK*}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;UTF8&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;gkai&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;\begin{algorithm}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;\caption&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;用归并排序求逆序数&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;\begin{algorithmic}&lt;/span&gt;[1] &lt;span class=&quot;c&quot;&gt;%每行显示行号&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;\Require&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;数组，&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;数组大小
            &lt;span class=&quot;k&quot;&gt;\Ensure&lt;/span&gt; 逆序数
            &lt;span class=&quot;k&quot;&gt;\Function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;MergerSort&lt;span class=&quot;p&quot;&gt;}{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array, left, right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\If&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;left &amp;lt; right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;middle &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;left &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; right&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; result &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\Call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;MergerSort&lt;span class=&quot;p&quot;&gt;}{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array, left, middle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; result &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\Call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;MergerSort&lt;span class=&quot;p&quot;&gt;}{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array, middle, right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; result &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\Call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Merger&lt;span class=&quot;p&quot;&gt;}{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array,left,middle,right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\EndIf&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\Return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;\EndFunction&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;\Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Merger&lt;span class=&quot;p&quot;&gt;}{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array, left, middle, right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; middle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\While&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&amp;lt;middle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\textbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;and&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&amp;lt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\If&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;&amp;lt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\Else&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; result &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;middle &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\EndIf&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\EndWhile&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\While&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&amp;lt;middle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\EndWhile&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\While&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&amp;lt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\EndWhile&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\For&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\to&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;left &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;\gets&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt; B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\EndFor&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;\State&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\Return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$}&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;\EndFunction&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;\end{algorithmic}&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;\end{algorithm}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{CJK*}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{document}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;效果&quot;&gt;效果&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/latex-pseudo.png&quot; alt=&quot;归并排序伪代码&quot; title=&quot;归并排序&quot; /&gt;&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>https深入分析</title>
        <link>http://localhost:4000/2017/03/26/introduction-to-https.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/26/introduction-to-https.html</guid>
        <pubDate>Sun, 26 Mar 2017 23:51:55 +0800</pubDate>
        <description>&lt;h2 id=&quot;8-对称加密和非对称加密&quot;&gt;8. 对称加密和非对称加密&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;对称密钥加密（英语：Symmetric-key algorithm）又称为对称加密、私钥加密、共享密钥加密，是密码学中的一类加密算法。这类算法在加密和解密时使用相同的密钥，或是使用两个可以简单地相互推算的密钥。实务上，这组密钥成为在两个或多个成员间的共同秘密，以便维持专属的通讯联系。与公开密钥加密相比，要求双方取得相同的密钥是对称密钥加密的主要缺点之一，常见的对称加密算法有DES、3DES、AES、Blowfish、IDEA、RC5、RC6。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举例：
首先，让我们先从一个情景开始讲起，想当初我们在初中，高中，甚至于大学，每次考试都有人在试图如何更加隐蔽的作弊！那大家都想了什么方法呢？比如张三学习比李四好，李四就想在考试的时候让张三“帮助”一下自己，当然，他们俩不可能像我们平常对话一样说，第一题选A，第二题选B等等，为什么？因为监考老师明白他俩在谈论什么，也就是说这种沟通交流方式属于“明文”，所以李四就想：“我需要发明一种，只有我和张三明白的交流方式”，那李四做了什么呢？恩，李四去找张三说：“当我连续咳嗽三声的时候你看我，然后如果我摸了下左耳朵，说明你可以开始给我传答案了，如果没反应，那说明我真的是在咳嗽。。。。”， 然后，怎么传答案呢？很简单，“你摸左耳朵代表A, 摸右耳朵代表B，左手放下代表C，右手放下代表D”，好了，这就是他们的“算法(规则)”，将信息的一种形式(A,B,C,D)，这里我们称为“明文”，转换成了另一种形式(摸左耳朵，摸右耳朵，放左手，放右手)，这里称为“密文”，经过这种转换，很显然监考老师不会明白这些“密文”，这样，张三和李四就通过“密文”的形式实现了信息的交换。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;公开密钥加密（英语：public-key cryptography，又译为公开密钥加密），也称为非对称加密（asymmetric cryptography），一种密码学算法类型，在这种密码学方法中，需要一对密钥(其实这里密钥说法不好，就是“钥”)，一个是私人密钥，另一个则是公开密钥。这两个密钥是数学相关，用某用户密钥加密后所得的信息，只能用该用户的解密密钥才能解密。如果知道了其中一个，并不能计算出另外一个。因此如果公开了一对密钥中的一个，并不会危害到另外一个的秘密性质。称公开的密钥为公钥；不公开的密钥为私钥。这种加密算法应用非常广泛，SSH, HTTPS, TLS，电子证书，电子签名，电子身份证等等&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举例：
比如说张三生成了他自己的一个“私钥-公钥”对，叫做“张三私钥-张三公钥”，李四生成了他自己的一个“私钥-公钥”对，叫做“李四私钥-李四公钥”，之前我们说过私钥要每个个体自己进行保存，公钥可以随便分享，目的是为什么呢？是为了加密信息！
比如，李四想给张三发送密文。于是李四开始给张三发QQ&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;李四：
“hi哥们，我想给你发个密文，把你的公钥给我发过来用用。”

张三：
“没问题的，这是我的公钥： d#8yHE8eU#hb*!neb，用这个公钥加密你的信息后给我发过来吧”

李四：
“这是我想对你说的话： *&amp;amp;#@uehuu(**#eehu&amp;amp;$##bfeu&amp;amp;&amp;amp;”
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;不过，这种非对称加密算法还不安全，如果黑客冒充了张三，发送给李四“黑客的公钥”，而不是“张三的公钥”，那么当李四收到该公钥的时候，就不假思索的使用该公钥加密了他的信息，然后毫不犹豫的将加密的密文发了过去，然后黑客得意的笑了。所以，问题在哪呢？这就是数字签名和数字证书的来历。&lt;/p&gt;

&lt;h2 id=&quot;9数字签名和数字证书&quot;&gt;9.数字签名和数字证书&lt;/h2&gt;

&lt;p&gt;首先明确几个基本概念：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;密钥对，在非对称加密技术中，有两种密钥，分为私钥和公钥，私钥是密钥对所有者持有，不可公布，公钥是密钥对持有者公布给他人的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;公钥，公钥用来给数据加密，用公钥加密的数据只能使用私钥解密。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;私钥，如上，用来解密公钥加密的数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;摘要，对需要传输的文本，做一个HASH计算，一般采用SHA1，SHA2来获得。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;签名，使用私钥对需要传输的文本的摘要进行加密，得到的密文即被称为该次传输过程的签名。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;签名验证，数据接收端，拿到传输文本，但是需要确认该文本是否就是发送发出的内容，中途是否曾经被篡改。因此拿自己持有的公钥对签名进行解密（密钥对中的一种密钥加密的数据必定能使用另一种密钥解密。），得到了文本的摘要，然后使用与发送方同样的HASH算法计算摘要值，再与解密得到的摘要做对比，发现二者完全一致，则说明文本没有被篡改过。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在签名的过程中，有一点很关键，收到数据的一方，需要自己保管好公钥，但是要知道每一个发送方都有一个公钥，那么接收数据的人需要保存非常多的公钥，这根本就管理不过来。并且本地保存的公钥有可能被篡改替换，无从发现。怎么解决这一问题了？由一个统一的证书管理机构来管理所有需要发送数据方的公钥，对公钥进行认证和加密。这个机构也就是我们常说的CA。认证加密后的公钥，即是证书，又称为CA证书，证书中包含了很多信息，最重要的是申请者的公钥。&lt;/p&gt;

&lt;p&gt;CA机构在给公钥加密时，用的是CA机构的一个统一的密钥对，在加密公钥时，用的是其中的私钥。这样，申请者拿到证书后，在发送数据时，用自己的私钥生成签名，将签名、证书和发送内容一起发给对方，对方拿到了证书后，需要对证书解密以获取到证书中的公钥，解密需要用到CA机构的”统一密钥对“中的公钥，这个公钥也就是我们常说的CA根证书，通常需要我们到证书颁发机构去下载并安装到相应的收取数据的客户端，如浏览器上面。这个公钥只需要安装一次。有了这个公钥之后，就可以解密证书，拿到发送方的公钥，然后解密发送方发过来的签名，获取摘要，重新计算摘要，作对比，以验证数据内容的完整性。&lt;/p&gt;

&lt;h2 id=&quot;10-sha1-sha2-and-sha256-hash-algorithms&quot;&gt;10. SHA1, SHA2 and SHA256 hash algorithms&lt;/h2&gt;
&lt;p&gt;SHA256由TBS Internet 于2008年发布，将在最近两年逐步替换SHA1.但是什么是SHA呢？&lt;/p&gt;

&lt;p&gt;SHA（secure hash algorithm）是CA中心用来生成数字证书和CRL(certificates revocation list)的hash算法，它首次被美国国家安全局（National Security Agency）
于1993年提出，当时是SHA0，用来对文件生成唯一hash值。
SHA1是现在常用的一种hash算法。SHA2是对SHA1的发展，它包括四种hash函数，SHA224, SHA256, SHA384 and SHA512.它和SHA1的工作方式相同，但是它更强大，可以生成更长的hash值。&lt;/p&gt;

&lt;h2 id=&quot;11现在流行两种hash-attack&quot;&gt;11.现在流行两种hash attack：&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/SHA-1&quot;&gt;深入理解SHA&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;hash collision：当两个不同文件生成同一个hash值时，说明有冲突，&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;the preimage：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;12-crl是什么&quot;&gt;12. CRL是什么&lt;/h2&gt;

&lt;h2 id=&quot;13-参考文献&quot;&gt;13. 参考文献&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/19577317&quot;&gt;http 和 https 有何区别？如何灵活使用？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000004461428&quot;&gt;白话解释 对称加密算法 VS 非对称加密算法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://codefine.co/1455.html&quot;&gt;公钥私钥加密解密数字证书数字签名详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tbs-certificates.co.uk/FAQ/en/sha256.html&quot;&gt;All about SHA1, SHA2 and SHA256 hash algorithms&lt;/a&gt;&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>在Jekyll中集成Disqus的原理</title>
        <link>http://localhost:4000/2017/03/24/principle-behind-jekyll.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/24/principle-behind-jekyll.html</guid>
        <pubDate>Fri, 24 Mar 2017 19:25:05 +0800</pubDate>
        <description>&lt;h1 id=&quot;disqus&quot;&gt;Disqus&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Disqus&lt;/strong&gt;是一个第三方的JavaScript应用，它通过用户向自己网站(比如Blog)注入js脚本，用户通过嵌入的一小段JavaScript脚本
向Disqus服务器发送请求，用来初始化JavaScript Loader，这个Loader可以在用户网站创建所需的iframe元素，并从Disqus服务器
获取数据，渲染魔板，并将该页面所需的评论数据注入页面。&lt;/p&gt;

&lt;h2 id=&quot;后端架构&quot;&gt;后端架构&lt;/h2&gt;
&lt;p&gt;Disqus后台用了很多技术来支持这看似简单的操作，后台每天需要处理上百万的读请求，主要用到的技术有Python，Django，PostgreSQL
，由于Disqus的业务绝大部分是实时业务，所以也需要用Redis缓存技术。&lt;/p&gt;

&lt;h2 id=&quot;加载第三方js的技术&quot;&gt;加载第三方JS的技术&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;网页中加载JS文件是一个老问题了，已经被讨论了一遍又一遍，这里不会再赘述各种经典的解决方案。JS文件可以通过来源来分为两个纬度：第一方JS和第三方JS。第一方JS是网页开发者自己使用的JS代码（内容开发者可控）。而第三方JS则是其他服务提供商提供的（内容开发者不可控），他们将自己的服务包装成JS SDK供网页开发者使用。Disqus用到了第三方JS文件的加载技术。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;涉及到的文件有_includes/disqus.html, _layouts/post.html&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>软链接的使用技巧</title>
        <link>http://localhost:4000/2017/03/24/how-to-use-ln.html</link>
        <guid isPermaLink="true">http://localhost:4000/2017/03/24/how-to-use-ln.html</guid>
        <pubDate>Fri, 24 Mar 2017 00:00:00 +0800</pubDate>
        <description>&lt;h2 id=&quot;链接是什么&quot;&gt;链接是什么&lt;/h2&gt;
&lt;p&gt;ln是一个非常常用的Linux命令，它可以用来为一个文件或目录在其他目录里创建一个同步的链接，它的链接分为硬链接与软链接两种，硬链接类似于拷贝，而软链接则类似于一个指向源目标文件或目录的一个指针，就像Windows中的快捷方式一样。&lt;/p&gt;

&lt;h2 id=&quot;什么时候需要用软链接&quot;&gt;什么时候需要用软链接&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;之前我的Mac笔记本一个分区的硬盘快用尽了，于是将快用尽的硬盘链接到一个空的硬盘上解决了这个问题。&lt;/li&gt;
  &lt;li&gt;最近在用jekyll写博客，由于我们屏幕截图的文件会保存在桌面，可以博客的图片资源在其他文件夹下，为
了解决这个问题，我打算在桌面做一个软链接文件指向博客的图片资源&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;具体用法&quot;&gt;具体用法&lt;/h2&gt;
&lt;p&gt;具体用法：&lt;strong&gt;ln –s 源文件 目标文件&lt;/strong&gt;
举例：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对某个文件建立一个硬链接，比如: &lt;strong&gt;ln Number.txt t3&lt;/strong&gt; 这种相当于是将Number.txt拷贝为了一份t3，通过ll命令可以发现两个文件都是一样的。&lt;/li&gt;
  &lt;li&gt;对某个文件建立一个软链接，比如: &lt;strong&gt;ln -s Number.txt t4&lt;/strong&gt; t4就相当于是Number.txt的一个快捷方式。&lt;/li&gt;
  &lt;li&gt;对目录创建一个软链接，比如: &lt;strong&gt;ln -s /Users/extends_die/code/blog/public/img/ /Users/extends_die/Desktop/BLOG&lt;/strong&gt; ,这个相当于
为img文件夹在桌面创建了一个快捷方式。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;注意&quot;&gt;注意&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化；&lt;/li&gt;
  &lt;li&gt;ln的链接又分软链接和硬链接两种，软链接就是 &lt;strong&gt;ln –s ** **&lt;/strong&gt; ，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 &lt;strong&gt;ln ** **&lt;/strong&gt; ，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。&lt;/li&gt;
  &lt;li&gt;用ln命令创建连接文件或目录的时候，路径一定要用全路径。 否则会出现“不能完成此操作，因为找不到XXX的原始项目”&lt;/li&gt;
  &lt;li&gt;链接的删除也很简单，直接用rm 链接名 删除即可，删除链接不会删除链接对应的源文件或目录。&lt;/li&gt;
&lt;/ol&gt;

</description>
      </item>
    
  </channel>
</rss>